{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9635bf9c-8442-438b-a936-edc4a0a17cf1",
   "metadata": {},
   "source": [
    "# 1.2 a) Visualize the optimization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f016cfd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdfbbde5d50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import matplotlib\n",
    "from matplotlib.pyplot import figure\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2169f05f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_dataset size: 60000 \n",
      "Test_dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(),  download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "def train_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size = batch_size, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def test_loader(batch_size):\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return test_loader\n",
    "\n",
    "print(\"Train_dataset size:\", len(train_dataset),\"\\nTest_dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01188f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 500)\n",
    "        self.fc2 = nn.Linear(500, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad85355e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainFunc(model,num_epochs,train_batch_size,status_interval):\n",
    "    model.train()\n",
    "    print('started')\n",
    "    train_load = train_loader(train_batch_size)\n",
    "    n_total_steps = len(train_load)\n",
    "    train_losses = []\n",
    "    train_epoch = []\n",
    "    train_acc = []\n",
    "    epoch = 0\n",
    "    modelParamWgt = pd.DataFrame()\n",
    "    trainAvgLossArr = []\n",
    "    trainAvgAccArr = []\n",
    "    firstParaWgt = {}\n",
    "\n",
    "    for epoch in range (num_epochs):\n",
    "        epoch += 1\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        lossSum =0\n",
    "        totalacc=0\n",
    "        epoch_df = pd.DataFrame()\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_load):  \n",
    "            images, labels = Variable(images),Variable(labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(images)\n",
    "            images.requires_grad = True\n",
    "\n",
    "            loss = loss_func(prediction, labels)\n",
    "            lossSum += loss.detach().numpy()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            acc = 100.0 * n_correct / n_samples\n",
    "            totalacc += acc\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_acc.append(acc)\n",
    "            train_epoch.append(epoch)\n",
    "\n",
    "            if (i+1) % status_interval == 0:\n",
    "                print (f'Train O/P: Epoch [{epoch}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}',end= '\\r',flush = True)\n",
    "        \n",
    "        for name, parameter in model.named_parameters():\n",
    "            if'weight' in name:\n",
    "                weights = torch.nn.utils.parameters_to_vector(parameter).detach().numpy() \n",
    "                epoch_df = pd.concat([epoch_df, pd.DataFrame(weights).T], axis = 1) \n",
    "\n",
    "        modelParamWgt = pd.concat([modelParamWgt, epoch_df], axis = 0)     \n",
    "\n",
    "        epochLoss = lossSum/n_total_steps\n",
    "        epochAcc = totalacc/n_total_steps\n",
    "        trainAvgLossArr.append(epochLoss)    \n",
    "        trainAvgAccArr.append(epochAcc)\n",
    "                       \n",
    "    return train_epoch,train_losses,train_acc,trainAvgLossArr,trainAvgAccArr, modelParamWgt\n",
    "\n",
    "def testFunction(model,loss_func,test_batch_size): \n",
    "    test_load = test_loader(test_batch_size)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        testLoss = 0\n",
    "        count = 0\n",
    "        for images, labels in test_load:\n",
    "            images, labels = Variable(images),Variable(labels)\n",
    "            \n",
    "            prediction = model(images)\n",
    "            testLoss += loss_func(prediction,labels).item()\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            count += 1\n",
    "    netTest_loss = testLoss/count\n",
    "    netTest_acc1 = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the test images: {netTest_acc1}% & Test Loss: {netTest_loss}',end= '\\r',flush = True)\n",
    "    return netTest_acc1, netTest_loss\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7aa731",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters:418060\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "tempModel = DNN()\n",
    "for i in tempModel.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f02c4e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0\n",
      "started\n",
      "Time: 1y of the network on the test images: 97.19% & Test Loss: 0.09281539544463158\n",
      "started\n",
      "Time: 2y of the network on the test images: 97.12% & Test Loss: 0.0990191750228405\n",
      "started\n",
      "Time: 3y of the network on the test images: 97.0% & Test Loss: 0.100449188798666\n",
      "started\n",
      "Time: 4y of the network on the test images: 97.12% & Test Loss: 0.09678149372339248\n",
      "started\n",
      "Time: 5y of the network on the test images: 97.26% & Test Loss: 0.09338565543293953\n",
      "started\n",
      "Time: 6y of the network on the test images: 97.23% & Test Loss: 0.09386177230626344\n",
      "started\n",
      "Time: 7y of the network on the test images: 97.04% & Test Loss: 0.09805064126849175\n",
      "started\n",
      "Accuracy of the network on the test images: 97.3% & Test Loss: 0.09169732015579939\r"
     ]
    }
   ],
   "source": [
    "max_epochs = 30\n",
    "all_df = pd.DataFrame()\n",
    "columns=[\"x\",\"y\",\"Times\"]\n",
    "trainAllacc={}\n",
    "testAllacc={}\n",
    "trainAllloss={}\n",
    "testAllloss={}\n",
    "train_batch_size = 1000\n",
    "test_batch_size = 1000\n",
    "status_interval = 60\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "firstLayer = pd.DataFrame()\n",
    "\n",
    "for count in range(8):\n",
    "\n",
    "    j = copy.deepcopy(count)\n",
    "\n",
    "    j = DNN()  \n",
    "\n",
    "    print(\"Time: \"+str(count))\n",
    "    optimizer = torch.optim.Adam(j.parameters(),lr = 0.0002,weight_decay=1e-4)\n",
    "    model_name1 = \"Times:\"+str(count)  \n",
    "    \n",
    "    train_epoch,train_losses,train_acc,trainAvgLoss,trainAvgAccArr, modelParamWgt = trainFunc(j,max_epochs,train_batch_size,status_interval)\n",
    "    testAcc, testLoss = testFunction(j,loss_func,test_batch_size)\n",
    "\n",
    "    all_df = pd.concat([all_df, modelParamWgt], ignore_index=True)\n",
    "    \n",
    "    testAllacc[count] = testAcc\n",
    "    trainAllloss[count] = trainAvgLoss\n",
    "    testAllloss[count] = testLoss\n",
    "    trainAllacc[count] = trainAvgAccArr\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88c197f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0             1             2             3             4    \\\n",
      "0    1.748428e-02 -3.643958e-04  5.497875e-03  6.346869e-06  4.730536e-05   \n",
      "1    9.235719e-03  1.078450e-05  9.246279e-04 -5.016550e-07  2.518564e-06   \n",
      "2    4.151181e-03  7.472493e-07  4.903737e-05  2.052048e-08  3.101327e-09   \n",
      "3    1.564623e-03 -4.997273e-08 -1.395133e-06 -9.733966e-10 -4.434934e-09   \n",
      "4    4.911066e-04 -2.006579e-09 -1.196437e-08  1.368816e-11  2.134083e-10   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "235 -1.279839e-29 -7.947436e-40 -2.368519e-38 -4.053281e-38  9.611934e-36   \n",
      "236  5.236417e-31 -7.947436e-40 -2.368519e-38 -1.923343e-38  3.542278e-37   \n",
      "237 -1.138815e-32 -7.947436e-40 -2.368519e-38 -1.923343e-38 -1.252804e-38   \n",
      "238 -4.097250e-34 -7.947436e-40 -2.368519e-38 -1.923343e-38 -1.252804e-38   \n",
      "239  7.086223e-35 -7.947436e-40 -2.368519e-38 -1.923343e-38 -1.252804e-38   \n",
      "\n",
      "              5             6             7             8             9    \\\n",
      "0    1.164070e-02  6.781843e-03 -4.054806e-03  2.004379e-02 -1.906049e-02   \n",
      "1    4.619203e-03  1.535290e-03 -3.895142e-04  1.142522e-02 -1.057534e-02   \n",
      "2    1.352314e-03  1.670132e-04  3.972764e-06  5.721457e-03 -5.098217e-03   \n",
      "3    2.820615e-04  4.692398e-06  2.865838e-07  2.486690e-03 -2.107970e-03   \n",
      "4    4.010972e-05 -2.705608e-07 -1.889994e-08  9.328733e-04 -7.431720e-04   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "235 -8.033568e-36  6.973815e-26  9.064655e-25 -2.144264e-29  4.045706e-30   \n",
      "236  3.082496e-37 -4.152562e-26 -7.719432e-26  9.226610e-31 -1.377120e-31   \n",
      "237  4.751838e-38  2.275678e-27  2.898549e-27 -3.930522e-32 -4.928464e-33   \n",
      "238  2.459437e-38 -7.940587e-29 -6.001314e-29  1.544651e-33  1.041607e-33   \n",
      "239  2.459437e-38  1.943582e-30 -3.534868e-31 -4.840947e-35 -9.043785e-35   \n",
      "\n",
      "     ...       490       491       492       493       494       495  \\\n",
      "0    ...  0.006021 -0.127967  0.052438 -0.064922  0.072371  0.126689   \n",
      "1    ...  0.008247 -0.139440  0.064022 -0.068040  0.085305  0.137721   \n",
      "2    ...  0.006106 -0.145865  0.067857 -0.065784  0.090021  0.143485   \n",
      "3    ...  0.004701 -0.149980  0.069713 -0.064992  0.092484  0.147787   \n",
      "4    ...  0.004469 -0.152648  0.070793 -0.064558  0.093941  0.151213   \n",
      "..   ...       ...       ...       ...       ...       ...       ...   \n",
      "235  ... -0.081406 -0.097221  0.171127 -0.024415 -0.085914 -0.131095   \n",
      "236  ... -0.082231 -0.097789  0.172101 -0.024641 -0.085506 -0.131324   \n",
      "237  ... -0.082532 -0.098259  0.173540 -0.024638 -0.085313 -0.131584   \n",
      "238  ... -0.082908 -0.098897  0.174534 -0.024718 -0.085297 -0.132085   \n",
      "239  ... -0.083487 -0.099466  0.175404 -0.025015 -0.085470 -0.132613   \n",
      "\n",
      "          496       497       498       499  \n",
      "0   -0.033197 -0.056366  0.036780 -0.131857  \n",
      "1   -0.043669 -0.064507  0.042256 -0.144895  \n",
      "2   -0.052058 -0.068286  0.041063 -0.156465  \n",
      "3   -0.058621 -0.071419  0.039972 -0.166959  \n",
      "4   -0.064796 -0.073954  0.039378 -0.176938  \n",
      "..        ...       ...       ...       ...  \n",
      "235  0.168902 -0.104591  0.011491  0.087071  \n",
      "236  0.170978 -0.104505  0.013919  0.087295  \n",
      "237  0.172634 -0.104351  0.017373  0.087824  \n",
      "238  0.174363 -0.104456  0.020114  0.088087  \n",
      "239  0.176316 -0.104724  0.022956  0.088351  \n",
      "\n",
      "[240 rows x 417500 columns]\n",
      "Shape of Loss:(240,) & Shape of Acc: (240,)\n",
      "              0             1             2             3             4    \\\n",
      "0    1.748428e-02 -3.643958e-04  5.497875e-03  6.346869e-06  4.730536e-05   \n",
      "1    9.235719e-03  1.078450e-05  9.246279e-04 -5.016550e-07  2.518564e-06   \n",
      "2    4.151181e-03  7.472493e-07  4.903737e-05  2.052048e-08  3.101327e-09   \n",
      "3    1.564623e-03 -4.997273e-08 -1.395133e-06 -9.733966e-10 -4.434934e-09   \n",
      "4    4.911066e-04 -2.006579e-09 -1.196437e-08  1.368816e-11  2.134083e-10   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "235 -1.279839e-29 -7.947436e-40 -2.368519e-38 -4.053281e-38  9.611934e-36   \n",
      "236  5.236417e-31 -7.947436e-40 -2.368519e-38 -1.923343e-38  3.542278e-37   \n",
      "237 -1.138815e-32 -7.947436e-40 -2.368519e-38 -1.923343e-38 -1.252804e-38   \n",
      "238 -4.097250e-34 -7.947436e-40 -2.368519e-38 -1.923343e-38 -1.252804e-38   \n",
      "239  7.086223e-35 -7.947436e-40 -2.368519e-38 -1.923343e-38 -1.252804e-38   \n",
      "\n",
      "              5             6             7             8             9    \\\n",
      "0    1.164070e-02  6.781843e-03 -4.054806e-03  2.004379e-02 -1.906049e-02   \n",
      "1    4.619203e-03  1.535290e-03 -3.895142e-04  1.142522e-02 -1.057534e-02   \n",
      "2    1.352314e-03  1.670132e-04  3.972764e-06  5.721457e-03 -5.098217e-03   \n",
      "3    2.820615e-04  4.692398e-06  2.865838e-07  2.486690e-03 -2.107970e-03   \n",
      "4    4.010972e-05 -2.705608e-07 -1.889994e-08  9.328733e-04 -7.431720e-04   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "235 -8.033568e-36  6.973815e-26  9.064655e-25 -2.144264e-29  4.045706e-30   \n",
      "236  3.082496e-37 -4.152562e-26 -7.719432e-26  9.226610e-31 -1.377120e-31   \n",
      "237  4.751838e-38  2.275678e-27  2.898549e-27 -3.930522e-32 -4.928464e-33   \n",
      "238  2.459437e-38 -7.940587e-29 -6.001314e-29  1.544651e-33  1.041607e-33   \n",
      "239  2.459437e-38  1.943582e-30 -3.534868e-31 -4.840947e-35 -9.043785e-35   \n",
      "\n",
      "     ...       490       491       492       493       494       495  \\\n",
      "0    ...  0.006021 -0.127967  0.052438 -0.064922  0.072371  0.126689   \n",
      "1    ...  0.008247 -0.139440  0.064022 -0.068040  0.085305  0.137721   \n",
      "2    ...  0.006106 -0.145865  0.067857 -0.065784  0.090021  0.143485   \n",
      "3    ...  0.004701 -0.149980  0.069713 -0.064992  0.092484  0.147787   \n",
      "4    ...  0.004469 -0.152648  0.070793 -0.064558  0.093941  0.151213   \n",
      "..   ...       ...       ...       ...       ...       ...       ...   \n",
      "235  ... -0.081406 -0.097221  0.171127 -0.024415 -0.085914 -0.131095   \n",
      "236  ... -0.082231 -0.097789  0.172101 -0.024641 -0.085506 -0.131324   \n",
      "237  ... -0.082532 -0.098259  0.173540 -0.024638 -0.085313 -0.131584   \n",
      "238  ... -0.082908 -0.098897  0.174534 -0.024718 -0.085297 -0.132085   \n",
      "239  ... -0.083487 -0.099466  0.175404 -0.025015 -0.085470 -0.132613   \n",
      "\n",
      "          496       497       498       499  \n",
      "0   -0.033197 -0.056366  0.036780 -0.131857  \n",
      "1   -0.043669 -0.064507  0.042256 -0.144895  \n",
      "2   -0.052058 -0.068286  0.041063 -0.156465  \n",
      "3   -0.058621 -0.071419  0.039972 -0.166959  \n",
      "4   -0.064796 -0.073954  0.039378 -0.176938  \n",
      "..        ...       ...       ...       ...  \n",
      "235  0.168902 -0.104591  0.011491  0.087071  \n",
      "236  0.170978 -0.104505  0.013919  0.087295  \n",
      "237  0.172634 -0.104351  0.017373  0.087824  \n",
      "238  0.174363 -0.104456  0.020114  0.088087  \n",
      "239  0.176316 -0.104724  0.022956  0.088351  \n",
      "\n",
      "[240 rows x 417500 columns]\n"
     ]
    }
   ],
   "source": [
    "trainAccArr = []\n",
    "for key,values in enumerate(trainAllacc):\n",
    "    trainAccArr.append(trainAllacc[key])\n",
    "trainLossArr = []\n",
    "for key,values in enumerate(trainAllloss):\n",
    "    trainLossArr.append(trainAllloss[key])\n",
    "print(all_df)\n",
    "train_acc_df = pd.DataFrame(trainAccArr)\n",
    "train_acc_data = np.array(train_acc_df).flatten()\n",
    "\n",
    "train_loss_df = pd.DataFrame(trainLossArr)\n",
    "train_loss_data = np.array(train_loss_df).flatten()\n",
    "\n",
    "print(f'Shape of Loss:{np.shape(train_loss_data)} & Shape of Acc: {np.shape(train_acc_data)}')\n",
    "t1 = all_df\n",
    "print(pd.DataFrame(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2818b2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pcaOps(paramDF,itr):\n",
    "    pcaOperation =  PCA(n_components=2)\n",
    "\n",
    "    pcaVal = pcaOperation.fit_transform(paramDF)\n",
    "    itrData = np.full((pcaVal.shape[0],1),itr)\n",
    "    pcaDf = pd.DataFrame(np.append(pcaVal,itrData,axis=1),columns=['x','y','Itr No.'])\n",
    "\n",
    "    return pcaDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "543c6d00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            x         y  Epoch  Iteration        Acc      Loss\n",
      "0    5.375694  2.385643      0          0  33.861445  1.740279\n",
      "1    6.642767  2.931668      1          0  82.955129  0.692817\n",
      "2    7.388725  3.262196      2          0  88.271317  0.419865\n",
      "3    7.921731  3.500775      3          0  90.326108  0.338610\n",
      "4    8.351595  3.691927      4          0  91.646955  0.295383\n",
      "..        ...       ...    ...        ...        ...       ...\n",
      "235 -3.698381 -5.050765     25          7  97.424987  0.087495\n",
      "236 -3.696678 -5.046556     26          7  97.665960  0.084108\n",
      "237 -3.695796 -5.044141     27          7  97.786623  0.081126\n",
      "238 -3.693208 -5.040688     28          7  97.868424  0.077604\n",
      "239 -3.692414 -5.035863     29          7  97.982265  0.074508\n",
      "\n",
      "[240 rows x 6 columns]\n",
      "            x         y  Epoch  Iteration        Acc      Loss\n",
      "0    7.388725  3.262196      2          0  88.271317  0.419865\n",
      "1    8.710108  3.849814      5          0  92.425957  0.266176\n",
      "2    9.508209  4.206992      8          0  94.144653  0.208080\n",
      "3    9.996516  4.430327     11          0  95.170278  0.170342\n",
      "4   10.294191  4.569250     14          0  95.877115  0.143619\n",
      "..        ...       ...    ...        ...        ...       ...\n",
      "75  -3.645247 -4.988532     17          7  96.408392  0.124505\n",
      "76  -3.678573 -5.030793     20          7  96.902655  0.108245\n",
      "77  -3.693161 -5.048370     23          7  97.351896  0.095136\n",
      "78  -3.696678 -5.046556     26          7  97.665960  0.084108\n",
      "79  -3.692414 -5.035863     29          7  97.982265  0.074508\n",
      "\n",
      "[80 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "t1 = np.array(t1)\n",
    "pca = PCA(n_components=2)\n",
    "new_data = pca.fit_transform(t1)\n",
    "\n",
    "allEpochDf = pd.DataFrame(new_data,columns=['x','y'])\n",
    "\n",
    "eps_each_time = [i for i in range(max_epochs)] * 8\n",
    "times = np.repeat([i for i in range(8)],max_epochs)\n",
    "\n",
    "allEpochDf['Epoch']=eps_each_time\n",
    "allEpochDf['Iteration']=(times)\n",
    "allEpochDf[\"Acc\"] = train_acc_data\n",
    "allEpochDf[\"Loss\"] = train_loss_data\n",
    "\n",
    "print(allEpochDf)\n",
    "epoch3Df = allEpochDf.loc[(allEpochDf['Epoch']+1)%3 == 0]\n",
    "epoch3Df = epoch3Df.reset_index(drop=True)\n",
    "print(epoch3Df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3acf6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = np.array(epoch3Df.Acc)\n",
    "mv = []\n",
    "for i in range(len(test)):\n",
    "    mv.append(str(int(test[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b783f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGxCAYAAABfrt1aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8J0lEQVR4nO3dd3xUVf7/8feQkNBChAQIkRCKEBAVEFCIC0jvdZHi0kRdBcGCfrHtLqCrKEvQ7w/Esl9kVXRRFEITJSxNKS4oKNKkE0MooSUUg8D9/XHIJJOekJk7k7yej8c8Zu6Zc2c+dy5m3p577h2HZVmWAAAAbFDK7gIAAEDJRRABAAC2IYgAAADbEEQAAIBtCCIAAMA2BBEAAGAbgggAALANQQQAANiGIAIAAGxDEAHy61//khyO9Ju/v1SjhvTAA1JCQtb+Bw5IY8dK9etLZctK5cpJjRpJf/lL9v0lqX9/89pjxxastkOHpB49pMqVzfpPPlnAjfOAtM9vy5aie82RI6VatYru9ex26JD5jP71r4Kvu2aNWXfNmqKtCXAzf7sLAHzOnDlSgwbSpUvSunXSlCnS2rXS9u1S+fKmz9Kl0uDBUmioCRVNm5ovie3bpfffl5Ytk7ZudX3dEyfMepL08cfStGlSmTL5q+mpp6TvvjOvHRYmVa9edNsLAG5EEAEK6rbbpObNzeN27aSrV6WXX5ZiY6U//Uk6eNCEkPr1pdWrpeDg9HXbt5cef1xauDDr6374ofT772ZkY9kyacEC6f7781fTzz9Ld90l9e17o1tnXL0qXbkiBQYWzesBQA44NAPcqJYtzf3hw+Z++nTpwgVp1izXEJLG4TCHYDJ7/32pWjXpgw/MoZz338/7vdOG4/ftk5YvTz9sdOiQef7IEWnoUKlqVRMqGjaUYmKka9fSXyPtcMDUqdLf/y7Vrm36rl6d/Xved585xJRRr17mNebPT2/74QfTtmSJa9+UFGn0aDNaFBJiPoujR137XLtm6mnQwNRStao0fLj06695fyaWZT77Jk3M51ipkjRggDlUlpdJk0zNP/1ktjM42BzuGj/eBLM9e6SuXaWgIHNIaOrUrK+Rn89cMts8cKB5reBgadAg6dix7OvaskXq3dvUUqaMGWH77LO8twfwAQQR4Ebt22fuq1Qx9ytWmECRFlDyY8MGadcu82UbEiL98Y/SqlVmdCU3d94pbdxoDsfcc495vHGjOTRz8qQUHW3qefllafFiqWNH6Zlnsp+D8v/+n3nPadNMqGnQIPv37NhR2rlTSkw0y1eumENTZctKcXHp/VauNPNo7r3Xdf2HHpJKl5Y++cR8ka9ZY764Mxo9Wnr2WalTJ1P3yy9LX31lticpKffP5JFHzByZjh3NKNWsWdKOHWbd48dzXzfNwIFS48bSF19IDz8svfGGOfzVt68ZsVq40IxuPfusGblKk9/P/NIl075ihTm0N3++2YeDBmWtZfVqs2/PnpXeeUdatMiErEGDCjeXBPA2FoD8mTPHsiTL2rTJsn7/3bJSUixr6VLLqlLFsoKCLOvYMdOvTBnLatmyYK89apR57V27zPLq1Wb5r3/N3/qRkZbVo4dr23PPmdf47jvX9tGjLcvhsKw9e8zywYOmX926lnX5ct7vtW+f6f/hh2b522/N8oQJllW7dnq/Tp0sKzo6fTnt8xszxvX1pk417YmJZnnXruz7ffedaX/hhfS2ESPMtqfZuNH0iYlxXTc+3rLKljU15mbixOzXb9LEtC9YkN72++9m3/fvn96W38/87bdNv0WLXPs9/LBpnzMnva1BA8tq2tS8X0Y9e1pW9eqWdfWqWU77N7N6de7bCHgZRkSAgmrZ0vwffVCQ1LOn+T/Z5cvNKEhhnD9vhtmjo9NHIdq2lerWNf/Hm3lIP79WrZJuvdXMHclo5Ehz+GLVKtf23r3NduWlbl1zWGLlSrMcFyfdfrsZ1Th4UNq/X0pNlb791vxff2a9e7su33GHuU87tJV2SGjkSNd+d91lDnP85z8517Z0qTm0MnSoGalJu4WFmRGO/J5R0rOn63LDhuZ1u3VLb/P3l265Jb1uKf+f+erV5t9P5s8i85ygffuk3bvN3CPJdZu6dzejUnv25G+bAC/FZFWgoD780Hwx+fub8JH5DJWaNfM+pJLRp5+aMDJwoBl+TzNwoBm2j4uTunQpeJ2nTmV/amt4ePrzGRXkTJsOHcyhEskEkk6dTBipVs0s16uXfvghs5AQ1+W0CbGXLrnWlV094eGuX/yZHT9uvvBzCoV16uS8bkaVK7suBwSY068zn8UUECAlJ6cv5/czP3Uq+xrDwlyX0w4lPfOMuWUnr0NVgJcjiAAF1bBh+lkz2enSRZoxQ9q0KX/zRGbPNvdPPpn99T9mzy5cEAkJSZ/HkVHaxNDQUNd2hyP/r92hg6nrv/81pw3/5S+mvX17E5wOH5YqVCjYPJmMdUum9ho1staeue6MQkPNdnzzTfZn/Lj7LKD8fuYhIeazyyzzZNW0/s8/n/0EZ0mKiipcrYCX4NAMUNSeespcT2TMGOncuazPW1b66bu7dpnJpX/8oxmuz3zr0MFMTsw8epEfHTqYSaU//ODa/uGH5su6XbuCv2bG13Y4pL/+VSpVSmrTxrR37Gjqjoszbfk51JNZ+/bmfu5c1/bNm83n1aFDzuv27Gk+34QEExYz326/veD1FER+P/N27czZQ4sXu/b75BPX5agoM7r044/Zb0/z5uYQD+DDGBEBilrt2tK8eeashiZN0i9oJpkvqfffN1+W/fqlj4ZMmJB1XoFkvqz+8x/zpfzEEwWr46mnzBdgjx7SSy9JkZHm+iSzZpmzUurXL/w2Vq1qrqeyYoX5Ui1XzrR37CidPm1u06cX7rWjoqQ//9mMKpUqZeZlHDpkQk9EhNmunNxzj1n3gQfMKa9t2phQmJho5qzcfrvZdnfJ72c+fLg5E2f4cOmVV0zY+PJL6euvs77mu++az6BLFzPX5Oabzee7a5cJPBlPmQZ8EEEEcIeePc1VVGNizCmX8fHmS7V2bXMdinHjzMXLPvrIhJXsQohkJiTWqGECS0GDSJUq5rTg5583t+RkM0di6lRzXYwb1bGj2caM80Bq1jRfqnv3Zj8/JL/efttMip09W3rrLXOdja5dzZyZzHNMMnv3XXNI6N13TQC4ds3M0bjnnpw/56KS38+8XDkzcfWJJ6TnnjOjJZ07mwAbHe36mu3amcM4r7xiDt2dOWM+g1tvNfOIAB/nsCzLsrsIAABQMjFHBAAA2IYgAgAAbEMQAQAAtiGIAAAA2xBEAACAbQgiAADANl59HZFr167p6NGjCgoKkqMgl58GAAC2sSxLKSkpCg8PV6lSuY95eHUQOXr0qCIiIuwuAwAAFEJ8fLxqZP7NqEy8OogEXf8Nhfj4eFWsWNHmagAAQH4kJycrIiLC+T2eG68OImmHYypWrEgQAQDAx+RnWgWTVQEAgG0IIgAAwDYEEQAAYBuCCAAAsA1BBAAA2IYgAgAAbEMQAQAAtiGIAAAA2xBEAACAbQodRNatW6devXopPDxcDodDsbGxLs+PHDlSDofD5dayZcsbrRcAABQjhQ4iFy5cUOPGjTVz5swc+3Tt2lWJiYnO25dfflnYt4MXS0qS+vaVHA5p6FBpxw6pUSMpOFiqVElq00bavdvuKgEA3qjQvzXTrVs3devWLdc+gYGBCgsLK+xbwEcMHCitXm0ef/yxtGCBdOlS+vPffCP16iUtWSI1aGBPjQAA7+TWOSJr1qxR1apVVb9+fT388MM6ceJErv1TU1OVnJzscoP3SwshaTKGkDT79kmHDnmkHACAD3FbEOnWrZs+/vhjrVq1SjExMdq8ebPat2+v1NTUHNeZMmWKgoODnbeIiAh3lQcbtG9vdwUAAG/jsCzLuuEXcTi0cOFC9e3bN8c+iYmJioyM1Lx589S/f/9s+6SmproEleTkZEVEROjcuXOqWLHijZYJNwkIkH7/Pe9+v/0mBQa6vx4AgL2Sk5MVHBycr+/vQs8RKajq1asrMjJSe/fuzbFPYGCgAvmm8jnNmkmbNuXep149QggAICuPXUfk1KlTio+PV/Xq1T31lvCQxMS8++zdK/34o/trAQD4lkIHkfPnz2vbtm3atm2bJOngwYPatm2bjhw5ovPnz+uZZ57Rxo0bdejQIa1Zs0a9evVSaGio+vXrV1S1w0s88UTuz5cvb+5zGQwDAJRQhQ4iW7ZsUdOmTdW0aVNJ0vjx49W0aVP97W9/k5+fn7Zv364+ffqofv36GjFihOrXr6+NGzcqKCioyIqHd3jqKenhh3N+/sIFcz9ypDRkiEdKAgD4iCKZrOouBZnsAnutW2eCxtGjUsYTo8qUMZNU0/j5SVeueLw8AIAHFeT7m9+aQZFo00Y6cCA9hIwbZ+4zhhBJeuYZz9YFAPBuBBEUqetH6jRjRvbPv/66uQQ8AAASQQRFbNq0rG0xMdL27VLZsmZ51y7P1gQA8F4eu44ISoYFC1yXmzaVnn7ata1FC8/VAwDwboyIoEj16ydVqGAev/yyFBsrRUeb5Xr1pBUrpMhI28oDAHgZggiKVIcOUkqKZFnSY49Jf/ubtHWr9Ne/mkMynTpJy5ZJXbuakAIAKNk4NAO3GT5cWrrUPH75ZXNhsyFDpJ49TVvlylIuP08EACgBGBGB26SFkGHDzP2MGVLr1vbVAwDwPlzQDG7TsKG0e3fOzzsc0tmzErsWAIoXLmgGr5DdqbwZWZa0fLlnagEAeCfmiMBtMk9GffxxM/qxZYv01VemrWNHj5cFAPAiBBG4zfDh5jdoLl+WJk82y5K0c6eZuDpsmBQSYm+NAAB7MUcEAAAUKeaIAAAAn0AQAQAAtiGIAAAA2xBE4FGnTkn9+5triIwaJZ04IfXubZZr1ky/CBoAoGRgsio8qlcv17ARHS1t2ODaJyFBCg/3bF0AgKLDZFV4rbQQMnq0ud+wQZo0Sbr55vQ+J096vCwAgE0IIvCoxo3N/dtvp7cFBppRkDRVq3q2JgCAfQgi8KjsLvteqZI0dWr68vr1nqsHAGAvrqwKj1qwwHX5jjukRx91bUsbNQEAFH+MiMCjBgyQKleWAgKkf/xDWrJEatfOPBcVJX35pVSvnr01AgA8hxEReFT79uYU3oxWrbKnFgCA/RgRAQAAtiGIAAAA2xBEAACAbQgiAADANgQRAABgG4IIAACwDUEEAADYhiACAABsQxABAAC2IYjAqxw/LrVuLTkcUvPm0k8/SV27mmWHQ/rgA7srBAAUJYdlWZbdReQkOTlZwcHBOnfunCpWrGh3OfCA3r3N78/kZudOqWFDz9QDACi4gnx/MyICr5IWQiZPTm+bPl3asSN9ef9+z9YEAHAfggi8SqtW5n7ixPS2wYOlCRPSl9u08WxNAAD3IYjAq8TEZG0bPVpatsw8njtXCgrybE0AAPchiMCrLFjguhwSIi1alL48dKi0dKlnawIAuA9BBF6lXbv0x6NGmdARFZXeVqmSFBnp+boAAO7hb3cBQEbdu0uZz+PavdueWgAA7seICAAAsA1BBAAA2IYgAgAAbEMQAQAAtiGIAAAA2xBEAACAbQgiAADANgQRAABgG4IIAACwDUEEAADYhiACAABsQxABAAC2IYgAAADbEEQAAIBtCCIAAMA2BBEAAGAbgggAALANQQQAANiGIAIAAGxDEAEAALYhiMDnnT4tDRkiORxSnz5SYqI0Z44UFCSFh0tLlthdIQAgJw7Lsiy7i8hJcnKygoODde7cOVWsWNHucuClRo6UPvggfTk0VEpKcu2TkGBCCQDA/Qry/c2ICHzenj2uy5lDiCSdOOGZWgAABUMQgc/z88u7D6MhAOCdCCLweevXm/s2bVzbM4aPdes8Vw8AIP8IIvB5HTua+8xh4+jR9MeNGnmuHgBA/hFE4PPmzpXuu8887tFDOnxYmjVLqlBBCguT5s+XGja0t0YAQPY4awYAABQpj5w1s27dOvXq1Uvh4eFyOByKjY11ed6yLE2aNEnh4eEqW7as7r33Xu3YsaOwbwcAAIqhQgeRCxcuqHHjxpo5c2a2z0+dOlXTp0/XzJkztXnzZoWFhalTp05KSUkpdLEAAKB48S/sit26dVO3bt2yfc6yLL355pt68cUX1b9/f0nSBx98oGrVqumTTz7RI488ku16qampSk1NdS4nJycXtjwAAOAD3DJZ9eDBgzp27Jg6d+7sbAsMDFTbtm21YcOGHNebMmWKgoODnbeIiAh3lAcAALyEW4LIsWPHJEnVqlVzaa9WrZrzuew8//zzOnfunPMWHx/vjvIAAICXKPShmfxwOBwuy5ZlZWnLKDAwUIGBge4sCQAAeBG3jIiEhYVJUpbRjxMnTmQZJQEAACWXW4JI7dq1FRYWpri4OGfb5cuXtXbtWkVHR7vjLQEAgA8q9KGZ8+fPa9++fc7lgwcPatu2bapcubJq1qypJ598Uq+++qrq1aunevXq6dVXX1W5cuV0//33F0nhAADA9xU6iGzZskXt2rVzLo8fP16SNGLECP3rX//ShAkTdOnSJY0ZM0ZnzpzR3XffrRUrVigoKOjGqwYAAMUCl3gHAABFyiOXeAcAALhRBBEAAGAbgggAALANQQQAANiGIAIAAGxDEAEAALYhiAAAANsQRAAAgG0IIgAAwDYEEQAAYBuCCAAAsA1BBAAA2IYgAgAAbEMQAQAAtiGIAAAA2xBEAACAbQgiAADANgQRAABgG4IIAACwDUEEAADYhiACAABsQxABAAC2IYgAAADbEEQAAIBtCCIAAMA2BBEAAGAbgggAALANQQQAANiGIAIAAGxDEAEAALYhiAAAANsQRAAAgG0IIgAAwDYEEQAAYBuCCAAAsA1BBAAA2IYgAgAAbEMQAQAAtiGIAAAA2xBEAACAbQgiAADANgQRAABgG4IIAACwDUEEAADYhiACAABsQxABAAC2IYgAAADbEEQAAIBtCCIAAMA2BBEAAGAbgggAALANQQQAANiGIAIAAGxDEAEAALYhiAAAANsQRAAAgG0IIgAAwDYEEQAAYBuCCAAAsA1BBAAA2IYgAgAAbEMQAQAAtiGIAAAA2xBEAACAbQgiAADANgQRAABgG4IIAACwDUEEAADYxq1BZNKkSXI4HC63sLAwd74lAADwIf7ufoNGjRpp5cqVzmU/Pz93vyUAAPARbg8i/v7+jIIAAIBsuX2OyN69exUeHq7atWtr8ODBOnDgQI59U1NTlZyc7HIDAADFl1uDyN13360PP/xQX3/9tf75z3/q2LFjio6O1qlTp7LtP2XKFAUHBztvERER7iwPAADYzGFZluWpN7tw4YLq1q2rCRMmaPz48VmeT01NVWpqqnM5OTlZEREROnfunCpWrOipMgEAKJ6SkqRBg6RVq8xyeLjk5yfFx6f3KV9e+uwzqXv3Qr9NcnKygoOD8/X97fY5IhmVL19et99+u/bu3Zvt84GBgQoMDPRkSQAAFH9nzkiPPy7NnevafvRo1r4XLkg//HBDQaQgPHodkdTUVO3atUvVq1f35NsCAFCyDR+eNYTkJpujFu7i1iDyzDPPaO3atTp48KC+++47DRgwQMnJyRoxYoQ73xYAgJItKUnq00dyOKT775eWLs29f9my6Y+bN5fKlXNvfRm49dDMr7/+qiFDhigpKUlVqlRRy5YttWnTJkVGRrrzbQEAKNlGjZKWLDGP//1vM+/jwoWc+1+6lP548mT31paJRyerFlRBJrsAAFCiJCVJDz4oLV4sDRkizZwpDR0qbdwonT1r+owbJ82YkfvrlColXbtmHvv7S7//fsOlee1kVQAAUEQyj3qcPClluJK5pLxDiJQeQiTppZeKrr584kfvAADwRWkhZNw4c585hGRWubKUcXSiQwepVi2palXpllukr7+Wnn/eLaXmhiACAIA3OHNGGjnSTBT929+kq1dN+7JlUteuUmysa/+mTc19fkY9mjaVTp+WMl6xfPZs6eBB6fhxae9eqXPnotiKAiOIAADgDYYPlz74wEwcffllado06cgRqWdPM1rx2Weu/adNy/oaMTHS9u1SQEB628svmxATHW2W69WTVqyQvOTEEeaIAADgDdJOsR02TProIzPSMWtWzv0XLHBdbtpUevpp17ZDh9IDx/r1RVZqUWJEBAAAb9Cggbn/6CNzn5BgRkTSzJvnemilXz+pQgXz2MtHPXLDiAgAAN5g2jRzGCYnliUtX25+K0Yyk01TUlz7eOmoR24IIgAAeIPMk1Eff9yc5bJli/TVV6atY0ePl+VuHJoBAKCwjh+XWrc2l1Jv3lz66SdzhovDYW4ffJD/1xo+XKpf35xS+8EH0v/+rznkEhMj/fGPJqiEhLhrS2zDlVUBACis3r3Tr+eRk507pYYNPVOPlyjI9zcjIgAAFFZaCMn4+yzTp0s7dqQv79/v2Zp8DEEEAIDCatXK3E+cmN42eLA0YUL6cps2nq3JxxBEAAAorJiYrG2jR5uroUrS3LlSUJBna/IxBBEAAAor80XFQkKkRYvSl4cOTb9QGbJFEAEAoLDatUt/PGqUCR1RUeltlSr5xEXF7MR1RAAAKKzu3c2FxjLavdueWnwUIyIAAMA2BBEAgG87fVoaMsRcQKxPHykxUZozx0wSDQ/P+zofsBUXNAMA+LaRI12vYBoaKiUlufZJSDChBB7BBc0AACXHnj2uy5lDiCSdOOGZWlBgBBEAgG/z88u7D6MhXosgAgDwbevXm/vMVzDNGD7WrfNcPSgQgggAwLd17GjuM4eNo0fTHzdq5Ll6UCAEEQCAb5s7V7rvPvO4Rw/p8GFp1iypQgUpLEyaP7/E/fqtL+GsGQAAUKQ4awYAAPgEgggAALANQQQAANiGIAIAAGxDEAEAALYhiAAAANsQRAAAgG0IIgAAwDYEEQAAYBuCCAAAsA1BBAAA2IYgAgAAbEMQAQAAtiGIAAAA2xBEAACAbQgiAADANgQRAABgG4IIAACwDUEEAADYhiACAABsQxABAAC2IYgAAADbEEQAAIBtCCIAAMA2BBEAAGAbgggAALANQQQAANiGIAIAAGxDEAEAALYhiAAAANsQRAAAgG0IIgAAwDYEEQAAYBt/uwsAkLt9K/bp464fS5ZZrnVvLR1ac8ilT+S9kRqxaoQcDofnCwSAG8CICODlMoYQSVlCiCQdXnNYOz/f6bmiAKCIMCICeDsr7y6S9PnAz7WwzEJd/e2qJKnPv/qoyYgm7qsLAIoAIyJAMZIWQiRp0chFOrnrpI3VAEDeCCKAl6vbrW6h133vzvcUEx6jPUv2FGFFAFB0CCKAlzu06lCh173y2xWdTzyveb3nKeVoStEVBQBFhCACeLk6neoUyetcOHGhSF4HAIoSQQTwcvcvuV/DVg6Tf1kzt9wvwK9QrxMUHlSUZQFAkSCIAD6gToc6evHii5poTdSflv9JZSuXVSn/bP7zzXQZkXJVyjkfH1532M1VAkDBcfou4GNqt6+tCacmuLR91OkjHVh5IMupvhdPXnQ+rtKoiifKA4AC8ciIyKxZs1S7dm2VKVNGzZo10zfffOOJtwVKjH5z++nW+26VJNXrUU9PHn5S3Wd1V0CFAFUIq6D75t+nKg0JIgC8j8OyrHxeLqlwPv30Uw0bNkyzZs3SPffco3fffVf/93//p507d6pmzZq5rpucnKzg4GCdO3dOFStWdGeZAACgiBTk+9vtQeTuu+/WnXfeqbffftvZ1rBhQ/Xt21dTpkzJdV2CCAAAvqcg399uPTRz+fJlff/99+rcubNLe+fOnbVhw4Ys/VNTU5WcnOxyAwAAxZdbg0hSUpKuXr2qatWqubRXq1ZNx44dy9J/ypQpCg4Odt4iIiLcWR4AALCZRyarZv5pcsuysv258ueff17nzp1z3uLj4z1RHgAAsIlbT98NDQ2Vn59fltGPEydOZBklkaTAwEAFBga6syQAAOBF3DoiEhAQoGbNmikuLs6lPS4uTtHR0e58awAA4APcfkGz8ePHa9iwYWrevLlatWql9957T0eOHNGjjz7q7rcGAABezu1BZNCgQTp16pReeuklJSYm6rbbbtOXX36pyMhId781AADwcm6/jsiN4DoiAAD4Hq+5jgiQ7rSkITK/ytZHUqKkOZKCJIVLWmJfaQAA2/Cjd/CQ8ZLmXX+8WNIGSUnXl89L6i0pQSaUAABKCkZEkA/HJbWWGc1oLuknSV2vLzskfZCP19iTaTkpmz4nbqBGAIAvIoiUaGckjZRUTtLfJF293r5MJmjEXl9+WNK31x9/L6mxpK8zvM5ISbvyeC+/fNTDaAgAlDQEkWIvSWZOhkPS/TJzNbpLqiSpqsxoxiVJL0uaJumIpJ4yQeOz66+RNn9jcobXnS5pR4bl/XnUsf76fZtM7RnDx7o8XgMAUNwQRHzeGUkDZXZlKUmPSWqn9MMmVWTmZEjSvyXdKmm5pLOSrlxvH3b9fobMIZjMWl2/n5ihbbCkCRmWMweMzDpev88cNo5meNwoj9cAABQ3BBGf11PSfEnW9dssSWty6X88m7aPrt8nyIyIpJknKVlSTDbrjJY5hCNJc2XOfsnNXEn3XX/cQ9Lh67VWkBR2fRsa5vEaAIDihrNmfN4GN762JTN6siVTe4ikRRmWh0qqKKlXLq9VTemHetKMvn4DAJRUjIiUSP8jabuyTiB9XNJfZCaqpukoc6gnzShJSyVFZWirJIkr5QIACo4RkRLpH9dvaWpIekXS8OvLOyWVl5k7EiIzuTXzBXh3u7lGAEBJQBDxeQ1U8FBwp6QfJNWT9JakTpmev1XS5zdeGgAAeSCI+LyjmZZvknRO6SMYoTKn6e5UzsEDAAB7MEfE5w3K8PgWmbNiBsqcuvuApJMy1/uwJP0iQggAwJsQRHzee0o/dXevpACZ026vSXrfxroAAMgbQQQAANiGIAIAAGxDEAEAALYhiAAAANsQRAAAgG0IIgAAwDYEEQAAYBuCCAAAsA1BBAAA2IYgAgAAbEMQAQAAtiGIAAAA2xBEAACAbQgiAADANgQRAABgG4IIAACwDUEEAADYhiACAD4q9exZrX/mGX3SqJHWjh2rSydPav/ChfqsRQstuPde/bp6td0lAnlyWJZl2V1ETpKTkxUcHKxz586pYsWKdpcDAF5l4wsv6OCiRc7lwJtuUurZsy59+q5erXJVq3q4MpR0Bfn+ZkQEAHxU8qFDLsuZQ4gkpZ465ZligEIiiACAj3KUyvtPeFlGQ+DlCCIA4KOStm6VJFVt3tylPWP4OLFli0drAgqKIAIAPiqsVStJWcPGpRMnnI+Db7nFozUBBUUQAQAf1eq111SzSxdJUnibNuoTF6cWf/2r/MuVU5nQUP1h+nQF161rc5VA7jhrBgBKuEtJSfr2qad08ocfVLlRI9398svaFhOjxPXrJUktX3lFdfr2tbdI+BTOmgGAEuz0pdMa8sUQOSY71OfffZSYkqg5W+coaEqQwmPCtWTPEpf+/504USd/+MGsu2OHlvfv7wwhkrTpxRd1bv9+j24DSg6CCAAUM+O/Hq95P8+TJC3+ZbHueOcOjVo8Sucvn1fi+UT1ntdbR1OOOvsnrFkjSbr9scecbXdOmKAeGa5Rcj4+3jPFo8QhiABAMbPn1B6X5aSLSVn6nLiQPqE1tHFjSdL2t95ytkV2766t06c7lzOfmQMUFYIIABQzfg6/PPuEB4U7HzedMCHL8/+dPFlH166VJLV6/XX5ly8vyVw0bd3jj+uTRo206S9/0W+nTmntY4/pk0aNFNuhg3N0BcgvgggAFDPr4838jjaRbVzaM4aPdYfXOR//unKlS7+Am25SQobfqdn47LPOgLHxhRf063/+I0k6sHCh1j3+uPO5i8eOae1jj+lihtOHgbwQRACgmOlYp6Mk17AhyWVeSKMqjZyPq951l/NxnX79dO+sWQqqVcvZFlCxosqHmxCTNkpSb9AgSVLStm26/bHHVLZaNWf/1NOni2hLUBJw+i4AFDPHzx/XuOXjNH/nfPWo10OzeszSsl+WacLKCaoQUEEzus3QgFsHFOq1v+zfX2f3uM5Bafzkk/rxzTedy6UCA3XrAw/otjFjVMrPTwlr1+qXjz/WLYMGKaJDhxvZNPiIgnx/E0SADC5duqQvv/xSP//8s+rXr6+ePXtq3759+uqrrxQQEKCePXsqKirK7jIB2xzbuFGrHnrIpa3FxIn6/fx5bYuJcWlv8tRTiuzeXYs6dZJkJsDe849/eKxW2IcgAhRSbGysfvzxR+dyuXLldPHiRZc+48ePV1BQkKdLA7zC5pde0t5PP3Uu31S/vs7+8otLn5vbt1fCqlXOwzWXjh+XJJWrXl3dPv9cG559Vkk//ijr2jXd3KaN7pk2zXMbAI8oyPe3v4dqAnzCqUw/mZ45hEjShQsXCCIoUY6fP64B8wfo2yPfqu+VJhocVEGXL57XvEbH9V3EL3rTv6NK7TyiUv7+unblihJWrZKUHkDSXExM1OJu3fR7crKz7fDy5Tq+ZYt+O3nS2Va6QgVFT52qm9u29cwGwlYEESADh8ORZx9CCEqah5c8rG+PfCtJivXfpthurs8Pu+0r7Zy1UxV3ntDaMWNyfa2MISRNxhAiSb+fP6/Tu3YRREoIzpoBMoi/fvXIyMhIl/aM4ePw4cMerQmw25JfzCXhJ9872dk2vfN07Rizw7m8/8x+52m9aeoPHaoyoaGFes+Gw4cXaj34HoIIkEGdOnUkZQ0bKSkpzsdVqlTxaE2A3VrVaCVJmrhmorNt8G2DNSEu/UJobSLbqHafPgqqVUvlb75ZLV99Vc2ff17Rr79e4PcLqlVL/uXK3Xjh8AkcmgEy6Nevn5YvX66dO3eqXr166tGjh3755RetXLlSAQEB6tatG0EEJU5M5xhFvx/t0jZ62Wgt27tMkjS331wFBQSpYrNm6rVsmUu/+EwXS8uP4Lp1C18sfA5BBMigQoUKuu+++1zaWrRooRYtWthUEWC/BbsWuCyHlA3Roj3pP4g3dOFQVQysqF5RvXTq4ik9vORhLdy9UA80eUAT7rlPv38xT6UvW/o19KpqJOV9+fn6Q4YU+TbAe3FoBgCQq3a12zkfj2oySkvvX6qokPTr6VQqU0mRN5l5VSNjR2rh7oWSpDnb5ujBA3/XyL479KeBOzXn9nilBFxxee1Kt94qv+u/YyOZq7yGtWrlzs2Bl+E6IgCAIuOYbM48G918tN7e8rYkaVLbSfrnD/9UQkqCJGnbI9vUOKyxbTXC/biOCJDB+fPnNX/+fB05ckTVq1dXnz59FBcXp/3790uS+vTpoyZNmthbJFBMNK7WWD8e/9EZQiQp0D/QGUIk6ZVvXtHSX5bqmehnNLHtRPmV8tOyX5Zpxn9n6NHmj6pvg742VA67cGgGxd6SJUt05MgRSVJiYqLeeecdZwiRpEWLFulkpusYACicaZ2zXiW1UplKmtpxqnN5/s75unTlkl5e97KmbZimI+eOqOe/e+rr/V/rsx2febJceAGCCIq9X65ffvree+91tnXu3FljMlx46cyZM54uCyiWMk9svaPaHXp02aOasHKCS/uwO4ZJkmb8d4aiZ6efkbMhfoNOXzqt7h93V6XXK6nilIoa8jmTV4szggiKvRo1akiS1qxZ42y77bbbFBcX51zOfAEzAIUz4NYBqly2sgL8AvSPTv/QkiFL1K6WmewaFRKlGhXNf48f/fSRJCkhJcHlsM3hc4fVcGZDLd+3XGd/O6uUyyn6bMdn6jq3qwJeDtATXz0hL57aiEJgsiqKvfj4eL3//vsubVFRUdpz/afM+/Xrp9tvvz1fl3cHcGOW/bJMPf/d84Zew9/hr9jBsepRv0cRVYWiVpDvb0ZEiqnTp09ryJAhcjgc6tOnjxITEzVnzhwFBQUpPDxcS5YssbtEj9m1a5fLctmyZZ0hRJIWLlzoPHwDwL1id8e6LD9+1+MKKx9WoNe4Yl3R5qObi7Aq2IkRkWJq5MiR+uCDD5zLoaGhSkpKcumTkJCg8PBwT5fmNhcvXtSSJUu0e/duNWnSRB07dtTixYtdQkaTJk3UrFkzxcbGOn9pt0yZMho5cqSqXf/JcgDu883hb/TQkod0+eplTb53soY3Hq5VB1epw4cdCvQ6F56/oHIBXAbeW3H6Llz+j19SlhAiSSdOnChWQSQ2NlZ79+6VJG3btk1JSUn69ddfXfq0b99eQUFBGjt2rB0lAiVe68jW2jPW9e9T5gmu+bH39F6uRVJMcGimmPLzy/syysUphEhyhpDmzZtLkn799Ve1bdvW5ZdzL1y4YEttAHLWr0E/VQioIEmqX7l+vtbZe3qvO0uCBxFEiqn169dLktq0aePSnjF8rFu3zqM1FbVLly4pNjZWr7zyilavXq2qVatKkrZs2eLs4+/v7/LLueUzXEoagHfoUKeDUp5PkTXRUqOqjbI8HxwYnKXtrpvv8kRp8ACCSDHVsWNHSVnDxtGjR52PGzXK+h+8L4mNjdWPP/6oK1euaN26dbr55puz9ClTpozzs5DMGTQAvNdjLR5T6VKlncsNQhroX33/pcplK0uSbgq8SSuGrlDN4Jp2lYgixhyRYmru3LkaN26c5s+frx49emjWrFlatmyZJkyYoAoVKmjGjBlq2LCh3WXekLRJqHfccYd++ukn/fzzzy7PlylTRssy/SQ5E1IB79ahTgdd/uvlLO1c9r34cutZM7Vq1dLhw4dd2p599lm99tpr+Vqfs2aQnYsXL2rRokX5OuW2Zs2aOnLkiEJCQtSlSxfVq1fPAxUCQMnmVWfNvPTSS3r44YedyxUqVHD3W6KYunTpkpYvX67t27fne50WLVrogQcecGNVAIAb4fYgEhQUpLCwgl2sBsgsu6uj5uSmm27S2bNnJUl16tRxY1UAgBvl9smqr7/+ukJCQtSkSRO98sorunw567G/NKmpqUpOTna5AZJcLs6Wl7Nnz6pOnToaNGiQypXjgkcA4M3cGkSeeOIJzZs3T6tXr9bYsWP15ptvuvziaWZTpkxRcHCw8xYREeHO8mx1/PhxtW7dWg6HQ82bN9dPP/2krl27yuFwyOFwFOiLtyS4evVqvvqlXT+lWbNmatCggTtLAgAUgQJPVp00aZImT56ca5/Nmzc7LyqV0RdffKEBAwYoKSlJISEhWZ5PTU1Vamqqczk5OVkRERHFcrJq79698/y9l507d/r8mS1FJa9/c5k98cQTuummm9xTDAAgV26drDp27FgNHjw41z61atXKtr1ly5aSpH379mUbRAIDAxUYGFjQknxSWgiZPHmyJk6cKEmaPn26unTp4ry+x/79+wki1zVr1kzff/99nv3KlSun/v37E0IAwEcUOIiEhoYqNDS0UG+2detWSVL16tULtX5x0qpVK23cuNEZQiRp8ODBLmcYZb4qakm2c+fOLG3ly5fPcsn2P//5zwoOznoVRgCAd3LbHJGNGzfqjTfe0LZt23Tw4EF99tlneuSRR9S7d2/VrMkV8WJiYrK0jR492nkBrrlz57r8RkpJl3FkyOFwqHv37oqMjJQklS5dWjfddJOGDh1KCAEAH+O2C5r98MMPGjNmjHbv3q3U1FRFRkZq8ODBmjBhQr7PZCjOFzT7n//5H02bNs25HBIS4vxZ+jSLFy9Wr169PF0aAAA3pCDf324bEbnzzju1adMmnT17VpcuXdLu3bs1adIkTqe8rl27ds7Ho0aN0tKlSxUVFeVsq1SpkvP/+AEAKK740bsbdOrUKfXv318Oh0OjRo3SiRMn1Lt3bzkcDtWsWVNLly7Ndr3u3bvLsixZlqXZs2erZcuW2r17t7Pt9OnTuuOOOzy8NQAAeJZbf2vmRvnCoZlevXq5hI3o6Ght2LDBpU9CQoLCw8M9XRoAALbwikMzJUVaCBk9erQkacOGDZo0aZLLT9KfPHnSltoAAPB2BJEb1LhxY0nS22+/7WwLDAxUQkKCc7lq1aoerwsAAF9AEMlFUlKS+vTpI4fDofvvv1+nT59W9+7dValSJVWsWFFDhgxxOfMlTaVKlTR16lTn8vr16z1ZNgAAPsPtv77ry0aNGuW8Auq///1vnTx5UitXrnQ+P3/+fFWqVMllnTvuuEOPPvqoS1vaqAkAAHDFiEgu0kLIuHHjJMklhEjSM888owEDBqhy5coKCAjQP/7xDy1ZssR5am5UVJS+/PJL1atXz7OFAwDgIzhrRuYQzIMPPqjFixdryJAhmjlzpoYOHaoVK1bk+auvP//8s/O3YQAAAGfN5MuZM2c0bNgw+fv7q2XLllq8eLEkcwgmNDRUy5cvzzaExMTEaPv27SpbtqwkadeuXR6tGwCA4qREzRHZtGmT2rZtq8uXL7u079+/32U5p0Gipk2b6umnn3Zpa9GiRdEWCQBACVKiRkSyCyH5NXbsWMXGxio6OlqSVK9ePa1YsYLLsAMAcANK1IhIQUPI559/rmHDhunSpUtq27atatasyam4AAAUoRIVRBwOR46HXTLy8/PT1atXNWDAAGcbh2AAACh6JerQzJ///Od89cs4SZVDMAAAuE+JGhGZP39+lrbsRklq1Kih999/X506dfJUaQAAlEglakTkj3/8o/Oxn5+f3nrrLQ0cOFAOh0Ply5dXrVq1tGLFCsXHxxNCAADwAC5oBgAAihQXNAMAAD6BIAIAAGxDEAEAALYhiAAAANsQRAAAgG0IIgAAwDYEEQAAYBuCCAAAsA1BBAAA2IYgAgAAbEMQAQAAtvHqX99N+xmc5ORkmysBAAD5lfa9nZ+fs/PqIJKSkiJJioiIsLkSAABQUCkpKQoODs61j1f/+u61a9d09OhRBQUFyeFw2F1OnpKTkxUREaH4+Phi/2vBJWlbpZK1vWxr8VWStrckbavkfdtrWZZSUlIUHh6uUqVynwXi1SMipUqVUo0aNewuo8AqVqzoFf8QPKEkbatUsraXbS2+StL2lqRtlbxre/MaCUnDZFUAAGAbgggAALANQaQIBQYGauLEiQoMDLS7FLcrSdsqlaztZVuLr5K0vSVpWyXf3l6vnqwKAACKN0ZEAACAbQgiAADANgQRAABgG4IIAACwDUEEAADYhiBSSGvWrJHD4cj2tnnz5hzXGzlyZJb+LVu29GDlhVerVq0stT/33HO5rmNZliZNmqTw8HCVLVtW9957r3bs2OGhigvn0KFDevDBB1W7dm2VLVtWdevW1cSJE3X58uVc1/OlfTtr1izVrl1bZcqUUbNmzfTNN9/k2n/t2rVq1qyZypQpozp16uidd97xUKWFN2XKFLVo0UJBQUGqWrWq+vbtqz179uS6Tk7/Xe/evdtDVRfepEmTstQdFhaW6zq+uF+l7P8WORwOPfbYY9n297X9um7dOvXq1Uvh4eFyOByKjY11eb6wf1e/+OIL3XrrrQoMDNStt96qhQsXumkLCoYgUkjR0dFKTEx0uT300EOqVauWmjdvnuu6Xbt2dVnvyy+/9FDVN+6ll15yqf0vf/lLrv2nTp2q6dOna+bMmdq8ebPCwsLUqVMn5w8aeqPdu3fr2rVrevfdd7Vjxw698cYbeuedd/TCCy/kua4v7NtPP/1UTz75pF588UVt3bpVrVu3Vrdu3XTkyJFs+x88eFDdu3dX69attXXrVr3wwgt6/PHH9cUXX3i48oJZu3atHnvsMW3atElxcXG6cuWKOnfurAsXLuS57p49e1z2Y7169TxQ8Y1r1KiRS93bt2/Psa+v7ldJ2rx5s8t2xsXFSZLuu+++XNfzlf164cIFNW7cWDNnzsz2+cL8Xd24caMGDRqkYcOG6ccff9SwYcM0cOBAfffdd+7ajPyzUCQuX75sVa1a1XrppZdy7TdixAirT58+nimqiEVGRlpvvPFGvvtfu3bNCgsLs1577TVn22+//WYFBwdb77zzjhsqdJ+pU6datWvXzrWPr+zbu+66y3r00Udd2ho0aGA999xz2fafMGGC1aBBA5e2Rx55xGrZsqXbanSHEydOWJKstWvX5thn9erVliTrzJkzniusiEycONFq3LhxvvsXl/1qWZb1xBNPWHXr1rWuXbuW7fO+vF8lWQsXLnQuF/bv6sCBA62uXbu6tHXp0sUaPHhwkddcUIyIFJHFixcrKSlJI0eOzLPvmjVrVLVqVdWvX18PP/ywTpw44f4Ci8jrr7+ukJAQNWnSRK+88kquhysOHjyoY8eOqXPnzs62wMBAtW3bVhs2bPBEuUXm3Llzqly5cp79vH3fXr58Wd9//73LPpGkzp0757hPNm7cmKV/ly5dtGXLFv3+++9uq7WonTt3TpLytR+bNm2q6tWrq0OHDlq9erW7Sysye/fuVXh4uGrXrq3BgwfrwIEDOfYtLvv18uXLmjt3rkaNGpXnr7T76n7NqLB/V3Pa397wt5ggUkRmz56tLl26KCIiItd+3bp108cff6xVq1YpJiZGmzdvVvv27ZWamuqhSgvviSee0Lx587R69WqNHTtWb775psaMGZNj/2PHjkmSqlWr5tJerVo153O+YP/+/ZoxY4YeffTRXPv5wr5NSkrS1atXC7RPjh07lm3/K1euKCkpyW21FiXLsjR+/Hj94Q9/0G233ZZjv+rVq+u9997TF198oQULFigqKkodOnTQunXrPFht4dx999368MMP9fXXX+uf//ynjh07pujoaJ06dSrb/sVhv0pSbGyszp49m+v/BPryfs2ssH9Xc9rfXvG32O4hGW8zceJES1Kut82bN7usEx8fb5UqVcr6/PPPC/x+R48etUqXLm198cUXRbUJBVKY7U3z+eefW5KspKSkbJ9fv369Jck6evSoS/tDDz1kdenSpci3JS+F2daEhATrlltusR588MECv5/d+zY7CQkJliRrw4YNLu1///vfraioqGzXqVevnvXqq6+6tH377beWJCsxMdFttRalMWPGWJGRkVZ8fHyB1+3Zs6fVq1cvN1TlXufPn7eqVatmxcTEZPt8cdivlmVZnTt3tnr27Fng9XxlvyrToZnC/l0tXbq09cknn7i0zZ071woMDCzSegvD3wNZx6eMHTtWgwcPzrVPrVq1XJbnzJmjkJAQ9e7du8DvV716dUVGRmrv3r0FXrcoFGZ706SdEbJv3z6FhIRkeT5txv6xY8dUvXp1Z/uJEyeyJHNPKOi2Hj16VO3atVOrVq303nvvFfj97N632QkNDZWfn1+W/wvKbZ+EhYVl29/f3z/b/e5txo0bp8WLF2vdunWqUaNGgddv2bKl5s6d64bK3Kt8+fK6/fbbc/z35+v7VZIOHz6slStXasGCBQVe11f3a2H/rua0v+34W5wZQSST0NBQhYaG5ru/ZVmaM2eOhg8frtKlSxf4/U6dOqX4+HiXf1CeVNDtzWjr1q2SlGPttWvXVlhYmOLi4tS0aVNJ5nju2rVr9frrrxeu4BtQkG1NSEhQu3bt1KxZM82ZM0elShX8KKbd+zY7AQEBatasmeLi4tSvXz9ne1xcnPr06ZPtOq1atdKSJUtc2lasWKHmzZsX6t+8p1iWpXHjxmnhwoVas2aNateuXajX2bp1q1ftw/xKTU3Vrl271Lp162yf99X9mtGcOXNUtWpV9ejRo8Dr+up+Lezf1VatWikuLk5PPfWUs23FihWKjo52e815sntIxtetXLnSkmTt3Lkz2+ejoqKsBQsWWJZlWSkpKdbTTz9tbdiwwTp48KC1evVqq1WrVtbNN99sJScne7LsAtuwYYM1ffp0a+vWrdaBAwesTz/91AoPD7d69+7t0i/j9lqWZb322mtWcHCwtWDBAmv79u3WkCFDrOrVq3v19qYdjmnfvr3166+/WomJic5bRr66b+fNm2eVLl3amj17trVz507rySeftMqXL28dOnTIsizLeu6556xhw4Y5+x84cMAqV66c9dRTT1k7d+60Zs+ebZUuXbpQhyI9afTo0VZwcLC1Zs0al3148eJFZ5/M2/rGG29YCxcutH755Rfr559/tp577jlLklcdXsvJ008/ba1Zs8Y6cOCAtWnTJqtnz55WUFBQsduvaa5evWrVrFnTevbZZ7M85+v7NSUlxdq6dau1detWS5Lzb+/hw4cty8rf39Vhw4a5nAm3fv16y8/Pz3rttdesXbt2Wa+99prl7+9vbdq0yePblxlB5AYNGTLEio6OzvF5SdacOXMsy7KsixcvWp07d7aqVKlilS5d2qpZs6Y1YsQI68iRIx6qtvC+//576+6777aCg4OtMmXKWFFRUdbEiROtCxcuuPTLuL2WZU41mzhxohUWFmYFBgZabdq0sbZv3+7h6gtmzpw5Oc4hyciX9+1bb71lRUZGWgEBAdadd97pckrriBEjrLZt27r0X7NmjdW0aVMrICDAqlWrlvX22297uOKCy2kfZvz3mXlbX3/9datu3bpWmTJlrEqVKll/+MMfrGXLlnm++EIYNGiQVb16dat06dJWeHi41b9/f2vHjh3O54vLfk3z9ddfW5KsPXv2ZHnO1/dr2unGmW8jRoywLCt/f1fbtm3r7J9m/vz5VlRUlFW6dGmrQYMGXhPEHJZlWR4bfgEAAMiA03cBAIBtCCIAAMA2BBEAAGAbgggAALANQQQAANiGIAIAAGxDEAEAALYhiAAAANsQRAAAgG0IIgAAwDYEEQAAYJv/D9KriETVlKEaAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 5000x2500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmap = ['red','blue','purple','green','yellow','brown','black','grey']\n",
    "\n",
    "for i in range(len(mv)):\n",
    "    m = mv[i]\n",
    "    c = epoch3Df['Iteration'][i]\n",
    "    plt.scatter(epoch3Df['x'][i],epoch3Df['y'][i],marker=f'${m}$',color = cmap[c])\n",
    "    plt.title(\"PCA for whole model\",color=\"r\")\n",
    "    \n",
    "plt.figure(figsize=[10,5],dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09888e03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            x         y  Epoch  Iteration        Acc      Loss\n",
      "0   -0.402965  0.403767      0          0  33.861445  1.740279\n",
      "1   -0.524997  0.510036      1          0  82.955129  0.692817\n",
      "2   -0.604255  0.583273      2          0  88.271317  0.419865\n",
      "3   -0.659958  0.636482      3          0  90.326108  0.338610\n",
      "4   -0.720845  0.687050      4          0  91.646955  0.295383\n",
      "..        ...       ...    ...        ...        ...       ...\n",
      "235 -0.214631 -0.635271     25          7  97.424987  0.087495\n",
      "236 -0.215318 -0.630923     26          7  97.665960  0.084108\n",
      "237 -0.213832 -0.625665     27          7  97.786623  0.081126\n",
      "238 -0.214297 -0.622087     28          7  97.868424  0.077604\n",
      "239 -0.214486 -0.616320     29          7  97.982265  0.074508\n",
      "\n",
      "[240 rows x 6 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer1_Df = all_df.iloc[:,0:7840]\n",
    "t2 = layer1_Df \n",
    "t2 = np.array(t2)\n",
    "pca = PCA(n_components=2)\n",
    "new_data2 = pca.fit_transform(t2)\n",
    "layer1Df = pd.DataFrame(new_data2,columns=['x','y'])\n",
    "\n",
    "eps_each_time = [i for i in range(max_epochs)] * 8\n",
    "times = np.repeat([i for i in range(8)],max_epochs)\n",
    "\n",
    "layer1Df['Epoch']=eps_each_time\n",
    "layer1Df['Iteration']=(times)\n",
    "layer1Df[\"Acc\"] = train_acc_data\n",
    "layer1Df[\"Loss\"] = train_loss_data\n",
    "print(layer1Df)\n",
    "for i in range(len(mv)):\n",
    "    m = mv[i]\n",
    "    c = layer1Df['Iteration'][i]\n",
    "    plt.scatter(layer1Df['x'][i],layer1Df['y'][i],marker=f'${m}$',color = cmap[c])\n",
    "    plt.title(\"PCA for Layer1\",color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f8bfe78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xshape: torch.Size([300, 1]) \n",
      " Yshape: torch.Size([300, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.712690/ipykernel_3318801/844140679.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x, y = torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1) \n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 300), dim=1)  \n",
    "y = (np.sin(5*np.pi*x))/(5*np.pi*x) \n",
    "print('Xshape:',x.shape,\"\\n Yshape:\",y.shape)\n",
    "\n",
    "x, y = torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "n_samples, n_features = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7d36f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class M1(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(M1, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 500)\n",
    "        self.fc2 = nn.Linear(500, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dcd9bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainFunc(model,x,y):\n",
    "    max_epoch = 2500\n",
    "    epoch_arr,loss_arr=[],[]\n",
    "    not_converged = True\n",
    "    epoch = 0\n",
    "    gradArr = []\n",
    "    while not_converged:\n",
    "        epoch += 1\n",
    "        prediction = model(x)     \n",
    "        loss = loss_func(prediction, y)     \n",
    "        optimizer.zero_grad()   \n",
    "        loss.backward()         \n",
    "        \n",
    "        epoch_arr.append(epoch)\n",
    "        loss_arr.append(loss.detach().numpy())\n",
    "\n",
    "        grad_all = 0.0\n",
    "        for p in model.parameters():\n",
    "            grad = 0.0\n",
    "            if p.grad is not None:\n",
    "                grad = (p.grad.cpu().data.numpy()**2).sum()\n",
    "            grad_all += grad\n",
    "        grad_norm = grad_all ** 0.5\n",
    "        \n",
    "\n",
    "        optimizer.step() \n",
    "        \n",
    "        gradArr.append(grad_norm)\n",
    "        \n",
    "        if epoch%100 == 0 : print(f'epoch: {epoch}, loss = {loss.item():.4f}, grad_norm = {grad_norm}') \n",
    "                \n",
    "        if epoch == max_epoch:\n",
    "                print(\"Max Epoch Reached\")\n",
    "                not_converged = False\n",
    "        elif (epoch > 5) and  (loss_arr[-1] < 0.001):\n",
    "            if abs(loss_arr[-3] - loss_arr[-2]) < 1.0e-05 and abs(loss_arr[-2] - loss_arr[-1]) < 1.0e-05:\n",
    "                print(\"Convergeance reached for loss:\",loss_arr[-1])\n",
    "                not_converged = False\n",
    "                \n",
    "    return epoch_arr,loss_arr,prediction,grad_norm,gradArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c7df923",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in Model (1): 1501\n",
      "epoch: 100, loss = 0.0518, grad_norm = 0.04283318368227245\n",
      "epoch: 200, loss = 0.0310, grad_norm = 0.03160978463406819\n",
      "epoch: 300, loss = 0.0158, grad_norm = 0.020891545870852447\n",
      "epoch: 400, loss = 0.0092, grad_norm = 0.012171428311993141\n",
      "epoch: 500, loss = 0.0066, grad_norm = 0.007926769205638116\n",
      "epoch: 600, loss = 0.0053, grad_norm = 0.005783667578245152\n",
      "epoch: 700, loss = 0.0045, grad_norm = 0.0047853965918377385\n",
      "epoch: 800, loss = 0.0038, grad_norm = 0.004264117367846688\n",
      "epoch: 900, loss = 0.0032, grad_norm = 0.0046597535463634675\n",
      "epoch: 1000, loss = 0.0027, grad_norm = 0.00405081146312055\n",
      "epoch: 1100, loss = 0.0023, grad_norm = 0.004377674298856849\n",
      "epoch: 1200, loss = 0.0020, grad_norm = 0.015536987059095514\n",
      "epoch: 1300, loss = 0.0017, grad_norm = 0.0031101834715484404\n",
      "epoch: 1400, loss = 0.0015, grad_norm = 0.002504985719799162\n",
      "epoch: 1500, loss = 0.0014, grad_norm = 0.002244380396155892\n",
      "epoch: 1600, loss = 0.0012, grad_norm = 0.007056509980745872\n",
      "epoch: 1700, loss = 0.0011, grad_norm = 0.008991248750764609\n",
      "epoch: 1800, loss = 0.0011, grad_norm = 0.014818320542008467\n",
      "Convergeance reached for loss: 0.0009992709\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "m1 = M1()     \n",
    "optimizer = torch.optim.Adam(m1.parameters(), lr=1e-3, weight_decay = 1e-4)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "a=[]\n",
    "for i in m1.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print('Total number of parameters in Model (1):', np.sum(a),)\n",
    "M1epoch_arr,M1loss_arr,M1prediction,M1grad_norm,M1gradArr = trainFunc(m1,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bdd6efc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(M1epoch_arr,M1gradArr,color=\"Red\")\n",
    "plt.title('Gradient Norm Observation During Training')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Gradient\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(M1epoch_arr,M1loss_arr,color=\"Blue\")\n",
    "plt.title('Loss Observation During Training')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d0d60",
   "metadata": {},
   "source": [
    "# Gradient is almost zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99cd4e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: autograd_lib in ./.local/lib/python3.11/site-packages (0.0.7)\n",
      "Requirement already satisfied: gin-config in ./.local/lib/python3.11/site-packages (from autograd_lib) (0.5.0)\n",
      "Requirement already satisfied: seaborn in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from autograd_lib) (0.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in ./.local/lib/python3.11/site-packages (from autograd_lib) (2.4.0)\n",
      "Requirement already satisfied: torch>=2.1.0 in ./.local/lib/python3.11/site-packages (from pytorch-lightning->autograd_lib) (2.4.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from pytorch-lightning->autograd_lib) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from pytorch-lightning->autograd_lib) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>=2022.5.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from pytorch-lightning->autograd_lib) (2023.4.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in ./.local/lib/python3.11/site-packages (from pytorch-lightning->autograd_lib) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from pytorch-lightning->autograd_lib) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in ./.local/lib/python3.11/site-packages (from pytorch-lightning->autograd_lib) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in ./.local/lib/python3.11/site-packages (from pytorch-lightning->autograd_lib) (0.11.7)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from seaborn->autograd_lib) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.25 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from seaborn->autograd_lib) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from seaborn->autograd_lib) (3.7.2)\n",
      "Requirement already satisfied: requests in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->autograd_lib) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->autograd_lib) (3.8.5)\n",
      "Requirement already satisfied: setuptools in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning->autograd_lib) (68.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->autograd_lib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->autograd_lib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->autograd_lib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->autograd_lib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->autograd_lib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->autograd_lib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->autograd_lib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from pandas>=0.25->seaborn->autograd_lib) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from pandas>=0.25->seaborn->autograd_lib) (2023.3)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (3.9.0)\n",
      "Requirement already satisfied: sympy in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (1.11.1)\n",
      "Requirement already satisfied: networkx in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (3.1)\n",
      "Requirement already satisfied: jinja2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.local/lib/python3.11/site-packages (from torch>=2.1.0->pytorch-lightning->autograd_lib) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->pytorch-lightning->autograd_lib) (12.6.68)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->autograd_lib) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->autograd_lib) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->autograd_lib) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->autograd_lib) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->autograd_lib) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->autograd_lib) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->autograd_lib) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn->autograd_lib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from jinja2->torch>=2.1.0->pytorch-lightning->autograd_lib) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning->autograd_lib) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning->autograd_lib) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning->autograd_lib) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from sympy->torch>=2.1.0->pytorch-lightning->autograd_lib) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install autograd_lib\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from autograd_lib import autograd_lib\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01ac0986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_func = lambda x : (torch.sin(5*np.pi*x)) /(5*np.pi*x) \n",
    "num_of_rows = 300\n",
    "X= torch.unsqueeze(torch.linspace(-1,1,num_of_rows),dim=1)\n",
    "Y = Y_func(X)\n",
    "dataset = TensorDataset(X,Y)\n",
    "data_loader = DataLoader(dataset,1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38753692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MathRegressor(nn.Module):\n",
    "    def __init__(self, num_hidden=128):\n",
    "        super().__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(1, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch,loss_fn):\n",
    "        inputs, targets = batch \n",
    "        out = self(inputs)                 \n",
    "        loss = loss_fn(out, targets)    \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch,loss_fn):\n",
    "        inputs, targets = batch \n",
    "        out = self(inputs)                 \n",
    "        loss = loss_fn(out, targets)    \n",
    "        return {'val_loss': loss.detach()}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   \n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def train_step(self, batch,loss_fn):\n",
    "        inputs, targets = batch \n",
    "        out = self(inputs)                 \n",
    "        loss = loss_fn(out, targets)    \n",
    "        return {'train_loss': loss.detach()}\n",
    "    \n",
    "    def train_epoch_end(self, outputs):\n",
    "        batch_losses = [x['train_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  \n",
    "        return {'train_loss': epoch_loss.item()}\n",
    "    \n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch, result['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8ff7041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradient_norm(model, criterion, train, target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    output = model(train)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = []\n",
    "    for p in model.regressor.children():\n",
    "        if isinstance(p, nn.Linear):\n",
    "            param_norm = p.weight.grad.norm(2).item()\n",
    "            grads.append(param_norm)\n",
    "\n",
    "    grad_mean = np.mean(grads) \n",
    "\n",
    "    return grad_mean\n",
    "\n",
    "def save_activations(layer, A, _):\n",
    "    activations[layer] = A\n",
    "\n",
    "def compute_hess(layer, _, B):\n",
    "    A = activations[layer]\n",
    "    BA = torch.einsum('nl,ni->nli', B, A) \n",
    "    hess[layer] += torch.einsum('nli,nkj->likj', BA, BA)\n",
    "\n",
    "def compute_minimum_ratio(model, criterion, train, target):\n",
    "    model.zero_grad()\n",
    "    \n",
    "    with autograd_lib.module_hook(save_activations):\n",
    "        output = model(train)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "    with autograd_lib.module_hook(compute_hess):\n",
    "        autograd_lib.backward_hessian(output, loss='LeastSquares')\n",
    "\n",
    "    layer_hess = list(hess.values())\n",
    "    minimum_ratio = []\n",
    "\n",
    "    for h in layer_hess:\n",
    "        size = h.shape[0] * h.shape[1]\n",
    "        h = h.reshape(size, size)\n",
    "        \n",
    "        h_eig = torch.linalg.eigvalsh(h, UPLO='U') \n",
    "        \n",
    "        num_greater = torch.sum(h_eig > 0).item()\n",
    "        minimum_ratio.append(num_greater / len(h_eig))\n",
    "\n",
    "    ratio_mean = np.mean(minimum_ratio)\n",
    "\n",
    "    return ratio_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c00338e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_norm_minimal_ratio(model,criterion):\n",
    "\n",
    "    gradient_norm = compute_gradient_norm(model, criterion, X, Y)\n",
    "    minimum_ratio = compute_minimum_ratio(model, criterion, X, Y)\n",
    "\n",
    "    print('gradient norm: {}, minimum ratio: {}'.format(gradient_norm, minimum_ratio))\n",
    "    result = {}\n",
    "    result[\"grad_norm\"] = gradient_norm\n",
    "    result[\"ratio\"] = minimum_ratio\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d045762d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model,loss_fn, val_loader):\n",
    "    outputs = [model.validation_step(batch,loss_fn) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def evaluate2(model,loss_fn, train_loader):\n",
    "    outputs = [model.train_step(batch,loss_fn) for batch in train_loader]\n",
    "    return model.train_epoch_end(outputs)\n",
    "\n",
    "def get_grad_norm(model):\n",
    "    grad_all=0.0\n",
    "    grad =0\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grad = (p.grad.cpu().data.numpy()**2).sum()\n",
    "            \n",
    "        grad_all+=grad\n",
    "        \n",
    "    grad_norm=grad_all ** 0.5\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "def fit(epochs, lr, model, data_loader, criterion,opt_func):\n",
    "    history = []\n",
    "    comparing_epoch_loss =1000.0\n",
    "    grad_norm_per_epoch={}\n",
    "    \n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(X)\n",
    "        loss = criterion(prediction, Y)\n",
    "        loss.backward()\n",
    "        grad_norm_per_epoch[epoch] = get_norm_minimal_ratio(model,criterion)\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "        result = evaluate(model,criterion, data_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        if epoch == 900:\n",
    "            comparing_epoch_loss= result[\"val_loss\"]\n",
    "    return history,grad_norm_per_epoch,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "565a6ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_of_rows = 300\n",
    "lr = 0.0004\n",
    "gamma_lr_scheduler = 0.1 \n",
    "weight_decay = 1e-4\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam\n",
    "num_epochs =2500\n",
    "criterion_name = \"MSE_LOSS_\"\n",
    "optimizer_name = \"ADAM_opt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4b83264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam\n",
    "num_epochs =2000\n",
    "criterion_name = \"MSE_LOSS_\"\n",
    "optimizer_name = \"ADAM_opt\"\n",
    "input_size=1\n",
    "output_size=1\n",
    "model= MathRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f31ddcd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autograd_lib.register(model)\n",
    "activations = defaultdict(int)\n",
    "hess = defaultdict(float)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b189615d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauravp/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    }
   ],
   "source": [
    "result_1 = evaluate(model,criterion,data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b835f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train,target = X,Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de149cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class M1(torch.nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(M1, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(1, 5)    \n",
    "        self.hidden2 = torch.nn.Linear(5, 10)\n",
    "        self.hidden3 = torch.nn.Linear (10, 10)\n",
    "        self.hidden4 = torch.nn.Linear (10, 10)\n",
    "        self.hidden5 = torch.nn.Linear (10, 10)\n",
    "        self.hidden6 = torch.nn.Linear (10, 10)\n",
    "        self.hidden7 = torch.nn.Linear (10,5)\n",
    "        self.predict = torch.nn.Linear(5, 1)     \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.hidden1(x))      \n",
    "        x = F.leaky_relu(self.hidden2(x))\n",
    "        x = F.leaky_relu(self.hidden3(x))\n",
    "        x = F.leaky_relu(self.hidden4(x))\n",
    "        x = F.leaky_relu(self.hidden5(x))\n",
    "        x = F.leaky_relu(self.hidden6(x))\n",
    "        x = F.leaky_relu(self.hidden7(x))\n",
    "        x = self.predict(x)                    \n",
    "        return x\n",
    "\n",
    "model_1 = M1()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c40c10de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient norm: 0.8235280029475689, minimum ratio: 0.6875\n",
      "Epoch [0], val_loss: 0.1185\n",
      "gradient norm: 0.8915635161101818, minimum ratio: 0.71875\n",
      "Epoch [1], val_loss: 0.1240\n",
      "gradient norm: 0.9603039901703596, minimum ratio: 0.73046875\n",
      "Epoch [2], val_loss: 0.1298\n",
      "gradient norm: 1.0295406244695187, minimum ratio: 0.6953125\n",
      "Epoch [3], val_loss: 0.1361\n",
      "gradient norm: 1.0992021076381207, minimum ratio: 0.71484375\n",
      "Epoch [4], val_loss: 0.1429\n",
      "gradient norm: 1.169234350323677, minimum ratio: 0.7265625\n",
      "Epoch [5], val_loss: 0.1502\n",
      "gradient norm: 1.2396116945892572, minimum ratio: 0.70703125\n",
      "Epoch [6], val_loss: 0.1580\n",
      "gradient norm: 1.3102730941027403, minimum ratio: 0.7109375\n",
      "Epoch [7], val_loss: 0.1662\n",
      "gradient norm: 1.3811864703893661, minimum ratio: 0.703125\n",
      "Epoch [8], val_loss: 0.1749\n",
      "gradient norm: 1.4523267354816198, minimum ratio: 0.73046875\n",
      "Epoch [9], val_loss: 0.1840\n",
      "gradient norm: 1.5236770026385784, minimum ratio: 0.7265625\n",
      "Epoch [10], val_loss: 0.1936\n",
      "gradient norm: 1.5952204205095768, minimum ratio: 0.72265625\n",
      "Epoch [11], val_loss: 0.2037\n",
      "gradient norm: 1.666943646967411, minimum ratio: 0.71875\n",
      "Epoch [12], val_loss: 0.2143\n",
      "gradient norm: 1.7388440147042274, minimum ratio: 0.72265625\n",
      "Epoch [13], val_loss: 0.2254\n",
      "gradient norm: 1.8109008111059666, minimum ratio: 0.73046875\n",
      "Epoch [14], val_loss: 0.2369\n",
      "gradient norm: 1.883105468004942, minimum ratio: 0.7265625\n",
      "Epoch [15], val_loss: 0.2489\n",
      "gradient norm: 1.9554706551134586, minimum ratio: 0.70703125\n",
      "Epoch [16], val_loss: 0.2614\n",
      "gradient norm: 2.027964860200882, minimum ratio: 0.73046875\n",
      "Epoch [17], val_loss: 0.2744\n",
      "gradient norm: 2.100593753159046, minimum ratio: 0.72265625\n",
      "Epoch [18], val_loss: 0.2878\n",
      "gradient norm: 2.1733493991196156, minimum ratio: 0.7265625\n",
      "Epoch [19], val_loss: 0.3017\n",
      "gradient norm: 2.246229086071253, minimum ratio: 0.7265625\n",
      "Epoch [20], val_loss: 0.3161\n",
      "gradient norm: 2.3192428275942802, minimum ratio: 0.70703125\n",
      "Epoch [21], val_loss: 0.3310\n",
      "gradient norm: 2.392392113804817, minimum ratio: 0.73046875\n",
      "Epoch [22], val_loss: 0.3464\n",
      "gradient norm: 2.4656244851648808, minimum ratio: 0.7265625\n",
      "Epoch [23], val_loss: 0.3623\n",
      "gradient norm: 2.538997668772936, minimum ratio: 0.72265625\n",
      "Epoch [24], val_loss: 0.3786\n",
      "gradient norm: 2.6124748215079308, minimum ratio: 0.74609375\n",
      "Epoch [25], val_loss: 0.3955\n",
      "gradient norm: 2.6860989294946194, minimum ratio: 0.7421875\n",
      "Epoch [26], val_loss: 0.4128\n",
      "gradient norm: 2.7598377987742424, minimum ratio: 0.7265625\n",
      "Epoch [27], val_loss: 0.4306\n",
      "gradient norm: 2.833683744072914, minimum ratio: 0.75\n",
      "Epoch [28], val_loss: 0.4489\n",
      "gradient norm: 2.9076696820557117, minimum ratio: 0.72265625\n",
      "Epoch [29], val_loss: 0.4677\n",
      "gradient norm: 2.9817573241889477, minimum ratio: 0.71484375\n",
      "Epoch [30], val_loss: 0.4870\n",
      "gradient norm: 3.055971086025238, minimum ratio: 0.72265625\n",
      "Epoch [31], val_loss: 0.5068\n",
      "gradient norm: 3.1302739195525646, minimum ratio: 0.73828125\n",
      "Epoch [32], val_loss: 0.5271\n",
      "gradient norm: 3.204734653234482, minimum ratio: 0.7421875\n",
      "Epoch [33], val_loss: 0.5479\n",
      "gradient norm: 3.2793236300349236, minimum ratio: 0.71875\n",
      "Epoch [34], val_loss: 0.5692\n",
      "gradient norm: 3.3540126383304596, minimum ratio: 0.7421875\n",
      "Epoch [35], val_loss: 0.5910\n",
      "gradient norm: 3.4288570806384087, minimum ratio: 0.72265625\n",
      "Epoch [36], val_loss: 0.6132\n",
      "gradient norm: 3.50380889326334, minimum ratio: 0.71484375\n",
      "Epoch [37], val_loss: 0.6360\n",
      "gradient norm: 3.578916296362877, minimum ratio: 0.75390625\n",
      "Epoch [38], val_loss: 0.6593\n",
      "gradient norm: 3.6541523709893227, minimum ratio: 0.70703125\n",
      "Epoch [39], val_loss: 0.6832\n",
      "gradient norm: 3.729537695646286, minimum ratio: 0.7578125\n",
      "Epoch [40], val_loss: 0.7075\n",
      "gradient norm: 3.8050200790166855, minimum ratio: 0.734375\n",
      "Epoch [41], val_loss: 0.7323\n",
      "gradient norm: 3.8806488066911697, minimum ratio: 0.73046875\n",
      "Epoch [42], val_loss: 0.7577\n",
      "gradient norm: 3.9564497768878937, minimum ratio: 0.73828125\n",
      "Epoch [43], val_loss: 0.7835\n",
      "gradient norm: 4.032356917858124, minimum ratio: 0.75390625\n",
      "Epoch [44], val_loss: 0.8099\n",
      "gradient norm: 4.108404003083706, minimum ratio: 0.73046875\n",
      "Epoch [45], val_loss: 0.8368\n",
      "gradient norm: 4.184586726129055, minimum ratio: 0.75390625\n",
      "Epoch [46], val_loss: 0.8642\n",
      "gradient norm: 4.26091980189085, minimum ratio: 0.7421875\n",
      "Epoch [47], val_loss: 0.8922\n",
      "gradient norm: 4.337426237761974, minimum ratio: 0.75\n",
      "Epoch [48], val_loss: 0.9207\n",
      "gradient norm: 4.414075657725334, minimum ratio: 0.73828125\n",
      "Epoch [49], val_loss: 0.9497\n",
      "gradient norm: 4.490904912352562, minimum ratio: 0.734375\n",
      "Epoch [50], val_loss: 0.9792\n",
      "gradient norm: 4.567852407693863, minimum ratio: 0.75390625\n",
      "Epoch [51], val_loss: 1.0092\n",
      "gradient norm: 4.644963726401329, minimum ratio: 0.73828125\n",
      "Epoch [52], val_loss: 1.0398\n",
      "gradient norm: 4.722197890281677, minimum ratio: 0.72265625\n",
      "Epoch [53], val_loss: 1.0710\n",
      "gradient norm: 4.79959224909544, minimum ratio: 0.75\n",
      "Epoch [54], val_loss: 1.1026\n",
      "gradient norm: 4.877159833908081, minimum ratio: 0.71875\n",
      "Epoch [55], val_loss: 1.1348\n",
      "gradient norm: 4.954899467527866, minimum ratio: 0.75390625\n",
      "Epoch [56], val_loss: 1.1676\n",
      "gradient norm: 5.032802276313305, minimum ratio: 0.74609375\n",
      "Epoch [57], val_loss: 1.2009\n",
      "gradient norm: 5.110862165689468, minimum ratio: 0.734375\n",
      "Epoch [58], val_loss: 1.2347\n",
      "gradient norm: 5.189081892371178, minimum ratio: 0.7421875\n",
      "Epoch [59], val_loss: 1.2691\n",
      "gradient norm: 5.267496608197689, minimum ratio: 0.73046875\n",
      "Epoch [60], val_loss: 1.3040\n",
      "gradient norm: 5.346025072038174, minimum ratio: 0.75390625\n",
      "Epoch [61], val_loss: 1.3395\n",
      "gradient norm: 5.424785807728767, minimum ratio: 0.734375\n",
      "Epoch [62], val_loss: 1.3756\n",
      "gradient norm: 5.503688119351864, minimum ratio: 0.7421875\n",
      "Epoch [63], val_loss: 1.4122\n",
      "gradient norm: 5.582839749753475, minimum ratio: 0.7265625\n",
      "Epoch [64], val_loss: 1.4493\n",
      "gradient norm: 5.662164203822613, minimum ratio: 0.73046875\n",
      "Epoch [65], val_loss: 1.4871\n",
      "gradient norm: 5.741659604012966, minimum ratio: 0.765625\n",
      "Epoch [66], val_loss: 1.5254\n",
      "gradient norm: 5.82134947180748, minimum ratio: 0.7421875\n",
      "Epoch [67], val_loss: 1.5642\n",
      "gradient norm: 5.901173494756222, minimum ratio: 0.75390625\n",
      "Epoch [68], val_loss: 1.6037\n",
      "gradient norm: 5.981208428740501, minimum ratio: 0.73828125\n",
      "Epoch [69], val_loss: 1.6437\n",
      "gradient norm: 6.061456233263016, minimum ratio: 0.7578125\n",
      "Epoch [70], val_loss: 1.6843\n",
      "gradient norm: 6.141924180090427, minimum ratio: 0.75\n",
      "Epoch [71], val_loss: 1.7255\n",
      "gradient norm: 6.222603932023048, minimum ratio: 0.72265625\n",
      "Epoch [72], val_loss: 1.7672\n",
      "gradient norm: 6.303423151373863, minimum ratio: 0.75\n",
      "Epoch [73], val_loss: 1.8096\n",
      "gradient norm: 6.3844787031412125, minimum ratio: 0.73828125\n",
      "Epoch [74], val_loss: 1.8525\n",
      "gradient norm: 6.465806171298027, minimum ratio: 0.7421875\n",
      "Epoch [75], val_loss: 1.8960\n",
      "gradient norm: 6.547274559736252, minimum ratio: 0.7578125\n",
      "Epoch [76], val_loss: 1.9402\n",
      "gradient norm: 6.62901009619236, minimum ratio: 0.75390625\n",
      "Epoch [77], val_loss: 1.9849\n",
      "gradient norm: 6.710871934890747, minimum ratio: 0.7265625\n",
      "Epoch [78], val_loss: 2.0302\n",
      "gradient norm: 6.793015629053116, minimum ratio: 0.74609375\n",
      "Epoch [79], val_loss: 2.0761\n",
      "gradient norm: 6.875336334109306, minimum ratio: 0.76953125\n",
      "Epoch [80], val_loss: 2.1227\n",
      "gradient norm: 6.957880184054375, minimum ratio: 0.765625\n",
      "Epoch [81], val_loss: 2.1698\n",
      "gradient norm: 7.040675163269043, minimum ratio: 0.75390625\n",
      "Epoch [82], val_loss: 2.2176\n",
      "gradient norm: 7.1236827075481415, minimum ratio: 0.7421875\n",
      "Epoch [83], val_loss: 2.2660\n",
      "gradient norm: 7.206952512264252, minimum ratio: 0.75\n",
      "Epoch [84], val_loss: 2.3150\n",
      "gradient norm: 7.290369421243668, minimum ratio: 0.71875\n",
      "Epoch [85], val_loss: 2.3646\n",
      "gradient norm: 7.374066516757011, minimum ratio: 0.7578125\n",
      "Epoch [86], val_loss: 2.4148\n",
      "gradient norm: 7.45803664624691, minimum ratio: 0.734375\n",
      "Epoch [87], val_loss: 2.4657\n",
      "gradient norm: 7.542224034667015, minimum ratio: 0.76171875\n",
      "Epoch [88], val_loss: 2.5172\n",
      "gradient norm: 7.626643612980843, minimum ratio: 0.73828125\n",
      "Epoch [89], val_loss: 2.5694\n",
      "gradient norm: 7.711345478892326, minimum ratio: 0.75\n",
      "Epoch [90], val_loss: 2.6222\n",
      "gradient norm: 7.7962973564863205, minimum ratio: 0.734375\n",
      "Epoch [91], val_loss: 2.6756\n",
      "gradient norm: 7.881477326154709, minimum ratio: 0.75\n",
      "Epoch [92], val_loss: 2.7297\n",
      "gradient norm: 7.966899171471596, minimum ratio: 0.73046875\n",
      "Epoch [93], val_loss: 2.7844\n",
      "gradient norm: 8.052534431219101, minimum ratio: 0.74609375\n",
      "Epoch [94], val_loss: 2.8398\n",
      "gradient norm: 8.138406410813332, minimum ratio: 0.734375\n",
      "Epoch [95], val_loss: 2.8959\n",
      "gradient norm: 8.224633201956749, minimum ratio: 0.74609375\n",
      "Epoch [96], val_loss: 2.9526\n",
      "gradient norm: 8.31118980050087, minimum ratio: 0.765625\n",
      "Epoch [97], val_loss: 3.0100\n",
      "gradient norm: 8.397935181856155, minimum ratio: 0.734375\n",
      "Epoch [98], val_loss: 3.0680\n",
      "gradient norm: 8.484957829117775, minimum ratio: 0.73828125\n",
      "Epoch [99], val_loss: 3.1268\n",
      "gradient norm: 8.572160735726357, minimum ratio: 0.75\n",
      "Epoch [100], val_loss: 3.1862\n",
      "gradient norm: 8.65964725613594, minimum ratio: 0.73828125\n",
      "Epoch [101], val_loss: 3.2463\n",
      "gradient norm: 8.74743638932705, minimum ratio: 0.75\n",
      "Epoch [102], val_loss: 3.3070\n",
      "gradient norm: 8.835606694221497, minimum ratio: 0.76171875\n",
      "Epoch [103], val_loss: 3.3685\n",
      "gradient norm: 8.923951581120491, minimum ratio: 0.74609375\n",
      "Epoch [104], val_loss: 3.4306\n",
      "gradient norm: 9.012556865811348, minimum ratio: 0.75\n",
      "Epoch [105], val_loss: 3.4935\n",
      "gradient norm: 9.101549744606018, minimum ratio: 0.75390625\n",
      "Epoch [106], val_loss: 3.5571\n",
      "gradient norm: 9.190784990787506, minimum ratio: 0.75\n",
      "Epoch [107], val_loss: 3.6213\n",
      "gradient norm: 9.280313432216644, minimum ratio: 0.74609375\n",
      "Epoch [108], val_loss: 3.6863\n",
      "gradient norm: 9.37017223238945, minimum ratio: 0.76953125\n",
      "Epoch [109], val_loss: 3.7520\n",
      "gradient norm: 9.460193052887917, minimum ratio: 0.73828125\n",
      "Epoch [110], val_loss: 3.8184\n",
      "gradient norm: 9.550587370991707, minimum ratio: 0.7421875\n",
      "Epoch [111], val_loss: 3.8855\n",
      "gradient norm: 9.641260907053947, minimum ratio: 0.7421875\n",
      "Epoch [112], val_loss: 3.9533\n",
      "gradient norm: 9.732250526547432, minimum ratio: 0.75390625\n",
      "Epoch [113], val_loss: 4.0219\n",
      "gradient norm: 9.823544561862946, minimum ratio: 0.74609375\n",
      "Epoch [114], val_loss: 4.0912\n",
      "gradient norm: 9.915119871497154, minimum ratio: 0.73828125\n",
      "Epoch [115], val_loss: 4.1612\n",
      "gradient norm: 10.006980136036873, minimum ratio: 0.75\n",
      "Epoch [116], val_loss: 4.2320\n",
      "gradient norm: 10.099140375852585, minimum ratio: 0.75390625\n",
      "Epoch [117], val_loss: 4.3036\n",
      "gradient norm: 10.191658914089203, minimum ratio: 0.75390625\n",
      "Epoch [118], val_loss: 4.3758\n",
      "gradient norm: 10.284352421760559, minimum ratio: 0.73828125\n",
      "Epoch [119], val_loss: 4.4489\n",
      "gradient norm: 10.377447187900543, minimum ratio: 0.765625\n",
      "Epoch [120], val_loss: 4.5227\n",
      "gradient norm: 10.470905154943466, minimum ratio: 0.7421875\n",
      "Epoch [121], val_loss: 4.5972\n",
      "gradient norm: 10.56469240784645, minimum ratio: 0.7578125\n",
      "Epoch [122], val_loss: 4.6726\n",
      "gradient norm: 10.658794492483139, minimum ratio: 0.76953125\n",
      "Epoch [123], val_loss: 4.7487\n",
      "gradient norm: 10.7532759308815, minimum ratio: 0.75\n",
      "Epoch [124], val_loss: 4.8256\n",
      "gradient norm: 10.847934603691101, minimum ratio: 0.7578125\n",
      "Epoch [125], val_loss: 4.9032\n",
      "gradient norm: 10.942928463220596, minimum ratio: 0.76953125\n",
      "Epoch [126], val_loss: 4.9817\n",
      "gradient norm: 11.038236647844315, minimum ratio: 0.7578125\n",
      "Epoch [127], val_loss: 5.0609\n",
      "gradient norm: 11.134035170078278, minimum ratio: 0.74609375\n",
      "Epoch [128], val_loss: 5.1410\n",
      "gradient norm: 11.230251759290695, minimum ratio: 0.71875\n",
      "Epoch [129], val_loss: 5.2218\n",
      "gradient norm: 11.32653722167015, minimum ratio: 0.74609375\n",
      "Epoch [130], val_loss: 5.3034\n",
      "gradient norm: 11.423258125782013, minimum ratio: 0.75\n",
      "Epoch [131], val_loss: 5.3859\n",
      "gradient norm: 11.520293653011322, minimum ratio: 0.75390625\n",
      "Epoch [132], val_loss: 5.4692\n",
      "gradient norm: 11.617722243070602, minimum ratio: 0.76171875\n",
      "Epoch [133], val_loss: 5.5533\n",
      "gradient norm: 11.71555957198143, minimum ratio: 0.73828125\n",
      "Epoch [134], val_loss: 5.6382\n",
      "gradient norm: 11.813597410917282, minimum ratio: 0.72265625\n",
      "Epoch [135], val_loss: 5.7239\n",
      "gradient norm: 11.911925315856934, minimum ratio: 0.75\n",
      "Epoch [136], val_loss: 5.8105\n",
      "gradient norm: 12.010816246271133, minimum ratio: 0.73828125\n",
      "Epoch [137], val_loss: 5.8980\n",
      "gradient norm: 12.110027819871902, minimum ratio: 0.76171875\n",
      "Epoch [138], val_loss: 5.9862\n",
      "gradient norm: 12.209516555070877, minimum ratio: 0.75\n",
      "Epoch [139], val_loss: 6.0754\n",
      "gradient norm: 12.30949580669403, minimum ratio: 0.74609375\n",
      "Epoch [140], val_loss: 6.1654\n",
      "gradient norm: 12.409775644540787, minimum ratio: 0.75\n",
      "Epoch [141], val_loss: 6.2562\n",
      "gradient norm: 12.510366827249527, minimum ratio: 0.73828125\n",
      "Epoch [142], val_loss: 6.3479\n",
      "gradient norm: 12.611181557178497, minimum ratio: 0.765625\n",
      "Epoch [143], val_loss: 6.4405\n",
      "gradient norm: 12.712216943502426, minimum ratio: 0.75390625\n",
      "Epoch [144], val_loss: 6.5340\n",
      "gradient norm: 12.813827455043793, minimum ratio: 0.74609375\n",
      "Epoch [145], val_loss: 6.6283\n",
      "gradient norm: 12.916087299585342, minimum ratio: 0.76171875\n",
      "Epoch [146], val_loss: 6.7235\n",
      "gradient norm: 13.018637746572495, minimum ratio: 0.74609375\n",
      "Epoch [147], val_loss: 6.8197\n",
      "gradient norm: 13.12119272351265, minimum ratio: 0.7421875\n",
      "Epoch [148], val_loss: 6.9167\n",
      "gradient norm: 13.224268287420273, minimum ratio: 0.7421875\n",
      "Epoch [149], val_loss: 7.0146\n",
      "gradient norm: 13.327939927577972, minimum ratio: 0.76953125\n",
      "Epoch [150], val_loss: 7.1134\n",
      "gradient norm: 13.432052582502365, minimum ratio: 0.74609375\n",
      "Epoch [151], val_loss: 7.2132\n",
      "gradient norm: 13.536457061767578, minimum ratio: 0.73828125\n",
      "Epoch [152], val_loss: 7.3138\n",
      "gradient norm: 13.64093890786171, minimum ratio: 0.734375\n",
      "Epoch [153], val_loss: 7.4154\n",
      "gradient norm: 13.745855450630188, minimum ratio: 0.7578125\n",
      "Epoch [154], val_loss: 7.5179\n",
      "gradient norm: 13.851466089487076, minimum ratio: 0.7578125\n",
      "Epoch [155], val_loss: 7.6214\n",
      "gradient norm: 13.957255899906158, minimum ratio: 0.7421875\n",
      "Epoch [156], val_loss: 7.7258\n",
      "gradient norm: 14.063598096370697, minimum ratio: 0.75390625\n",
      "Epoch [157], val_loss: 7.8311\n",
      "gradient norm: 14.170308500528336, minimum ratio: 0.7578125\n",
      "Epoch [158], val_loss: 7.9374\n",
      "gradient norm: 14.277278244495392, minimum ratio: 0.74609375\n",
      "Epoch [159], val_loss: 8.0446\n",
      "gradient norm: 14.384952247142792, minimum ratio: 0.78515625\n",
      "Epoch [160], val_loss: 8.1528\n",
      "gradient norm: 14.492628991603851, minimum ratio: 0.7578125\n",
      "Epoch [161], val_loss: 8.2620\n",
      "gradient norm: 14.600599527359009, minimum ratio: 0.74609375\n",
      "Epoch [162], val_loss: 8.3722\n",
      "gradient norm: 14.709129184484482, minimum ratio: 0.75390625\n",
      "Epoch [163], val_loss: 8.4833\n",
      "gradient norm: 14.818209707736969, minimum ratio: 0.73828125\n",
      "Epoch [164], val_loss: 8.5955\n",
      "gradient norm: 14.927930563688278, minimum ratio: 0.75390625\n",
      "Epoch [165], val_loss: 8.7086\n",
      "gradient norm: 15.03765320777893, minimum ratio: 0.734375\n",
      "Epoch [166], val_loss: 8.8227\n",
      "gradient norm: 15.147936016321182, minimum ratio: 0.73828125\n",
      "Epoch [167], val_loss: 8.9379\n",
      "gradient norm: 15.2585389316082, minimum ratio: 0.75390625\n",
      "Epoch [168], val_loss: 9.0540\n",
      "gradient norm: 15.36992958188057, minimum ratio: 0.74609375\n",
      "Epoch [169], val_loss: 9.1712\n",
      "gradient norm: 15.481323331594467, minimum ratio: 0.765625\n",
      "Epoch [170], val_loss: 9.2894\n",
      "gradient norm: 15.593197464942932, minimum ratio: 0.74609375\n",
      "Epoch [171], val_loss: 9.4086\n",
      "gradient norm: 15.705587655305862, minimum ratio: 0.765625\n",
      "Epoch [172], val_loss: 9.5289\n",
      "gradient norm: 15.818132847547531, minimum ratio: 0.7578125\n",
      "Epoch [173], val_loss: 9.6502\n",
      "gradient norm: 15.93162226676941, minimum ratio: 0.75\n",
      "Epoch [174], val_loss: 9.7726\n",
      "gradient norm: 16.045261293649673, minimum ratio: 0.7578125\n",
      "Epoch [175], val_loss: 9.8960\n",
      "gradient norm: 16.15961766242981, minimum ratio: 0.7734375\n",
      "Epoch [176], val_loss: 10.0205\n",
      "gradient norm: 16.27416580915451, minimum ratio: 0.76953125\n",
      "Epoch [177], val_loss: 10.1461\n",
      "gradient norm: 16.389291405677795, minimum ratio: 0.7734375\n",
      "Epoch [178], val_loss: 10.2727\n",
      "gradient norm: 16.504624485969543, minimum ratio: 0.74609375\n",
      "Epoch [179], val_loss: 10.4005\n",
      "gradient norm: 16.62038028240204, minimum ratio: 0.75\n",
      "Epoch [180], val_loss: 10.5293\n",
      "gradient norm: 16.736924946308136, minimum ratio: 0.7578125\n",
      "Epoch [181], val_loss: 10.6592\n",
      "gradient norm: 16.853754222393036, minimum ratio: 0.765625\n",
      "Epoch [182], val_loss: 10.7902\n",
      "gradient norm: 16.970906257629395, minimum ratio: 0.7734375\n",
      "Epoch [183], val_loss: 10.9223\n",
      "gradient norm: 17.088582396507263, minimum ratio: 0.7734375\n",
      "Epoch [184], val_loss: 11.0555\n",
      "gradient norm: 17.206425666809082, minimum ratio: 0.75390625\n",
      "Epoch [185], val_loss: 11.1898\n",
      "gradient norm: 17.325096666812897, minimum ratio: 0.75390625\n",
      "Epoch [186], val_loss: 11.3252\n",
      "gradient norm: 17.444326043128967, minimum ratio: 0.75390625\n",
      "Epoch [187], val_loss: 11.4618\n",
      "gradient norm: 17.563884019851685, minimum ratio: 0.7578125\n",
      "Epoch [188], val_loss: 11.5996\n",
      "gradient norm: 17.683681547641754, minimum ratio: 0.7734375\n",
      "Epoch [189], val_loss: 11.7384\n",
      "gradient norm: 17.8039191365242, minimum ratio: 0.73828125\n",
      "Epoch [190], val_loss: 11.8785\n",
      "gradient norm: 17.924633860588074, minimum ratio: 0.765625\n",
      "Epoch [191], val_loss: 12.0197\n",
      "gradient norm: 18.04622209072113, minimum ratio: 0.76171875\n",
      "Epoch [192], val_loss: 12.1620\n",
      "gradient norm: 18.168228566646576, minimum ratio: 0.765625\n",
      "Epoch [193], val_loss: 12.3056\n",
      "gradient norm: 18.290478348731995, minimum ratio: 0.7578125\n",
      "Epoch [194], val_loss: 12.4503\n",
      "gradient norm: 18.412890255451202, minimum ratio: 0.75390625\n",
      "Epoch [195], val_loss: 12.5963\n",
      "gradient norm: 18.53627586364746, minimum ratio: 0.7265625\n",
      "Epoch [196], val_loss: 12.7434\n",
      "gradient norm: 18.660108625888824, minimum ratio: 0.75390625\n",
      "Epoch [197], val_loss: 12.8918\n",
      "gradient norm: 18.784533262252808, minimum ratio: 0.7265625\n",
      "Epoch [198], val_loss: 13.0413\n",
      "gradient norm: 18.909347355365753, minimum ratio: 0.74609375\n",
      "Epoch [199], val_loss: 13.1921\n",
      "gradient norm: 19.03425794839859, minimum ratio: 0.765625\n",
      "Epoch [200], val_loss: 13.3441\n",
      "gradient norm: 19.159932672977448, minimum ratio: 0.765625\n",
      "Epoch [201], val_loss: 13.4973\n",
      "gradient norm: 19.286004841327667, minimum ratio: 0.75\n",
      "Epoch [202], val_loss: 13.6518\n",
      "gradient norm: 19.412811994552612, minimum ratio: 0.75\n",
      "Epoch [203], val_loss: 13.8075\n",
      "gradient norm: 19.53963816165924, minimum ratio: 0.75\n",
      "Epoch [204], val_loss: 13.9645\n",
      "gradient norm: 19.667536318302155, minimum ratio: 0.75390625\n",
      "Epoch [205], val_loss: 14.1227\n",
      "gradient norm: 19.795384526252747, minimum ratio: 0.765625\n",
      "Epoch [206], val_loss: 14.2822\n",
      "gradient norm: 19.923970103263855, minimum ratio: 0.76953125\n",
      "Epoch [207], val_loss: 14.4430\n",
      "gradient norm: 20.05348688364029, minimum ratio: 0.75390625\n",
      "Epoch [208], val_loss: 14.6051\n",
      "gradient norm: 20.182883977890015, minimum ratio: 0.7578125\n",
      "Epoch [209], val_loss: 14.7685\n",
      "gradient norm: 20.313156366348267, minimum ratio: 0.75390625\n",
      "Epoch [210], val_loss: 14.9332\n",
      "gradient norm: 20.443967163562775, minimum ratio: 0.76953125\n",
      "Epoch [211], val_loss: 15.0991\n",
      "gradient norm: 20.57516449689865, minimum ratio: 0.76171875\n",
      "Epoch [212], val_loss: 15.2664\n",
      "gradient norm: 20.706835508346558, minimum ratio: 0.7734375\n",
      "Epoch [213], val_loss: 15.4351\n",
      "gradient norm: 20.838925182819366, minimum ratio: 0.74609375\n",
      "Epoch [214], val_loss: 15.6050\n",
      "gradient norm: 20.97147649526596, minimum ratio: 0.765625\n",
      "Epoch [215], val_loss: 15.7763\n",
      "gradient norm: 21.104323029518127, minimum ratio: 0.74609375\n",
      "Epoch [216], val_loss: 15.9489\n",
      "gradient norm: 21.23850190639496, minimum ratio: 0.76171875\n",
      "Epoch [217], val_loss: 16.1229\n",
      "gradient norm: 21.372909784317017, minimum ratio: 0.75\n",
      "Epoch [218], val_loss: 16.2983\n",
      "gradient norm: 21.507905304431915, minimum ratio: 0.734375\n",
      "Epoch [219], val_loss: 16.4750\n",
      "gradient norm: 21.642957866191864, minimum ratio: 0.74609375\n",
      "Epoch [220], val_loss: 16.6531\n",
      "gradient norm: 21.77872908115387, minimum ratio: 0.75\n",
      "Epoch [221], val_loss: 16.8326\n",
      "gradient norm: 21.915141582489014, minimum ratio: 0.74609375\n",
      "Epoch [222], val_loss: 17.0135\n",
      "gradient norm: 22.052405953407288, minimum ratio: 0.7421875\n",
      "Epoch [223], val_loss: 17.1958\n",
      "gradient norm: 22.19037163257599, minimum ratio: 0.75\n",
      "Epoch [224], val_loss: 17.3795\n",
      "gradient norm: 22.327941119670868, minimum ratio: 0.76171875\n",
      "Epoch [225], val_loss: 17.5647\n",
      "gradient norm: 22.466145873069763, minimum ratio: 0.75390625\n",
      "Epoch [226], val_loss: 17.7512\n",
      "gradient norm: 22.604863345623016, minimum ratio: 0.74609375\n",
      "Epoch [227], val_loss: 17.9392\n",
      "gradient norm: 22.7449631690979, minimum ratio: 0.7734375\n",
      "Epoch [228], val_loss: 18.1286\n",
      "gradient norm: 22.88552051782608, minimum ratio: 0.75390625\n",
      "Epoch [229], val_loss: 18.3195\n",
      "gradient norm: 23.02625221014023, minimum ratio: 0.765625\n",
      "Epoch [230], val_loss: 18.5118\n",
      "gradient norm: 23.167146027088165, minimum ratio: 0.7734375\n",
      "Epoch [231], val_loss: 18.7056\n",
      "gradient norm: 23.308553636074066, minimum ratio: 0.76171875\n",
      "Epoch [232], val_loss: 18.9009\n",
      "gradient norm: 23.451205790042877, minimum ratio: 0.765625\n",
      "Epoch [233], val_loss: 19.0976\n",
      "gradient norm: 23.59380578994751, minimum ratio: 0.7734375\n",
      "Epoch [234], val_loss: 19.2959\n",
      "gradient norm: 23.737947285175323, minimum ratio: 0.76171875\n",
      "Epoch [235], val_loss: 19.4956\n",
      "gradient norm: 23.88148272037506, minimum ratio: 0.76953125\n",
      "Epoch [236], val_loss: 19.6969\n",
      "gradient norm: 24.025334239006042, minimum ratio: 0.75\n",
      "Epoch [237], val_loss: 19.8996\n",
      "gradient norm: 24.170790195465088, minimum ratio: 0.78125\n",
      "Epoch [238], val_loss: 20.1039\n",
      "gradient norm: 24.31643670797348, minimum ratio: 0.76953125\n",
      "Epoch [239], val_loss: 20.3097\n",
      "gradient norm: 24.46280688047409, minimum ratio: 0.75\n",
      "Epoch [240], val_loss: 20.5171\n",
      "gradient norm: 24.609951555728912, minimum ratio: 0.76171875\n",
      "Epoch [241], val_loss: 20.7260\n",
      "gradient norm: 24.757156789302826, minimum ratio: 0.78125\n",
      "Epoch [242], val_loss: 20.9365\n",
      "gradient norm: 24.90558785200119, minimum ratio: 0.76171875\n",
      "Epoch [243], val_loss: 21.1486\n",
      "gradient norm: 25.053687632083893, minimum ratio: 0.7734375\n",
      "Epoch [244], val_loss: 21.3622\n",
      "gradient norm: 25.202625393867493, minimum ratio: 0.75\n",
      "Epoch [245], val_loss: 21.5774\n",
      "gradient norm: 25.35180902481079, minimum ratio: 0.7890625\n",
      "Epoch [246], val_loss: 21.7942\n",
      "gradient norm: 25.50300705432892, minimum ratio: 0.76171875\n",
      "Epoch [247], val_loss: 22.0126\n",
      "gradient norm: 25.653994917869568, minimum ratio: 0.74609375\n",
      "Epoch [248], val_loss: 22.2326\n",
      "gradient norm: 25.805219769477844, minimum ratio: 0.75\n",
      "Epoch [249], val_loss: 22.4543\n",
      "gradient norm: 25.956881999969482, minimum ratio: 0.76953125\n",
      "Epoch [250], val_loss: 22.6776\n",
      "gradient norm: 26.109384059906006, minimum ratio: 0.75\n",
      "Epoch [251], val_loss: 22.9025\n",
      "gradient norm: 26.262591004371643, minimum ratio: 0.77734375\n",
      "Epoch [252], val_loss: 23.1291\n",
      "gradient norm: 26.417019724845886, minimum ratio: 0.765625\n",
      "Epoch [253], val_loss: 23.3573\n",
      "gradient norm: 26.571117043495178, minimum ratio: 0.7734375\n",
      "Epoch [254], val_loss: 23.5872\n",
      "gradient norm: 26.72601592540741, minimum ratio: 0.76953125\n",
      "Epoch [255], val_loss: 23.8188\n",
      "gradient norm: 26.88122820854187, minimum ratio: 0.7421875\n",
      "Epoch [256], val_loss: 24.0520\n",
      "gradient norm: 27.037115573883057, minimum ratio: 0.75\n",
      "Epoch [257], val_loss: 24.2870\n",
      "gradient norm: 27.19456446170807, minimum ratio: 0.765625\n",
      "Epoch [258], val_loss: 24.5236\n",
      "gradient norm: 27.351747393608093, minimum ratio: 0.76953125\n",
      "Epoch [259], val_loss: 24.7619\n",
      "gradient norm: 27.509708166122437, minimum ratio: 0.765625\n",
      "Epoch [260], val_loss: 25.0020\n",
      "gradient norm: 27.667835235595703, minimum ratio: 0.7578125\n",
      "Epoch [261], val_loss: 25.2438\n",
      "gradient norm: 27.826779961586, minimum ratio: 0.78515625\n",
      "Epoch [262], val_loss: 25.4873\n",
      "gradient norm: 27.986095666885376, minimum ratio: 0.74609375\n",
      "Epoch [263], val_loss: 25.7326\n",
      "gradient norm: 28.146803498268127, minimum ratio: 0.7578125\n",
      "Epoch [264], val_loss: 25.9796\n",
      "gradient norm: 28.30738341808319, minimum ratio: 0.7578125\n",
      "Epoch [265], val_loss: 26.2285\n",
      "gradient norm: 28.46953284740448, minimum ratio: 0.7734375\n",
      "Epoch [266], val_loss: 26.4790\n",
      "gradient norm: 28.631513595581055, minimum ratio: 0.77734375\n",
      "Epoch [267], val_loss: 26.7314\n",
      "gradient norm: 28.79410672187805, minimum ratio: 0.75\n",
      "Epoch [268], val_loss: 26.9856\n",
      "gradient norm: 28.957695960998535, minimum ratio: 0.74609375\n",
      "Epoch [269], val_loss: 27.2416\n",
      "gradient norm: 29.122185945510864, minimum ratio: 0.73828125\n",
      "Epoch [270], val_loss: 27.4994\n",
      "gradient norm: 29.28665041923523, minimum ratio: 0.76171875\n",
      "Epoch [271], val_loss: 27.7591\n",
      "gradient norm: 29.452451825141907, minimum ratio: 0.75390625\n",
      "Epoch [272], val_loss: 28.0205\n",
      "gradient norm: 29.61804223060608, minimum ratio: 0.7578125\n",
      "Epoch [273], val_loss: 28.2838\n",
      "gradient norm: 29.784618139266968, minimum ratio: 0.7734375\n",
      "Epoch [274], val_loss: 28.5490\n",
      "gradient norm: 29.951084971427917, minimum ratio: 0.76953125\n",
      "Epoch [275], val_loss: 28.8160\n",
      "gradient norm: 30.118988156318665, minimum ratio: 0.7578125\n",
      "Epoch [276], val_loss: 29.0849\n",
      "gradient norm: 30.287484526634216, minimum ratio: 0.7734375\n",
      "Epoch [277], val_loss: 29.3557\n",
      "gradient norm: 30.4575617313385, minimum ratio: 0.7578125\n",
      "Epoch [278], val_loss: 29.6284\n",
      "gradient norm: 30.627432703971863, minimum ratio: 0.76953125\n",
      "Epoch [279], val_loss: 29.9030\n",
      "gradient norm: 30.7976633310318, minimum ratio: 0.80078125\n",
      "Epoch [280], val_loss: 30.1795\n",
      "gradient norm: 30.96788203716278, minimum ratio: 0.76171875\n",
      "Epoch [281], val_loss: 30.4580\n",
      "gradient norm: 31.139859557151794, minimum ratio: 0.7734375\n",
      "Epoch [282], val_loss: 30.7383\n",
      "gradient norm: 31.312097907066345, minimum ratio: 0.75390625\n",
      "Epoch [283], val_loss: 31.0206\n",
      "gradient norm: 31.4859778881073, minimum ratio: 0.796875\n",
      "Epoch [284], val_loss: 31.3048\n",
      "gradient norm: 31.65953528881073, minimum ratio: 0.7890625\n",
      "Epoch [285], val_loss: 31.5910\n",
      "gradient norm: 31.832794547080994, minimum ratio: 0.76171875\n",
      "Epoch [286], val_loss: 31.8792\n",
      "gradient norm: 32.00792992115021, minimum ratio: 0.7734375\n",
      "Epoch [287], val_loss: 32.1694\n",
      "gradient norm: 32.18345582485199, minimum ratio: 0.76171875\n",
      "Epoch [288], val_loss: 32.4616\n",
      "gradient norm: 32.35963523387909, minimum ratio: 0.76171875\n",
      "Epoch [289], val_loss: 32.7557\n",
      "gradient norm: 32.5371755361557, minimum ratio: 0.75\n",
      "Epoch [290], val_loss: 33.0519\n",
      "gradient norm: 32.71473002433777, minimum ratio: 0.75\n",
      "Epoch [291], val_loss: 33.3502\n",
      "gradient norm: 32.89196443557739, minimum ratio: 0.75390625\n",
      "Epoch [292], val_loss: 33.6504\n",
      "gradient norm: 33.07160496711731, minimum ratio: 0.765625\n",
      "Epoch [293], val_loss: 33.9527\n",
      "gradient norm: 33.25058472156525, minimum ratio: 0.765625\n",
      "Epoch [294], val_loss: 34.2570\n",
      "gradient norm: 33.431074142456055, minimum ratio: 0.78515625\n",
      "Epoch [295], val_loss: 34.5635\n",
      "gradient norm: 33.611759662628174, minimum ratio: 0.7734375\n",
      "Epoch [296], val_loss: 34.8719\n",
      "gradient norm: 33.793330669403076, minimum ratio: 0.7890625\n",
      "Epoch [297], val_loss: 35.1825\n",
      "gradient norm: 33.975273966789246, minimum ratio: 0.7578125\n",
      "Epoch [298], val_loss: 35.4951\n",
      "gradient norm: 34.1580536365509, minimum ratio: 0.75390625\n",
      "Epoch [299], val_loss: 35.8099\n",
      "gradient norm: 34.34217381477356, minimum ratio: 0.78125\n",
      "Epoch [300], val_loss: 36.1268\n",
      "gradient norm: 34.526695251464844, minimum ratio: 0.765625\n",
      "Epoch [301], val_loss: 36.4457\n",
      "gradient norm: 34.711416244506836, minimum ratio: 0.7734375\n",
      "Epoch [302], val_loss: 36.7669\n",
      "gradient norm: 34.89703023433685, minimum ratio: 0.75390625\n",
      "Epoch [303], val_loss: 37.0901\n",
      "gradient norm: 35.08287703990936, minimum ratio: 0.7421875\n",
      "Epoch [304], val_loss: 37.4156\n",
      "gradient norm: 35.26929593086243, minimum ratio: 0.76171875\n",
      "Epoch [305], val_loss: 37.7431\n",
      "gradient norm: 35.457539796829224, minimum ratio: 0.73828125\n",
      "Epoch [306], val_loss: 38.0728\n",
      "gradient norm: 35.64583492279053, minimum ratio: 0.75390625\n",
      "Epoch [307], val_loss: 38.4048\n",
      "gradient norm: 35.83448910713196, minimum ratio: 0.7734375\n",
      "Epoch [308], val_loss: 38.7389\n",
      "gradient norm: 36.02384388446808, minimum ratio: 0.73828125\n",
      "Epoch [309], val_loss: 39.0752\n",
      "gradient norm: 36.21467399597168, minimum ratio: 0.7421875\n",
      "Epoch [310], val_loss: 39.4137\n",
      "gradient norm: 36.405001163482666, minimum ratio: 0.76171875\n",
      "Epoch [311], val_loss: 39.7545\n",
      "gradient norm: 36.597424149513245, minimum ratio: 0.7578125\n",
      "Epoch [312], val_loss: 40.0975\n",
      "gradient norm: 36.78995382785797, minimum ratio: 0.74609375\n",
      "Epoch [313], val_loss: 40.4427\n",
      "gradient norm: 36.98275077342987, minimum ratio: 0.75390625\n",
      "Epoch [314], val_loss: 40.7902\n",
      "gradient norm: 37.17578172683716, minimum ratio: 0.765625\n",
      "Epoch [315], val_loss: 41.1399\n",
      "gradient norm: 37.3707093000412, minimum ratio: 0.765625\n",
      "Epoch [316], val_loss: 41.4920\n",
      "gradient norm: 37.56574988365173, minimum ratio: 0.7578125\n",
      "Epoch [317], val_loss: 41.8463\n",
      "gradient norm: 37.761948227882385, minimum ratio: 0.7734375\n",
      "Epoch [318], val_loss: 42.2029\n",
      "gradient norm: 37.958500266075134, minimum ratio: 0.75390625\n",
      "Epoch [319], val_loss: 42.5618\n",
      "gradient norm: 38.15461277961731, minimum ratio: 0.7734375\n",
      "Epoch [320], val_loss: 42.9230\n",
      "gradient norm: 38.35251784324646, minimum ratio: 0.76171875\n",
      "Epoch [321], val_loss: 43.2866\n",
      "gradient norm: 38.55046081542969, minimum ratio: 0.7578125\n",
      "Epoch [322], val_loss: 43.6525\n",
      "gradient norm: 38.75088107585907, minimum ratio: 0.765625\n",
      "Epoch [323], val_loss: 44.0208\n",
      "gradient norm: 38.95126914978027, minimum ratio: 0.765625\n",
      "Epoch [324], val_loss: 44.3915\n",
      "gradient norm: 39.1523163318634, minimum ratio: 0.7890625\n",
      "Epoch [325], val_loss: 44.7646\n",
      "gradient norm: 39.35311758518219, minimum ratio: 0.76171875\n",
      "Epoch [326], val_loss: 45.1399\n",
      "gradient norm: 39.5550080537796, minimum ratio: 0.765625\n",
      "Epoch [327], val_loss: 45.5177\n",
      "gradient norm: 39.75765287876129, minimum ratio: 0.76953125\n",
      "Epoch [328], val_loss: 45.8978\n",
      "gradient norm: 39.96140432357788, minimum ratio: 0.7734375\n",
      "Epoch [329], val_loss: 46.2804\n",
      "gradient norm: 40.164998292922974, minimum ratio: 0.765625\n",
      "Epoch [330], val_loss: 46.6654\n",
      "gradient norm: 40.37116885185242, minimum ratio: 0.78515625\n",
      "Epoch [331], val_loss: 47.0529\n",
      "gradient norm: 40.57639479637146, minimum ratio: 0.76171875\n",
      "Epoch [332], val_loss: 47.4429\n",
      "gradient norm: 40.78210461139679, minimum ratio: 0.7734375\n",
      "Epoch [333], val_loss: 47.8353\n",
      "gradient norm: 40.98922872543335, minimum ratio: 0.7421875\n",
      "Epoch [334], val_loss: 48.2302\n",
      "gradient norm: 41.1967476606369, minimum ratio: 0.76953125\n",
      "Epoch [335], val_loss: 48.6276\n",
      "gradient norm: 41.40601968765259, minimum ratio: 0.79296875\n",
      "Epoch [336], val_loss: 49.0276\n",
      "gradient norm: 41.615853786468506, minimum ratio: 0.76953125\n",
      "Epoch [337], val_loss: 49.4300\n",
      "gradient norm: 41.82501554489136, minimum ratio: 0.76953125\n",
      "Epoch [338], val_loss: 49.8349\n",
      "gradient norm: 42.0361692905426, minimum ratio: 0.78125\n",
      "Epoch [339], val_loss: 50.2424\n",
      "gradient norm: 42.24769401550293, minimum ratio: 0.7578125\n",
      "Epoch [340], val_loss: 50.6523\n",
      "gradient norm: 42.459651470184326, minimum ratio: 0.77734375\n",
      "Epoch [341], val_loss: 51.0648\n",
      "gradient norm: 42.672627449035645, minimum ratio: 0.765625\n",
      "Epoch [342], val_loss: 51.4799\n",
      "gradient norm: 42.88690543174744, minimum ratio: 0.76953125\n",
      "Epoch [343], val_loss: 51.8977\n",
      "gradient norm: 43.101139545440674, minimum ratio: 0.76171875\n",
      "Epoch [344], val_loss: 52.3180\n",
      "gradient norm: 43.31584692001343, minimum ratio: 0.765625\n",
      "Epoch [345], val_loss: 52.7409\n",
      "gradient norm: 43.531800508499146, minimum ratio: 0.78125\n",
      "Epoch [346], val_loss: 53.1664\n",
      "gradient norm: 43.748088359832764, minimum ratio: 0.7421875\n",
      "Epoch [347], val_loss: 53.5945\n",
      "gradient norm: 43.9651415348053, minimum ratio: 0.7578125\n",
      "Epoch [348], val_loss: 54.0253\n",
      "gradient norm: 44.18348789215088, minimum ratio: 0.78125\n",
      "Epoch [349], val_loss: 54.4587\n",
      "gradient norm: 44.40266466140747, minimum ratio: 0.765625\n",
      "Epoch [350], val_loss: 54.8948\n",
      "gradient norm: 44.62206697463989, minimum ratio: 0.75390625\n",
      "Epoch [351], val_loss: 55.3336\n",
      "gradient norm: 44.84123253822327, minimum ratio: 0.80859375\n",
      "Epoch [352], val_loss: 55.7750\n",
      "gradient norm: 45.06248474121094, minimum ratio: 0.7734375\n",
      "Epoch [353], val_loss: 56.2192\n",
      "gradient norm: 45.284648180007935, minimum ratio: 0.765625\n",
      "Epoch [354], val_loss: 56.6661\n",
      "gradient norm: 45.50638818740845, minimum ratio: 0.76171875\n",
      "Epoch [355], val_loss: 57.1157\n",
      "gradient norm: 45.73100233078003, minimum ratio: 0.765625\n",
      "Epoch [356], val_loss: 57.5682\n",
      "gradient norm: 45.954872846603394, minimum ratio: 0.74609375\n",
      "Epoch [357], val_loss: 58.0233\n",
      "gradient norm: 46.17931509017944, minimum ratio: 0.75390625\n",
      "Epoch [358], val_loss: 58.4812\n",
      "gradient norm: 46.40478491783142, minimum ratio: 0.77734375\n",
      "Epoch [359], val_loss: 58.9418\n",
      "gradient norm: 46.63083624839783, minimum ratio: 0.74609375\n",
      "Epoch [360], val_loss: 59.4052\n",
      "gradient norm: 46.85735464096069, minimum ratio: 0.76953125\n",
      "Epoch [361], val_loss: 59.8713\n",
      "gradient norm: 47.08517265319824, minimum ratio: 0.7890625\n",
      "Epoch [362], val_loss: 60.3403\n",
      "gradient norm: 47.315747022628784, minimum ratio: 0.77734375\n",
      "Epoch [363], val_loss: 60.8121\n",
      "gradient norm: 47.544921875, minimum ratio: 0.76171875\n",
      "Epoch [364], val_loss: 61.2866\n",
      "gradient norm: 47.7750985622406, minimum ratio: 0.76953125\n",
      "Epoch [365], val_loss: 61.7640\n",
      "gradient norm: 48.00611090660095, minimum ratio: 0.7421875\n",
      "Epoch [366], val_loss: 62.2443\n",
      "gradient norm: 48.23690342903137, minimum ratio: 0.76953125\n",
      "Epoch [367], val_loss: 62.7274\n",
      "gradient norm: 48.46934962272644, minimum ratio: 0.765625\n",
      "Epoch [368], val_loss: 63.2134\n",
      "gradient norm: 48.702821254730225, minimum ratio: 0.75\n",
      "Epoch [369], val_loss: 63.7024\n",
      "gradient norm: 48.93725299835205, minimum ratio: 0.765625\n",
      "Epoch [370], val_loss: 64.1942\n",
      "gradient norm: 49.17164993286133, minimum ratio: 0.7578125\n",
      "Epoch [371], val_loss: 64.6889\n",
      "gradient norm: 49.40685796737671, minimum ratio: 0.77734375\n",
      "Epoch [372], val_loss: 65.1866\n",
      "gradient norm: 49.642576456069946, minimum ratio: 0.73046875\n",
      "Epoch [373], val_loss: 65.6872\n",
      "gradient norm: 49.87927174568176, minimum ratio: 0.75390625\n",
      "Epoch [374], val_loss: 66.1907\n",
      "gradient norm: 50.117311239242554, minimum ratio: 0.73046875\n",
      "Epoch [375], val_loss: 66.6972\n",
      "gradient norm: 50.35818576812744, minimum ratio: 0.73046875\n",
      "Epoch [376], val_loss: 67.2067\n",
      "gradient norm: 50.59649991989136, minimum ratio: 0.76171875\n",
      "Epoch [377], val_loss: 67.7191\n",
      "gradient norm: 50.83659648895264, minimum ratio: 0.7578125\n",
      "Epoch [378], val_loss: 68.2346\n",
      "gradient norm: 51.077489137649536, minimum ratio: 0.7578125\n",
      "Epoch [379], val_loss: 68.7530\n",
      "gradient norm: 51.31845188140869, minimum ratio: 0.7578125\n",
      "Epoch [380], val_loss: 69.2745\n",
      "gradient norm: 51.56025958061218, minimum ratio: 0.76171875\n",
      "Epoch [381], val_loss: 69.7990\n",
      "gradient norm: 51.804028034210205, minimum ratio: 0.75\n",
      "Epoch [382], val_loss: 70.3265\n",
      "gradient norm: 52.04923129081726, minimum ratio: 0.76171875\n",
      "Epoch [383], val_loss: 70.8572\n",
      "gradient norm: 52.293827056884766, minimum ratio: 0.77734375\n",
      "Epoch [384], val_loss: 71.3909\n",
      "gradient norm: 52.53936219215393, minimum ratio: 0.7578125\n",
      "Epoch [385], val_loss: 71.9277\n",
      "gradient norm: 52.78448987007141, minimum ratio: 0.765625\n",
      "Epoch [386], val_loss: 72.4676\n",
      "gradient norm: 53.032045125961304, minimum ratio: 0.7421875\n",
      "Epoch [387], val_loss: 73.0107\n",
      "gradient norm: 53.280744552612305, minimum ratio: 0.765625\n",
      "Epoch [388], val_loss: 73.5568\n",
      "gradient norm: 53.52993321418762, minimum ratio: 0.76171875\n",
      "Epoch [389], val_loss: 74.1061\n",
      "gradient norm: 53.78042936325073, minimum ratio: 0.76953125\n",
      "Epoch [390], val_loss: 74.6585\n",
      "gradient norm: 54.03025841712952, minimum ratio: 0.734375\n",
      "Epoch [391], val_loss: 75.2141\n",
      "gradient norm: 54.28094720840454, minimum ratio: 0.76171875\n",
      "Epoch [392], val_loss: 75.7728\n",
      "gradient norm: 54.53248167037964, minimum ratio: 0.78125\n",
      "Epoch [393], val_loss: 76.3347\n",
      "gradient norm: 54.78584146499634, minimum ratio: 0.74609375\n",
      "Epoch [394], val_loss: 76.8998\n",
      "gradient norm: 55.04027986526489, minimum ratio: 0.75390625\n",
      "Epoch [395], val_loss: 77.4681\n",
      "gradient norm: 55.294599771499634, minimum ratio: 0.7734375\n",
      "Epoch [396], val_loss: 78.0397\n",
      "gradient norm: 55.550779819488525, minimum ratio: 0.78125\n",
      "Epoch [397], val_loss: 78.6145\n",
      "gradient norm: 55.806273221969604, minimum ratio: 0.75\n",
      "Epoch [398], val_loss: 79.1925\n",
      "gradient norm: 56.061766147613525, minimum ratio: 0.76953125\n",
      "Epoch [399], val_loss: 79.7738\n",
      "gradient norm: 56.32023644447327, minimum ratio: 0.7578125\n",
      "Epoch [400], val_loss: 80.3583\n",
      "gradient norm: 56.57847046852112, minimum ratio: 0.765625\n",
      "Epoch [401], val_loss: 80.9462\n",
      "gradient norm: 56.836925983428955, minimum ratio: 0.73828125\n",
      "Epoch [402], val_loss: 81.5374\n",
      "gradient norm: 57.09808135032654, minimum ratio: 0.765625\n",
      "Epoch [403], val_loss: 82.1320\n",
      "gradient norm: 57.3594012260437, minimum ratio: 0.7734375\n",
      "Epoch [404], val_loss: 82.7299\n",
      "gradient norm: 57.62058424949646, minimum ratio: 0.75\n",
      "Epoch [405], val_loss: 83.3311\n",
      "gradient norm: 57.883185625076294, minimum ratio: 0.76171875\n",
      "Epoch [406], val_loss: 83.9356\n",
      "gradient norm: 58.147400856018066, minimum ratio: 0.76171875\n",
      "Epoch [407], val_loss: 84.5435\n",
      "gradient norm: 58.41097831726074, minimum ratio: 0.7734375\n",
      "Epoch [408], val_loss: 85.1548\n",
      "gradient norm: 58.675878286361694, minimum ratio: 0.76171875\n",
      "Epoch [409], val_loss: 85.7694\n",
      "gradient norm: 58.94222044944763, minimum ratio: 0.75390625\n",
      "Epoch [410], val_loss: 86.3874\n",
      "gradient norm: 59.20985126495361, minimum ratio: 0.7734375\n",
      "Epoch [411], val_loss: 87.0089\n",
      "gradient norm: 59.47554850578308, minimum ratio: 0.765625\n",
      "Epoch [412], val_loss: 87.6338\n",
      "gradient norm: 59.7449209690094, minimum ratio: 0.7734375\n",
      "Epoch [413], val_loss: 88.2621\n",
      "gradient norm: 60.013166666030884, minimum ratio: 0.75390625\n",
      "Epoch [414], val_loss: 88.8939\n",
      "gradient norm: 60.283287048339844, minimum ratio: 0.76171875\n",
      "Epoch [415], val_loss: 89.5292\n",
      "gradient norm: 60.55221366882324, minimum ratio: 0.765625\n",
      "Epoch [416], val_loss: 90.1680\n",
      "gradient norm: 60.82420635223389, minimum ratio: 0.75\n",
      "Epoch [417], val_loss: 90.8103\n",
      "gradient norm: 61.097697496414185, minimum ratio: 0.75\n",
      "Epoch [418], val_loss: 91.4562\n",
      "gradient norm: 61.371363162994385, minimum ratio: 0.76953125\n",
      "Epoch [419], val_loss: 92.1056\n",
      "gradient norm: 61.645644664764404, minimum ratio: 0.7421875\n",
      "Epoch [420], val_loss: 92.7586\n",
      "gradient norm: 61.91996765136719, minimum ratio: 0.76953125\n",
      "Epoch [421], val_loss: 93.4152\n",
      "gradient norm: 62.19692015647888, minimum ratio: 0.765625\n",
      "Epoch [422], val_loss: 94.0752\n",
      "gradient norm: 62.473769426345825, minimum ratio: 0.74609375\n",
      "Epoch [423], val_loss: 94.7389\n",
      "gradient norm: 62.75059986114502, minimum ratio: 0.73828125\n",
      "Epoch [424], val_loss: 95.4061\n",
      "gradient norm: 63.0295844078064, minimum ratio: 0.76171875\n",
      "Epoch [425], val_loss: 96.0770\n",
      "gradient norm: 63.30950951576233, minimum ratio: 0.76953125\n",
      "Epoch [426], val_loss: 96.7515\n",
      "gradient norm: 63.58990144729614, minimum ratio: 0.76171875\n",
      "Epoch [427], val_loss: 97.4297\n",
      "gradient norm: 63.86988687515259, minimum ratio: 0.7578125\n",
      "Epoch [428], val_loss: 98.1116\n",
      "gradient norm: 64.1509850025177, minimum ratio: 0.7578125\n",
      "Epoch [429], val_loss: 98.7970\n",
      "gradient norm: 64.4333107471466, minimum ratio: 0.77734375\n",
      "Epoch [430], val_loss: 99.4861\n",
      "gradient norm: 64.71645212173462, minimum ratio: 0.76953125\n",
      "Epoch [431], val_loss: 100.1789\n",
      "gradient norm: 64.99931836128235, minimum ratio: 0.78125\n",
      "Epoch [432], val_loss: 100.8755\n",
      "gradient norm: 65.28708052635193, minimum ratio: 0.7734375\n",
      "Epoch [433], val_loss: 101.5758\n",
      "gradient norm: 65.57257390022278, minimum ratio: 0.76953125\n",
      "Epoch [434], val_loss: 102.2798\n",
      "gradient norm: 65.85958743095398, minimum ratio: 0.765625\n",
      "Epoch [435], val_loss: 102.9876\n",
      "gradient norm: 66.14703631401062, minimum ratio: 0.78515625\n",
      "Epoch [436], val_loss: 103.6992\n",
      "gradient norm: 66.43601369857788, minimum ratio: 0.77734375\n",
      "Epoch [437], val_loss: 104.4146\n",
      "gradient norm: 66.72554564476013, minimum ratio: 0.77734375\n",
      "Epoch [438], val_loss: 105.1337\n",
      "gradient norm: 67.01695466041565, minimum ratio: 0.75390625\n",
      "Epoch [439], val_loss: 105.8566\n",
      "gradient norm: 67.30804824829102, minimum ratio: 0.75390625\n",
      "Epoch [440], val_loss: 106.5833\n",
      "gradient norm: 67.60087466239929, minimum ratio: 0.77734375\n",
      "Epoch [441], val_loss: 107.3138\n",
      "gradient norm: 67.89265775680542, minimum ratio: 0.76171875\n",
      "Epoch [442], val_loss: 108.0482\n",
      "gradient norm: 68.18528914451599, minimum ratio: 0.75\n",
      "Epoch [443], val_loss: 108.7866\n",
      "gradient norm: 68.47947454452515, minimum ratio: 0.79296875\n",
      "Epoch [444], val_loss: 109.5288\n",
      "gradient norm: 68.7748351097107, minimum ratio: 0.78125\n",
      "Epoch [445], val_loss: 110.2750\n",
      "gradient norm: 69.07330465316772, minimum ratio: 0.765625\n",
      "Epoch [446], val_loss: 111.0251\n",
      "gradient norm: 69.36953115463257, minimum ratio: 0.76953125\n",
      "Epoch [447], val_loss: 111.7792\n",
      "gradient norm: 69.66914892196655, minimum ratio: 0.76171875\n",
      "Epoch [448], val_loss: 112.5372\n",
      "gradient norm: 69.96776247024536, minimum ratio: 0.76171875\n",
      "Epoch [449], val_loss: 113.2991\n",
      "gradient norm: 70.26648330688477, minimum ratio: 0.76171875\n",
      "Epoch [450], val_loss: 114.0650\n",
      "gradient norm: 70.56741905212402, minimum ratio: 0.765625\n",
      "Epoch [451], val_loss: 114.8348\n",
      "gradient norm: 70.87000036239624, minimum ratio: 0.7734375\n",
      "Epoch [452], val_loss: 115.6087\n",
      "gradient norm: 71.17323684692383, minimum ratio: 0.7734375\n",
      "Epoch [453], val_loss: 116.3866\n",
      "gradient norm: 71.47639322280884, minimum ratio: 0.76171875\n",
      "Epoch [454], val_loss: 117.1686\n",
      "gradient norm: 71.78211545944214, minimum ratio: 0.765625\n",
      "Epoch [455], val_loss: 117.9546\n",
      "gradient norm: 72.08731317520142, minimum ratio: 0.75\n",
      "Epoch [456], val_loss: 118.7447\n",
      "gradient norm: 72.39252281188965, minimum ratio: 0.76953125\n",
      "Epoch [457], val_loss: 119.5389\n",
      "gradient norm: 72.69874620437622, minimum ratio: 0.77734375\n",
      "Epoch [458], val_loss: 120.3371\n",
      "gradient norm: 73.00729942321777, minimum ratio: 0.74609375\n",
      "Epoch [459], val_loss: 121.1395\n",
      "gradient norm: 73.3165602684021, minimum ratio: 0.75390625\n",
      "Epoch [460], val_loss: 121.9459\n",
      "gradient norm: 73.62639331817627, minimum ratio: 0.78515625\n",
      "Epoch [461], val_loss: 122.7564\n",
      "gradient norm: 73.9365782737732, minimum ratio: 0.75390625\n",
      "Epoch [462], val_loss: 123.5711\n",
      "gradient norm: 74.25089597702026, minimum ratio: 0.765625\n",
      "Epoch [463], val_loss: 124.3899\n",
      "gradient norm: 74.5626540184021, minimum ratio: 0.76171875\n",
      "Epoch [464], val_loss: 125.2130\n",
      "gradient norm: 74.87406158447266, minimum ratio: 0.765625\n",
      "Epoch [465], val_loss: 126.0402\n",
      "gradient norm: 75.18792247772217, minimum ratio: 0.76171875\n",
      "Epoch [466], val_loss: 126.8717\n",
      "gradient norm: 75.50291681289673, minimum ratio: 0.7578125\n",
      "Epoch [467], val_loss: 127.7073\n",
      "gradient norm: 75.8198561668396, minimum ratio: 0.75390625\n",
      "Epoch [468], val_loss: 128.5473\n",
      "gradient norm: 76.13807535171509, minimum ratio: 0.7578125\n",
      "Epoch [469], val_loss: 129.3915\n",
      "gradient norm: 76.4561710357666, minimum ratio: 0.74609375\n",
      "Epoch [470], val_loss: 130.2400\n",
      "gradient norm: 76.77551746368408, minimum ratio: 0.77734375\n",
      "Epoch [471], val_loss: 131.0927\n",
      "gradient norm: 77.0955138206482, minimum ratio: 0.734375\n",
      "Epoch [472], val_loss: 131.9498\n",
      "gradient norm: 77.41522884368896, minimum ratio: 0.76953125\n",
      "Epoch [473], val_loss: 132.8113\n",
      "gradient norm: 77.73590469360352, minimum ratio: 0.75390625\n",
      "Epoch [474], val_loss: 133.6771\n",
      "gradient norm: 78.05776977539062, minimum ratio: 0.75\n",
      "Epoch [475], val_loss: 134.5471\n",
      "gradient norm: 78.38155364990234, minimum ratio: 0.78125\n",
      "Epoch [476], val_loss: 135.4215\n",
      "gradient norm: 78.70490074157715, minimum ratio: 0.7734375\n",
      "Epoch [477], val_loss: 136.3002\n",
      "gradient norm: 79.03034973144531, minimum ratio: 0.765625\n",
      "Epoch [478], val_loss: 137.1833\n",
      "gradient norm: 79.35782814025879, minimum ratio: 0.76953125\n",
      "Epoch [479], val_loss: 138.0708\n",
      "gradient norm: 79.68462562561035, minimum ratio: 0.76953125\n",
      "Epoch [480], val_loss: 138.9627\n",
      "gradient norm: 80.01269626617432, minimum ratio: 0.7578125\n",
      "Epoch [481], val_loss: 139.8591\n",
      "gradient norm: 80.34200429916382, minimum ratio: 0.7578125\n",
      "Epoch [482], val_loss: 140.7600\n",
      "gradient norm: 80.66897106170654, minimum ratio: 0.76171875\n",
      "Epoch [483], val_loss: 141.6654\n",
      "gradient norm: 81.00171041488647, minimum ratio: 0.765625\n",
      "Epoch [484], val_loss: 142.5752\n",
      "gradient norm: 81.33387994766235, minimum ratio: 0.76171875\n",
      "Epoch [485], val_loss: 143.4895\n",
      "gradient norm: 81.6668610572815, minimum ratio: 0.75\n",
      "Epoch [486], val_loss: 144.4083\n",
      "gradient norm: 82.00052261352539, minimum ratio: 0.75390625\n",
      "Epoch [487], val_loss: 145.3316\n",
      "gradient norm: 82.3354926109314, minimum ratio: 0.75390625\n",
      "Epoch [488], val_loss: 146.2594\n",
      "gradient norm: 82.67068529129028, minimum ratio: 0.7578125\n",
      "Epoch [489], val_loss: 147.1918\n",
      "gradient norm: 83.00695753097534, minimum ratio: 0.7578125\n",
      "Epoch [490], val_loss: 148.1287\n",
      "gradient norm: 83.34126091003418, minimum ratio: 0.75390625\n",
      "Epoch [491], val_loss: 149.0703\n",
      "gradient norm: 83.67936611175537, minimum ratio: 0.76953125\n",
      "Epoch [492], val_loss: 150.0164\n",
      "gradient norm: 84.01873588562012, minimum ratio: 0.76171875\n",
      "Epoch [493], val_loss: 150.9672\n",
      "gradient norm: 84.36116170883179, minimum ratio: 0.78125\n",
      "Epoch [494], val_loss: 151.9226\n",
      "gradient norm: 84.70337104797363, minimum ratio: 0.76953125\n",
      "Epoch [495], val_loss: 152.8826\n",
      "gradient norm: 85.04441165924072, minimum ratio: 0.7734375\n",
      "Epoch [496], val_loss: 153.8474\n",
      "gradient norm: 85.38787841796875, minimum ratio: 0.7734375\n",
      "Epoch [497], val_loss: 154.8167\n",
      "gradient norm: 85.72995281219482, minimum ratio: 0.7734375\n",
      "Epoch [498], val_loss: 155.7909\n",
      "gradient norm: 86.07561540603638, minimum ratio: 0.7578125\n",
      "Epoch [499], val_loss: 156.7698\n",
      "gradient norm: 86.41982746124268, minimum ratio: 0.78125\n",
      "Epoch [500], val_loss: 157.7535\n",
      "gradient norm: 86.76539182662964, minimum ratio: 0.76171875\n",
      "Epoch [501], val_loss: 158.7419\n",
      "gradient norm: 87.11550903320312, minimum ratio: 0.76953125\n",
      "Epoch [502], val_loss: 159.7350\n",
      "gradient norm: 87.46610021591187, minimum ratio: 0.7421875\n",
      "Epoch [503], val_loss: 160.7328\n",
      "gradient norm: 87.81596946716309, minimum ratio: 0.765625\n",
      "Epoch [504], val_loss: 161.7355\n",
      "gradient norm: 88.1643295288086, minimum ratio: 0.7734375\n",
      "Epoch [505], val_loss: 162.7429\n",
      "gradient norm: 88.51622009277344, minimum ratio: 0.75390625\n",
      "Epoch [506], val_loss: 163.7551\n",
      "gradient norm: 88.86709642410278, minimum ratio: 0.76171875\n",
      "Epoch [507], val_loss: 164.7721\n",
      "gradient norm: 89.22138261795044, minimum ratio: 0.734375\n",
      "Epoch [508], val_loss: 165.7940\n",
      "gradient norm: 89.5753846168518, minimum ratio: 0.7578125\n",
      "Epoch [509], val_loss: 166.8207\n",
      "gradient norm: 89.93153285980225, minimum ratio: 0.76953125\n",
      "Epoch [510], val_loss: 167.8523\n",
      "gradient norm: 90.28785610198975, minimum ratio: 0.80859375\n",
      "Epoch [511], val_loss: 168.8888\n",
      "gradient norm: 90.64451932907104, minimum ratio: 0.77734375\n",
      "Epoch [512], val_loss: 169.9304\n",
      "gradient norm: 91.00183391571045, minimum ratio: 0.7578125\n",
      "Epoch [513], val_loss: 170.9768\n",
      "gradient norm: 91.36082315444946, minimum ratio: 0.73828125\n",
      "Epoch [514], val_loss: 172.0282\n",
      "gradient norm: 91.71992826461792, minimum ratio: 0.75\n",
      "Epoch [515], val_loss: 173.0845\n",
      "gradient norm: 92.0806474685669, minimum ratio: 0.7578125\n",
      "Epoch [516], val_loss: 174.1458\n",
      "gradient norm: 92.44416093826294, minimum ratio: 0.78125\n",
      "Epoch [517], val_loss: 175.2122\n",
      "gradient norm: 92.80810737609863, minimum ratio: 0.78515625\n",
      "Epoch [518], val_loss: 176.2838\n",
      "gradient norm: 93.17037725448608, minimum ratio: 0.78125\n",
      "Epoch [519], val_loss: 177.3603\n",
      "gradient norm: 93.53567457199097, minimum ratio: 0.7421875\n",
      "Epoch [520], val_loss: 178.4418\n",
      "gradient norm: 93.9020562171936, minimum ratio: 0.7734375\n",
      "Epoch [521], val_loss: 179.5284\n",
      "gradient norm: 94.26903200149536, minimum ratio: 0.7734375\n",
      "Epoch [522], val_loss: 180.6202\n",
      "gradient norm: 94.63700723648071, minimum ratio: 0.76171875\n",
      "Epoch [523], val_loss: 181.7169\n",
      "gradient norm: 95.0063066482544, minimum ratio: 0.7890625\n",
      "Epoch [524], val_loss: 182.8188\n",
      "gradient norm: 95.37580108642578, minimum ratio: 0.7578125\n",
      "Epoch [525], val_loss: 183.9259\n",
      "gradient norm: 95.74838733673096, minimum ratio: 0.78515625\n",
      "Epoch [526], val_loss: 185.0382\n",
      "gradient norm: 96.12052011489868, minimum ratio: 0.73828125\n",
      "Epoch [527], val_loss: 186.1557\n",
      "gradient norm: 96.49256134033203, minimum ratio: 0.76953125\n",
      "Epoch [528], val_loss: 187.2783\n",
      "gradient norm: 96.86475992202759, minimum ratio: 0.765625\n",
      "Epoch [529], val_loss: 188.4061\n",
      "gradient norm: 97.23907995223999, minimum ratio: 0.76171875\n",
      "Epoch [530], val_loss: 189.5392\n",
      "gradient norm: 97.61397123336792, minimum ratio: 0.76953125\n",
      "Epoch [531], val_loss: 190.6774\n",
      "gradient norm: 97.99065113067627, minimum ratio: 0.76953125\n",
      "Epoch [532], val_loss: 191.8210\n",
      "gradient norm: 98.36965370178223, minimum ratio: 0.7734375\n",
      "Epoch [533], val_loss: 192.9698\n",
      "gradient norm: 98.74747514724731, minimum ratio: 0.78125\n",
      "Epoch [534], val_loss: 194.1239\n",
      "gradient norm: 99.12949275970459, minimum ratio: 0.77734375\n",
      "Epoch [535], val_loss: 195.2833\n",
      "gradient norm: 99.51133680343628, minimum ratio: 0.75390625\n",
      "Epoch [536], val_loss: 196.4481\n",
      "gradient norm: 99.89262199401855, minimum ratio: 0.7578125\n",
      "Epoch [537], val_loss: 197.6181\n",
      "gradient norm: 100.27545738220215, minimum ratio: 0.7734375\n",
      "Epoch [538], val_loss: 198.7935\n",
      "gradient norm: 100.65786981582642, minimum ratio: 0.7578125\n",
      "Epoch [539], val_loss: 199.9741\n",
      "gradient norm: 101.04189682006836, minimum ratio: 0.76171875\n",
      "Epoch [540], val_loss: 201.1602\n",
      "gradient norm: 101.42860555648804, minimum ratio: 0.76953125\n",
      "Epoch [541], val_loss: 202.3518\n",
      "gradient norm: 101.81463575363159, minimum ratio: 0.78515625\n",
      "Epoch [542], val_loss: 203.5487\n",
      "gradient norm: 102.20320320129395, minimum ratio: 0.7734375\n",
      "Epoch [543], val_loss: 204.7510\n",
      "gradient norm: 102.59220266342163, minimum ratio: 0.75\n",
      "Epoch [544], val_loss: 205.9589\n",
      "gradient norm: 102.98156547546387, minimum ratio: 0.78515625\n",
      "Epoch [545], val_loss: 207.1723\n",
      "gradient norm: 103.36953687667847, minimum ratio: 0.78125\n",
      "Epoch [546], val_loss: 208.3911\n",
      "gradient norm: 103.76135396957397, minimum ratio: 0.76953125\n",
      "Epoch [547], val_loss: 209.6154\n",
      "gradient norm: 104.15408277511597, minimum ratio: 0.74609375\n",
      "Epoch [548], val_loss: 210.8452\n",
      "gradient norm: 104.54661703109741, minimum ratio: 0.765625\n",
      "Epoch [549], val_loss: 212.0806\n",
      "gradient norm: 104.94231224060059, minimum ratio: 0.7578125\n",
      "Epoch [550], val_loss: 213.3214\n",
      "gradient norm: 105.33824920654297, minimum ratio: 0.76953125\n",
      "Epoch [551], val_loss: 214.5680\n",
      "gradient norm: 105.73655939102173, minimum ratio: 0.73828125\n",
      "Epoch [552], val_loss: 215.8201\n",
      "gradient norm: 106.13614463806152, minimum ratio: 0.78125\n",
      "Epoch [553], val_loss: 217.0778\n",
      "gradient norm: 106.53550434112549, minimum ratio: 0.78125\n",
      "Epoch [554], val_loss: 218.3410\n",
      "gradient norm: 106.93693256378174, minimum ratio: 0.75390625\n",
      "Epoch [555], val_loss: 219.6099\n",
      "gradient norm: 107.33475685119629, minimum ratio: 0.78125\n",
      "Epoch [556], val_loss: 220.8845\n",
      "gradient norm: 107.73583650588989, minimum ratio: 0.79296875\n",
      "Epoch [557], val_loss: 222.1645\n",
      "gradient norm: 108.14000415802002, minimum ratio: 0.7890625\n",
      "Epoch [558], val_loss: 223.4503\n",
      "gradient norm: 108.5424575805664, minimum ratio: 0.77734375\n",
      "Epoch [559], val_loss: 224.7419\n",
      "gradient norm: 108.94812726974487, minimum ratio: 0.78125\n",
      "Epoch [560], val_loss: 226.0392\n",
      "gradient norm: 109.3547739982605, minimum ratio: 0.75390625\n",
      "Epoch [561], val_loss: 227.3425\n",
      "gradient norm: 109.76104879379272, minimum ratio: 0.765625\n",
      "Epoch [562], val_loss: 228.6514\n",
      "gradient norm: 110.1679310798645, minimum ratio: 0.75\n",
      "Epoch [563], val_loss: 229.9660\n",
      "gradient norm: 110.57646989822388, minimum ratio: 0.73828125\n",
      "Epoch [564], val_loss: 231.2864\n",
      "gradient norm: 110.98698806762695, minimum ratio: 0.76171875\n",
      "Epoch [565], val_loss: 232.6127\n",
      "gradient norm: 111.39694213867188, minimum ratio: 0.77734375\n",
      "Epoch [566], val_loss: 233.9447\n",
      "gradient norm: 111.8107967376709, minimum ratio: 0.77734375\n",
      "Epoch [567], val_loss: 235.2826\n",
      "gradient norm: 112.22482109069824, minimum ratio: 0.78125\n",
      "Epoch [568], val_loss: 236.6261\n",
      "gradient norm: 112.64018487930298, minimum ratio: 0.76953125\n",
      "Epoch [569], val_loss: 237.9756\n",
      "gradient norm: 113.0564169883728, minimum ratio: 0.75390625\n",
      "Epoch [570], val_loss: 239.3309\n",
      "gradient norm: 113.47257232666016, minimum ratio: 0.7890625\n",
      "Epoch [571], val_loss: 240.6922\n",
      "gradient norm: 113.88812494277954, minimum ratio: 0.7578125\n",
      "Epoch [572], val_loss: 242.0595\n",
      "gradient norm: 114.30598211288452, minimum ratio: 0.765625\n",
      "Epoch [573], val_loss: 243.4327\n",
      "gradient norm: 114.72293901443481, minimum ratio: 0.73828125\n",
      "Epoch [574], val_loss: 244.8118\n",
      "gradient norm: 115.14090967178345, minimum ratio: 0.76953125\n",
      "Epoch [575], val_loss: 246.1969\n",
      "gradient norm: 115.56383609771729, minimum ratio: 0.76953125\n",
      "Epoch [576], val_loss: 247.5881\n",
      "gradient norm: 115.98702383041382, minimum ratio: 0.76171875\n",
      "Epoch [577], val_loss: 248.9851\n",
      "gradient norm: 116.41314220428467, minimum ratio: 0.77734375\n",
      "Epoch [578], val_loss: 250.3882\n",
      "gradient norm: 116.83423280715942, minimum ratio: 0.78125\n",
      "Epoch [579], val_loss: 251.7973\n",
      "gradient norm: 117.261155128479, minimum ratio: 0.7734375\n",
      "Epoch [580], val_loss: 253.2125\n",
      "gradient norm: 117.6881217956543, minimum ratio: 0.76953125\n",
      "Epoch [581], val_loss: 254.6339\n",
      "gradient norm: 118.11658573150635, minimum ratio: 0.7890625\n",
      "Epoch [582], val_loss: 256.0613\n",
      "gradient norm: 118.5457935333252, minimum ratio: 0.75390625\n",
      "Epoch [583], val_loss: 257.4948\n",
      "gradient norm: 118.97348690032959, minimum ratio: 0.76171875\n",
      "Epoch [584], val_loss: 258.9344\n",
      "gradient norm: 119.40637588500977, minimum ratio: 0.76171875\n",
      "Epoch [585], val_loss: 260.3800\n",
      "gradient norm: 119.8376350402832, minimum ratio: 0.765625\n",
      "Epoch [586], val_loss: 261.8317\n",
      "gradient norm: 120.27286052703857, minimum ratio: 0.765625\n",
      "Epoch [587], val_loss: 263.2897\n",
      "gradient norm: 120.70567607879639, minimum ratio: 0.765625\n",
      "Epoch [588], val_loss: 264.7539\n",
      "gradient norm: 121.1406192779541, minimum ratio: 0.7421875\n",
      "Epoch [589], val_loss: 266.2244\n",
      "gradient norm: 121.57439041137695, minimum ratio: 0.75\n",
      "Epoch [590], val_loss: 267.7012\n",
      "gradient norm: 122.01083374023438, minimum ratio: 0.75\n",
      "Epoch [591], val_loss: 269.1843\n",
      "gradient norm: 122.44894504547119, minimum ratio: 0.75390625\n",
      "Epoch [592], val_loss: 270.6737\n",
      "gradient norm: 122.88745212554932, minimum ratio: 0.74609375\n",
      "Epoch [593], val_loss: 272.1693\n",
      "gradient norm: 123.32765865325928, minimum ratio: 0.7890625\n",
      "Epoch [594], val_loss: 273.6713\n",
      "gradient norm: 123.7672233581543, minimum ratio: 0.76953125\n",
      "Epoch [595], val_loss: 275.1796\n",
      "gradient norm: 124.21127986907959, minimum ratio: 0.77734375\n",
      "Epoch [596], val_loss: 276.6943\n",
      "gradient norm: 124.65599060058594, minimum ratio: 0.7578125\n",
      "Epoch [597], val_loss: 278.2155\n",
      "gradient norm: 125.10205078125, minimum ratio: 0.765625\n",
      "Epoch [598], val_loss: 279.7432\n",
      "gradient norm: 125.54874801635742, minimum ratio: 0.75390625\n",
      "Epoch [599], val_loss: 281.2772\n",
      "gradient norm: 125.99633502960205, minimum ratio: 0.74609375\n",
      "Epoch [600], val_loss: 282.8176\n",
      "gradient norm: 126.4428596496582, minimum ratio: 0.765625\n",
      "Epoch [601], val_loss: 284.3644\n",
      "gradient norm: 126.8921709060669, minimum ratio: 0.765625\n",
      "Epoch [602], val_loss: 285.9176\n",
      "gradient norm: 127.34437656402588, minimum ratio: 0.7578125\n",
      "Epoch [603], val_loss: 287.4772\n",
      "gradient norm: 127.79144954681396, minimum ratio: 0.77734375\n",
      "Epoch [604], val_loss: 289.0433\n",
      "gradient norm: 128.2470359802246, minimum ratio: 0.75\n",
      "Epoch [605], val_loss: 290.6160\n",
      "gradient norm: 128.69840621948242, minimum ratio: 0.74609375\n",
      "Epoch [606], val_loss: 292.1952\n",
      "gradient norm: 129.14965152740479, minimum ratio: 0.74609375\n",
      "Epoch [607], val_loss: 293.7811\n",
      "gradient norm: 129.6073513031006, minimum ratio: 0.7265625\n",
      "Epoch [608], val_loss: 295.3735\n",
      "gradient norm: 130.0639362335205, minimum ratio: 0.765625\n",
      "Epoch [609], val_loss: 296.9726\n",
      "gradient norm: 130.52017402648926, minimum ratio: 0.75390625\n",
      "Epoch [610], val_loss: 298.5783\n",
      "gradient norm: 130.97829341888428, minimum ratio: 0.7578125\n",
      "Epoch [611], val_loss: 300.1906\n",
      "gradient norm: 131.43901920318604, minimum ratio: 0.79296875\n",
      "Epoch [612], val_loss: 301.8094\n",
      "gradient norm: 131.90092182159424, minimum ratio: 0.765625\n",
      "Epoch [613], val_loss: 303.4349\n",
      "gradient norm: 132.3630132675171, minimum ratio: 0.78515625\n",
      "Epoch [614], val_loss: 305.0669\n",
      "gradient norm: 132.82750988006592, minimum ratio: 0.75\n",
      "Epoch [615], val_loss: 306.7057\n",
      "gradient norm: 133.29193592071533, minimum ratio: 0.78125\n",
      "Epoch [616], val_loss: 308.3513\n",
      "gradient norm: 133.75672054290771, minimum ratio: 0.76953125\n",
      "Epoch [617], val_loss: 310.0035\n",
      "gradient norm: 134.22266101837158, minimum ratio: 0.75\n",
      "Epoch [618], val_loss: 311.6624\n",
      "gradient norm: 134.69027137756348, minimum ratio: 0.76953125\n",
      "Epoch [619], val_loss: 313.3280\n",
      "gradient norm: 135.15773963928223, minimum ratio: 0.78125\n",
      "Epoch [620], val_loss: 315.0003\n",
      "gradient norm: 135.62279891967773, minimum ratio: 0.7734375\n",
      "Epoch [621], val_loss: 316.6793\n",
      "gradient norm: 136.0950164794922, minimum ratio: 0.7734375\n",
      "Epoch [622], val_loss: 318.3651\n",
      "gradient norm: 136.56403255462646, minimum ratio: 0.765625\n",
      "Epoch [623], val_loss: 320.0577\n",
      "gradient norm: 137.03740406036377, minimum ratio: 0.77734375\n",
      "Epoch [624], val_loss: 321.7573\n",
      "gradient norm: 137.51325225830078, minimum ratio: 0.7890625\n",
      "Epoch [625], val_loss: 323.4637\n",
      "gradient norm: 137.98773765563965, minimum ratio: 0.7578125\n",
      "Epoch [626], val_loss: 325.1773\n",
      "gradient norm: 138.4656639099121, minimum ratio: 0.765625\n",
      "Epoch [627], val_loss: 326.8976\n",
      "gradient norm: 138.93897533416748, minimum ratio: 0.75390625\n",
      "Epoch [628], val_loss: 328.6248\n",
      "gradient norm: 139.4182939529419, minimum ratio: 0.765625\n",
      "Epoch [629], val_loss: 330.3589\n",
      "gradient norm: 139.89911937713623, minimum ratio: 0.75390625\n",
      "Epoch [630], val_loss: 332.0999\n",
      "gradient norm: 140.38169193267822, minimum ratio: 0.7578125\n",
      "Epoch [631], val_loss: 333.8478\n",
      "gradient norm: 140.86339473724365, minimum ratio: 0.75\n",
      "Epoch [632], val_loss: 335.6027\n",
      "gradient norm: 141.34836769104004, minimum ratio: 0.7734375\n",
      "Epoch [633], val_loss: 337.3646\n",
      "gradient norm: 141.8299331665039, minimum ratio: 0.77734375\n",
      "Epoch [634], val_loss: 339.1336\n",
      "gradient norm: 142.31490230560303, minimum ratio: 0.765625\n",
      "Epoch [635], val_loss: 340.9095\n",
      "gradient norm: 142.80216121673584, minimum ratio: 0.76953125\n",
      "Epoch [636], val_loss: 342.6923\n",
      "gradient norm: 143.288800239563, minimum ratio: 0.7734375\n",
      "Epoch [637], val_loss: 344.4823\n",
      "gradient norm: 143.77581024169922, minimum ratio: 0.7421875\n",
      "Epoch [638], val_loss: 346.2793\n",
      "gradient norm: 144.26271152496338, minimum ratio: 0.75\n",
      "Epoch [639], val_loss: 348.0834\n",
      "gradient norm: 144.75138759613037, minimum ratio: 0.7734375\n",
      "Epoch [640], val_loss: 349.8947\n",
      "gradient norm: 145.2433271408081, minimum ratio: 0.75390625\n",
      "Epoch [641], val_loss: 351.7132\n",
      "gradient norm: 145.73330402374268, minimum ratio: 0.76953125\n",
      "Epoch [642], val_loss: 353.5388\n",
      "gradient norm: 146.22801876068115, minimum ratio: 0.78515625\n",
      "Epoch [643], val_loss: 355.3716\n",
      "gradient norm: 146.7234926223755, minimum ratio: 0.76953125\n",
      "Epoch [644], val_loss: 357.2117\n",
      "gradient norm: 147.22017002105713, minimum ratio: 0.76171875\n",
      "Epoch [645], val_loss: 359.0591\n",
      "gradient norm: 147.71448612213135, minimum ratio: 0.75390625\n",
      "Epoch [646], val_loss: 360.9136\n",
      "gradient norm: 148.2153377532959, minimum ratio: 0.73828125\n",
      "Epoch [647], val_loss: 362.7754\n",
      "gradient norm: 148.7154188156128, minimum ratio: 0.7578125\n",
      "Epoch [648], val_loss: 364.6446\n",
      "gradient norm: 149.2148952484131, minimum ratio: 0.79296875\n",
      "Epoch [649], val_loss: 366.5211\n",
      "gradient norm: 149.71536922454834, minimum ratio: 0.7578125\n",
      "Epoch [650], val_loss: 368.4049\n",
      "gradient norm: 150.21607875823975, minimum ratio: 0.75\n",
      "Epoch [651], val_loss: 370.2961\n",
      "gradient norm: 150.72389602661133, minimum ratio: 0.76953125\n",
      "Epoch [652], val_loss: 372.1945\n",
      "gradient norm: 151.23032760620117, minimum ratio: 0.76953125\n",
      "Epoch [653], val_loss: 374.1003\n",
      "gradient norm: 151.7341718673706, minimum ratio: 0.76953125\n",
      "Epoch [654], val_loss: 376.0136\n",
      "gradient norm: 152.23940563201904, minimum ratio: 0.77734375\n",
      "Epoch [655], val_loss: 377.9343\n",
      "gradient norm: 152.74777507781982, minimum ratio: 0.76171875\n",
      "Epoch [656], val_loss: 379.8624\n",
      "gradient norm: 153.25599098205566, minimum ratio: 0.765625\n",
      "Epoch [657], val_loss: 381.7980\n",
      "gradient norm: 153.76798057556152, minimum ratio: 0.76953125\n",
      "Epoch [658], val_loss: 383.7410\n",
      "gradient norm: 154.27786922454834, minimum ratio: 0.77734375\n",
      "Epoch [659], val_loss: 385.6916\n",
      "gradient norm: 154.78978729248047, minimum ratio: 0.7578125\n",
      "Epoch [660], val_loss: 387.6496\n",
      "gradient norm: 155.3047742843628, minimum ratio: 0.7734375\n",
      "Epoch [661], val_loss: 389.6151\n",
      "gradient norm: 155.82012367248535, minimum ratio: 0.7578125\n",
      "Epoch [662], val_loss: 391.5882\n",
      "gradient norm: 156.33678531646729, minimum ratio: 0.765625\n",
      "Epoch [663], val_loss: 393.5690\n",
      "gradient norm: 156.8520965576172, minimum ratio: 0.7734375\n",
      "Epoch [664], val_loss: 395.5573\n",
      "gradient norm: 157.37248134613037, minimum ratio: 0.75390625\n",
      "Epoch [665], val_loss: 397.5532\n",
      "gradient norm: 157.8920135498047, minimum ratio: 0.76953125\n",
      "Epoch [666], val_loss: 399.5566\n",
      "gradient norm: 158.4149570465088, minimum ratio: 0.7734375\n",
      "Epoch [667], val_loss: 401.5677\n",
      "gradient norm: 158.93624687194824, minimum ratio: 0.76171875\n",
      "Epoch [668], val_loss: 403.5862\n",
      "gradient norm: 159.45855617523193, minimum ratio: 0.78125\n",
      "Epoch [669], val_loss: 405.6126\n",
      "gradient norm: 159.98294162750244, minimum ratio: 0.76171875\n",
      "Epoch [670], val_loss: 407.6465\n",
      "gradient norm: 160.51024532318115, minimum ratio: 0.7578125\n",
      "Epoch [671], val_loss: 409.6881\n",
      "gradient norm: 161.03672122955322, minimum ratio: 0.7734375\n",
      "Epoch [672], val_loss: 411.7374\n",
      "gradient norm: 161.56389617919922, minimum ratio: 0.77734375\n",
      "Epoch [673], val_loss: 413.7943\n",
      "gradient norm: 162.08970832824707, minimum ratio: 0.76953125\n",
      "Epoch [674], val_loss: 415.8592\n",
      "gradient norm: 162.61627292633057, minimum ratio: 0.73828125\n",
      "Epoch [675], val_loss: 417.9320\n",
      "gradient norm: 163.14367485046387, minimum ratio: 0.75390625\n",
      "Epoch [676], val_loss: 420.0128\n",
      "gradient norm: 163.67613697052002, minimum ratio: 0.77734375\n",
      "Epoch [677], val_loss: 422.1015\n",
      "gradient norm: 164.2066297531128, minimum ratio: 0.7734375\n",
      "Epoch [678], val_loss: 424.1979\n",
      "gradient norm: 164.74270248413086, minimum ratio: 0.78125\n",
      "Epoch [679], val_loss: 426.3022\n",
      "gradient norm: 165.2777271270752, minimum ratio: 0.76171875\n",
      "Epoch [680], val_loss: 428.4143\n",
      "gradient norm: 165.81413459777832, minimum ratio: 0.76953125\n",
      "Epoch [681], val_loss: 430.5344\n",
      "gradient norm: 166.35481071472168, minimum ratio: 0.76171875\n",
      "Epoch [682], val_loss: 432.6624\n",
      "gradient norm: 166.89679718017578, minimum ratio: 0.7578125\n",
      "Epoch [683], val_loss: 434.7984\n",
      "gradient norm: 167.43870639801025, minimum ratio: 0.76953125\n",
      "Epoch [684], val_loss: 436.9423\n",
      "gradient norm: 167.97882461547852, minimum ratio: 0.76953125\n",
      "Epoch [685], val_loss: 439.0942\n",
      "gradient norm: 168.523118019104, minimum ratio: 0.73046875\n",
      "Epoch [686], val_loss: 441.2541\n",
      "gradient norm: 169.06675052642822, minimum ratio: 0.7421875\n",
      "Epoch [687], val_loss: 443.4222\n",
      "gradient norm: 169.61091709136963, minimum ratio: 0.765625\n",
      "Epoch [688], val_loss: 445.5982\n",
      "gradient norm: 170.15497398376465, minimum ratio: 0.7578125\n",
      "Epoch [689], val_loss: 447.7821\n",
      "gradient norm: 170.7005500793457, minimum ratio: 0.76953125\n",
      "Epoch [690], val_loss: 449.9743\n",
      "gradient norm: 171.2514886856079, minimum ratio: 0.76171875\n",
      "Epoch [691], val_loss: 452.1746\n",
      "gradient norm: 171.80509757995605, minimum ratio: 0.75390625\n",
      "Epoch [692], val_loss: 454.3829\n",
      "gradient norm: 172.3588056564331, minimum ratio: 0.765625\n",
      "Epoch [693], val_loss: 456.5995\n",
      "gradient norm: 172.90519332885742, minimum ratio: 0.7578125\n",
      "Epoch [694], val_loss: 458.8241\n",
      "gradient norm: 173.4596643447876, minimum ratio: 0.76171875\n",
      "Epoch [695], val_loss: 461.0569\n",
      "gradient norm: 174.0092544555664, minimum ratio: 0.76171875\n",
      "Epoch [696], val_loss: 463.2977\n",
      "gradient norm: 174.5645170211792, minimum ratio: 0.76953125\n",
      "Epoch [697], val_loss: 465.5466\n",
      "gradient norm: 175.1196060180664, minimum ratio: 0.7890625\n",
      "Epoch [698], val_loss: 467.8038\n",
      "gradient norm: 175.67639827728271, minimum ratio: 0.78515625\n",
      "Epoch [699], val_loss: 470.0693\n",
      "gradient norm: 176.23711681365967, minimum ratio: 0.7734375\n",
      "Epoch [700], val_loss: 472.3428\n",
      "gradient norm: 176.7948179244995, minimum ratio: 0.765625\n",
      "Epoch [701], val_loss: 474.6247\n",
      "gradient norm: 177.36262226104736, minimum ratio: 0.74609375\n",
      "Epoch [702], val_loss: 476.9150\n",
      "gradient norm: 177.92328643798828, minimum ratio: 0.76953125\n",
      "Epoch [703], val_loss: 479.2137\n",
      "gradient norm: 178.4882640838623, minimum ratio: 0.7578125\n",
      "Epoch [704], val_loss: 481.5206\n",
      "gradient norm: 179.05261135101318, minimum ratio: 0.75390625\n",
      "Epoch [705], val_loss: 483.8360\n",
      "gradient norm: 179.61554718017578, minimum ratio: 0.78125\n",
      "Epoch [706], val_loss: 486.1597\n",
      "gradient norm: 180.18046855926514, minimum ratio: 0.7890625\n",
      "Epoch [707], val_loss: 488.4917\n",
      "gradient norm: 180.74910354614258, minimum ratio: 0.77734375\n",
      "Epoch [708], val_loss: 490.8323\n",
      "gradient norm: 181.31770038604736, minimum ratio: 0.75390625\n",
      "Epoch [709], val_loss: 493.1813\n",
      "gradient norm: 181.8878583908081, minimum ratio: 0.75\n",
      "Epoch [710], val_loss: 495.5387\n",
      "gradient norm: 182.45764255523682, minimum ratio: 0.7421875\n",
      "Epoch [711], val_loss: 497.9046\n",
      "gradient norm: 183.03173351287842, minimum ratio: 0.796875\n",
      "Epoch [712], val_loss: 500.2792\n",
      "gradient norm: 183.60821342468262, minimum ratio: 0.765625\n",
      "Epoch [713], val_loss: 502.6624\n",
      "gradient norm: 184.18094730377197, minimum ratio: 0.80078125\n",
      "Epoch [714], val_loss: 505.0542\n",
      "gradient norm: 184.75659084320068, minimum ratio: 0.77734375\n",
      "Epoch [715], val_loss: 507.4544\n",
      "gradient norm: 185.33466720581055, minimum ratio: 0.7734375\n",
      "Epoch [716], val_loss: 509.8630\n",
      "gradient norm: 185.91426467895508, minimum ratio: 0.7578125\n",
      "Epoch [717], val_loss: 512.2801\n",
      "gradient norm: 186.49246788024902, minimum ratio: 0.78515625\n",
      "Epoch [718], val_loss: 514.7060\n",
      "gradient norm: 187.07364082336426, minimum ratio: 0.78125\n",
      "Epoch [719], val_loss: 517.1406\n",
      "gradient norm: 187.65952110290527, minimum ratio: 0.78125\n",
      "Epoch [720], val_loss: 519.5837\n",
      "gradient norm: 188.24258518218994, minimum ratio: 0.765625\n",
      "Epoch [721], val_loss: 522.0355\n",
      "gradient norm: 188.8258762359619, minimum ratio: 0.74609375\n",
      "Epoch [722], val_loss: 524.4961\n",
      "gradient norm: 189.41672801971436, minimum ratio: 0.75390625\n",
      "Epoch [723], val_loss: 526.9653\n",
      "gradient norm: 190.0027256011963, minimum ratio: 0.76953125\n",
      "Epoch [724], val_loss: 529.4432\n",
      "gradient norm: 190.59314823150635, minimum ratio: 0.765625\n",
      "Epoch [725], val_loss: 531.9299\n",
      "gradient norm: 191.1825647354126, minimum ratio: 0.796875\n",
      "Epoch [726], val_loss: 534.4254\n",
      "gradient norm: 191.76771926879883, minimum ratio: 0.75390625\n",
      "Epoch [727], val_loss: 536.9296\n",
      "gradient norm: 192.35692596435547, minimum ratio: 0.78515625\n",
      "Epoch [728], val_loss: 539.4426\n",
      "gradient norm: 192.94898223876953, minimum ratio: 0.76171875\n",
      "Epoch [729], val_loss: 541.9646\n",
      "gradient norm: 193.54571437835693, minimum ratio: 0.7734375\n",
      "Epoch [730], val_loss: 544.4954\n",
      "gradient norm: 194.14403820037842, minimum ratio: 0.78515625\n",
      "Epoch [731], val_loss: 547.0349\n",
      "gradient norm: 194.7391119003296, minimum ratio: 0.7734375\n",
      "Epoch [732], val_loss: 549.5833\n",
      "gradient norm: 195.33568954467773, minimum ratio: 0.79296875\n",
      "Epoch [733], val_loss: 552.1407\n",
      "gradient norm: 195.9338665008545, minimum ratio: 0.76171875\n",
      "Epoch [734], val_loss: 554.7071\n",
      "gradient norm: 196.53758430480957, minimum ratio: 0.75\n",
      "Epoch [735], val_loss: 557.2823\n",
      "gradient norm: 197.13841915130615, minimum ratio: 0.79296875\n",
      "Epoch [736], val_loss: 559.8666\n",
      "gradient norm: 197.74069213867188, minimum ratio: 0.76171875\n",
      "Epoch [737], val_loss: 562.4597\n",
      "gradient norm: 198.34460163116455, minimum ratio: 0.7578125\n",
      "Epoch [738], val_loss: 565.0618\n",
      "gradient norm: 198.94950103759766, minimum ratio: 0.75\n",
      "Epoch [739], val_loss: 567.6728\n",
      "gradient norm: 199.55541229248047, minimum ratio: 0.77734375\n",
      "Epoch [740], val_loss: 570.2928\n",
      "gradient norm: 200.16390895843506, minimum ratio: 0.7734375\n",
      "Epoch [741], val_loss: 572.9220\n",
      "gradient norm: 200.7744436264038, minimum ratio: 0.77734375\n",
      "Epoch [742], val_loss: 575.5599\n",
      "gradient norm: 201.38260746002197, minimum ratio: 0.78125\n",
      "Epoch [743], val_loss: 578.2073\n",
      "gradient norm: 201.99292087554932, minimum ratio: 0.73046875\n",
      "Epoch [744], val_loss: 580.8640\n",
      "gradient norm: 202.60340881347656, minimum ratio: 0.75\n",
      "Epoch [745], val_loss: 583.5298\n",
      "gradient norm: 203.21248531341553, minimum ratio: 0.7734375\n",
      "Epoch [746], val_loss: 586.2050\n",
      "gradient norm: 203.82677936553955, minimum ratio: 0.765625\n",
      "Epoch [747], val_loss: 588.8893\n",
      "gradient norm: 204.441876411438, minimum ratio: 0.78125\n",
      "Epoch [748], val_loss: 591.5829\n",
      "gradient norm: 205.05875778198242, minimum ratio: 0.7578125\n",
      "Epoch [749], val_loss: 594.2854\n",
      "gradient norm: 205.68031311035156, minimum ratio: 0.76953125\n",
      "Epoch [750], val_loss: 596.9974\n",
      "gradient norm: 206.30031967163086, minimum ratio: 0.76953125\n",
      "Epoch [751], val_loss: 599.7185\n",
      "gradient norm: 206.92185878753662, minimum ratio: 0.76953125\n",
      "Epoch [752], val_loss: 602.4488\n",
      "gradient norm: 207.54601287841797, minimum ratio: 0.75390625\n",
      "Epoch [753], val_loss: 605.1884\n",
      "gradient norm: 208.16719818115234, minimum ratio: 0.78125\n",
      "Epoch [754], val_loss: 607.9373\n",
      "gradient norm: 208.79170417785645, minimum ratio: 0.7578125\n",
      "Epoch [755], val_loss: 610.6958\n",
      "gradient norm: 209.42301750183105, minimum ratio: 0.7734375\n",
      "Epoch [756], val_loss: 613.4634\n",
      "gradient norm: 210.05326652526855, minimum ratio: 0.765625\n",
      "Epoch [757], val_loss: 616.2406\n",
      "gradient norm: 210.67583465576172, minimum ratio: 0.77734375\n",
      "Epoch [758], val_loss: 619.0272\n",
      "gradient norm: 211.30084991455078, minimum ratio: 0.77734375\n",
      "Epoch [759], val_loss: 621.8230\n",
      "gradient norm: 211.93047332763672, minimum ratio: 0.76953125\n",
      "Epoch [760], val_loss: 624.6284\n",
      "gradient norm: 212.5605297088623, minimum ratio: 0.765625\n",
      "Epoch [761], val_loss: 627.4434\n",
      "gradient norm: 213.1936206817627, minimum ratio: 0.7734375\n",
      "Epoch [762], val_loss: 630.2677\n",
      "gradient norm: 213.83019256591797, minimum ratio: 0.76171875\n",
      "Epoch [763], val_loss: 633.1014\n",
      "gradient norm: 214.46384811401367, minimum ratio: 0.76953125\n",
      "Epoch [764], val_loss: 635.9446\n",
      "gradient norm: 215.10063362121582, minimum ratio: 0.7734375\n",
      "Epoch [765], val_loss: 638.7974\n",
      "gradient norm: 215.7372341156006, minimum ratio: 0.76171875\n",
      "Epoch [766], val_loss: 641.6597\n",
      "gradient norm: 216.38260078430176, minimum ratio: 0.765625\n",
      "Epoch [767], val_loss: 644.5318\n",
      "gradient norm: 217.02366065979004, minimum ratio: 0.74609375\n",
      "Epoch [768], val_loss: 647.4135\n",
      "gradient norm: 217.66728591918945, minimum ratio: 0.7734375\n",
      "Epoch [769], val_loss: 650.3047\n",
      "gradient norm: 218.30828094482422, minimum ratio: 0.765625\n",
      "Epoch [770], val_loss: 653.2056\n",
      "gradient norm: 218.95043182373047, minimum ratio: 0.74609375\n",
      "Epoch [771], val_loss: 656.1160\n",
      "gradient norm: 219.58733558654785, minimum ratio: 0.76171875\n",
      "Epoch [772], val_loss: 659.0363\n",
      "gradient norm: 220.23371696472168, minimum ratio: 0.7890625\n",
      "Epoch [773], val_loss: 661.9661\n",
      "gradient norm: 220.88415908813477, minimum ratio: 0.7421875\n",
      "Epoch [774], val_loss: 664.9055\n",
      "gradient norm: 221.53673362731934, minimum ratio: 0.75\n",
      "Epoch [775], val_loss: 667.8547\n",
      "gradient norm: 222.19131660461426, minimum ratio: 0.7734375\n",
      "Epoch [776], val_loss: 670.8138\n",
      "gradient norm: 222.8430061340332, minimum ratio: 0.75\n",
      "Epoch [777], val_loss: 673.7826\n",
      "gradient norm: 223.49984550476074, minimum ratio: 0.76953125\n",
      "Epoch [778], val_loss: 676.7614\n",
      "gradient norm: 224.14782524108887, minimum ratio: 0.75\n",
      "Epoch [779], val_loss: 679.7502\n",
      "gradient norm: 224.80060958862305, minimum ratio: 0.7734375\n",
      "Epoch [780], val_loss: 682.7488\n",
      "gradient norm: 225.45823669433594, minimum ratio: 0.7734375\n",
      "Epoch [781], val_loss: 685.7571\n",
      "gradient norm: 226.11627578735352, minimum ratio: 0.76171875\n",
      "Epoch [782], val_loss: 688.7755\n",
      "gradient norm: 226.77593612670898, minimum ratio: 0.765625\n",
      "Epoch [783], val_loss: 691.8041\n",
      "gradient norm: 227.4276180267334, minimum ratio: 0.76953125\n",
      "Epoch [784], val_loss: 694.8424\n",
      "gradient norm: 228.08185386657715, minimum ratio: 0.75390625\n",
      "Epoch [785], val_loss: 697.8907\n",
      "gradient norm: 228.74496841430664, minimum ratio: 0.77734375\n",
      "Epoch [786], val_loss: 700.9489\n",
      "gradient norm: 229.41098403930664, minimum ratio: 0.765625\n",
      "Epoch [787], val_loss: 704.0172\n",
      "gradient norm: 230.07827377319336, minimum ratio: 0.77734375\n",
      "Epoch [788], val_loss: 707.0952\n",
      "gradient norm: 230.7496337890625, minimum ratio: 0.78125\n",
      "Epoch [789], val_loss: 710.1835\n",
      "gradient norm: 231.42040061950684, minimum ratio: 0.7578125\n",
      "Epoch [790], val_loss: 713.2819\n",
      "gradient norm: 232.09036445617676, minimum ratio: 0.75390625\n",
      "Epoch [791], val_loss: 716.3904\n",
      "gradient norm: 232.7564239501953, minimum ratio: 0.765625\n",
      "Epoch [792], val_loss: 719.5090\n",
      "gradient norm: 233.42710494995117, minimum ratio: 0.80078125\n",
      "Epoch [793], val_loss: 722.6376\n",
      "gradient norm: 234.1016616821289, minimum ratio: 0.765625\n",
      "Epoch [794], val_loss: 725.7764\n",
      "gradient norm: 234.77619171142578, minimum ratio: 0.77734375\n",
      "Epoch [795], val_loss: 728.9254\n",
      "gradient norm: 235.4499797821045, minimum ratio: 0.7578125\n",
      "Epoch [796], val_loss: 732.0845\n",
      "gradient norm: 236.12847709655762, minimum ratio: 0.75390625\n",
      "Epoch [797], val_loss: 735.2539\n",
      "gradient norm: 236.80262565612793, minimum ratio: 0.76171875\n",
      "Epoch [798], val_loss: 738.4335\n",
      "gradient norm: 237.47842979431152, minimum ratio: 0.79296875\n",
      "Epoch [799], val_loss: 741.6234\n",
      "gradient norm: 238.16037559509277, minimum ratio: 0.75390625\n",
      "Epoch [800], val_loss: 744.8234\n",
      "gradient norm: 238.84755516052246, minimum ratio: 0.7734375\n",
      "Epoch [801], val_loss: 748.0338\n",
      "gradient norm: 239.53068733215332, minimum ratio: 0.77734375\n",
      "Epoch [802], val_loss: 751.2545\n",
      "gradient norm: 240.2106647491455, minimum ratio: 0.73828125\n",
      "Epoch [803], val_loss: 754.4854\n",
      "gradient norm: 240.89117813110352, minimum ratio: 0.77734375\n",
      "Epoch [804], val_loss: 757.7269\n",
      "gradient norm: 241.57861709594727, minimum ratio: 0.76171875\n",
      "Epoch [805], val_loss: 760.9791\n",
      "gradient norm: 242.26810264587402, minimum ratio: 0.7734375\n",
      "Epoch [806], val_loss: 764.2412\n",
      "gradient norm: 242.9604721069336, minimum ratio: 0.7734375\n",
      "Epoch [807], val_loss: 767.5137\n",
      "gradient norm: 243.6511993408203, minimum ratio: 0.765625\n",
      "Epoch [808], val_loss: 770.7963\n",
      "gradient norm: 244.34062957763672, minimum ratio: 0.76953125\n",
      "Epoch [809], val_loss: 774.0894\n",
      "gradient norm: 245.0332374572754, minimum ratio: 0.7734375\n",
      "Epoch [810], val_loss: 777.3928\n",
      "gradient norm: 245.72973442077637, minimum ratio: 0.78125\n",
      "Epoch [811], val_loss: 780.7071\n",
      "gradient norm: 246.41829872131348, minimum ratio: 0.7578125\n",
      "Epoch [812], val_loss: 784.0320\n",
      "gradient norm: 247.1113796234131, minimum ratio: 0.76953125\n",
      "Epoch [813], val_loss: 787.3674\n",
      "gradient norm: 247.8145923614502, minimum ratio: 0.76171875\n",
      "Epoch [814], val_loss: 790.7133\n",
      "gradient norm: 248.51133918762207, minimum ratio: 0.7734375\n",
      "Epoch [815], val_loss: 794.0698\n",
      "gradient norm: 249.21366119384766, minimum ratio: 0.76953125\n",
      "Epoch [816], val_loss: 797.4370\n",
      "gradient norm: 249.9153823852539, minimum ratio: 0.7734375\n",
      "Epoch [817], val_loss: 800.8146\n",
      "gradient norm: 250.61899948120117, minimum ratio: 0.7578125\n",
      "Epoch [818], val_loss: 804.2029\n",
      "gradient norm: 251.32446479797363, minimum ratio: 0.7578125\n",
      "Epoch [819], val_loss: 807.6017\n",
      "gradient norm: 252.03057289123535, minimum ratio: 0.76953125\n",
      "Epoch [820], val_loss: 811.0113\n",
      "gradient norm: 252.74083709716797, minimum ratio: 0.78515625\n",
      "Epoch [821], val_loss: 814.4316\n",
      "gradient norm: 253.44930839538574, minimum ratio: 0.76953125\n",
      "Epoch [822], val_loss: 817.8625\n",
      "gradient norm: 254.15867233276367, minimum ratio: 0.765625\n",
      "Epoch [823], val_loss: 821.3042\n",
      "gradient norm: 254.87202835083008, minimum ratio: 0.7890625\n",
      "Epoch [824], val_loss: 824.7567\n",
      "gradient norm: 255.57101249694824, minimum ratio: 0.77734375\n",
      "Epoch [825], val_loss: 828.2205\n",
      "gradient norm: 256.28700828552246, minimum ratio: 0.765625\n",
      "Epoch [826], val_loss: 831.6951\n",
      "gradient norm: 256.9941215515137, minimum ratio: 0.78515625\n",
      "Epoch [827], val_loss: 835.1806\n",
      "gradient norm: 257.70678520202637, minimum ratio: 0.77734375\n",
      "Epoch [828], val_loss: 838.6771\n",
      "gradient norm: 258.42501068115234, minimum ratio: 0.79296875\n",
      "Epoch [829], val_loss: 842.1846\n",
      "gradient norm: 259.1454391479492, minimum ratio: 0.7734375\n",
      "Epoch [830], val_loss: 845.7026\n",
      "gradient norm: 259.866024017334, minimum ratio: 0.7578125\n",
      "Epoch [831], val_loss: 849.2313\n",
      "gradient norm: 260.5857620239258, minimum ratio: 0.7734375\n",
      "Epoch [832], val_loss: 852.7711\n",
      "gradient norm: 261.3104438781738, minimum ratio: 0.74609375\n",
      "Epoch [833], val_loss: 856.3218\n",
      "gradient norm: 262.0383548736572, minimum ratio: 0.7578125\n",
      "Epoch [834], val_loss: 859.8834\n",
      "gradient norm: 262.7640132904053, minimum ratio: 0.765625\n",
      "Epoch [835], val_loss: 863.4561\n",
      "gradient norm: 263.4966812133789, minimum ratio: 0.76171875\n",
      "Epoch [836], val_loss: 867.0400\n",
      "gradient norm: 264.22546195983887, minimum ratio: 0.77734375\n",
      "Epoch [837], val_loss: 870.6351\n",
      "gradient norm: 264.9539051055908, minimum ratio: 0.78125\n",
      "Epoch [838], val_loss: 874.2413\n",
      "gradient norm: 265.67918586730957, minimum ratio: 0.78125\n",
      "Epoch [839], val_loss: 877.8586\n",
      "gradient norm: 266.4120407104492, minimum ratio: 0.78515625\n",
      "Epoch [840], val_loss: 881.4872\n",
      "gradient norm: 267.14108657836914, minimum ratio: 0.77734375\n",
      "Epoch [841], val_loss: 885.1268\n",
      "gradient norm: 267.8772716522217, minimum ratio: 0.765625\n",
      "Epoch [842], val_loss: 888.7775\n",
      "gradient norm: 268.60631370544434, minimum ratio: 0.77734375\n",
      "Epoch [843], val_loss: 892.4394\n",
      "gradient norm: 269.34329414367676, minimum ratio: 0.765625\n",
      "Epoch [844], val_loss: 896.1124\n",
      "gradient norm: 270.07349014282227, minimum ratio: 0.75390625\n",
      "Epoch [845], val_loss: 899.7966\n",
      "gradient norm: 270.8093204498291, minimum ratio: 0.75390625\n",
      "Epoch [846], val_loss: 903.4919\n",
      "gradient norm: 271.5506248474121, minimum ratio: 0.76953125\n",
      "Epoch [847], val_loss: 907.1983\n",
      "gradient norm: 272.29491424560547, minimum ratio: 0.76953125\n",
      "Epoch [848], val_loss: 910.9160\n",
      "gradient norm: 273.042516708374, minimum ratio: 0.76953125\n",
      "Epoch [849], val_loss: 914.6449\n",
      "gradient norm: 273.788667678833, minimum ratio: 0.75\n",
      "Epoch [850], val_loss: 918.3852\n",
      "gradient norm: 274.53499031066895, minimum ratio: 0.78125\n",
      "Epoch [851], val_loss: 922.1367\n",
      "gradient norm: 275.2786388397217, minimum ratio: 0.78515625\n",
      "Epoch [852], val_loss: 925.8995\n",
      "gradient norm: 276.0232582092285, minimum ratio: 0.76953125\n",
      "Epoch [853], val_loss: 929.6738\n",
      "gradient norm: 276.7726078033447, minimum ratio: 0.76953125\n",
      "Epoch [854], val_loss: 933.4596\n",
      "gradient norm: 277.5205497741699, minimum ratio: 0.80078125\n",
      "Epoch [855], val_loss: 937.2569\n",
      "gradient norm: 278.2741527557373, minimum ratio: 0.76171875\n",
      "Epoch [856], val_loss: 941.0654\n",
      "gradient norm: 279.0283546447754, minimum ratio: 0.765625\n",
      "Epoch [857], val_loss: 944.8856\n",
      "gradient norm: 279.7809638977051, minimum ratio: 0.76171875\n",
      "Epoch [858], val_loss: 948.7173\n",
      "gradient norm: 280.5346050262451, minimum ratio: 0.76171875\n",
      "Epoch [859], val_loss: 952.5605\n",
      "gradient norm: 281.2949161529541, minimum ratio: 0.80078125\n",
      "Epoch [860], val_loss: 956.4157\n",
      "gradient norm: 282.059289932251, minimum ratio: 0.77734375\n",
      "Epoch [861], val_loss: 960.2823\n",
      "gradient norm: 282.8191337585449, minimum ratio: 0.765625\n",
      "Epoch [862], val_loss: 964.1606\n",
      "gradient norm: 283.5842514038086, minimum ratio: 0.78125\n",
      "Epoch [863], val_loss: 968.0506\n",
      "gradient norm: 284.3459949493408, minimum ratio: 0.75390625\n",
      "Epoch [864], val_loss: 971.9522\n",
      "gradient norm: 285.1140480041504, minimum ratio: 0.7890625\n",
      "Epoch [865], val_loss: 975.8653\n",
      "gradient norm: 285.86586570739746, minimum ratio: 0.765625\n",
      "Epoch [866], val_loss: 979.7901\n",
      "gradient norm: 286.6331539154053, minimum ratio: 0.78125\n",
      "Epoch [867], val_loss: 983.7267\n",
      "gradient norm: 287.3983211517334, minimum ratio: 0.74609375\n",
      "Epoch [868], val_loss: 987.6750\n",
      "gradient norm: 288.16779708862305, minimum ratio: 0.7734375\n",
      "Epoch [869], val_loss: 991.6350\n",
      "gradient norm: 288.9383792877197, minimum ratio: 0.77734375\n",
      "Epoch [870], val_loss: 995.6064\n",
      "gradient norm: 289.71239280700684, minimum ratio: 0.76953125\n",
      "Epoch [871], val_loss: 999.5900\n",
      "gradient norm: 290.4824848175049, minimum ratio: 0.76953125\n",
      "Epoch [872], val_loss: 1003.5852\n",
      "gradient norm: 291.2596378326416, minimum ratio: 0.76171875\n",
      "Epoch [873], val_loss: 1007.5924\n",
      "gradient norm: 292.0377426147461, minimum ratio: 0.76171875\n",
      "Epoch [874], val_loss: 1011.6116\n",
      "gradient norm: 292.810941696167, minimum ratio: 0.75\n",
      "Epoch [875], val_loss: 1015.6426\n",
      "gradient norm: 293.5892581939697, minimum ratio: 0.78125\n",
      "Epoch [876], val_loss: 1019.6857\n",
      "gradient norm: 294.37340545654297, minimum ratio: 0.77734375\n",
      "Epoch [877], val_loss: 1023.7405\n",
      "gradient norm: 295.1553497314453, minimum ratio: 0.77734375\n",
      "Epoch [878], val_loss: 1027.8075\n",
      "gradient norm: 295.9322681427002, minimum ratio: 0.76953125\n",
      "Epoch [879], val_loss: 1031.8862\n",
      "gradient norm: 296.71178245544434, minimum ratio: 0.78125\n",
      "Epoch [880], val_loss: 1035.9771\n",
      "gradient norm: 297.4963798522949, minimum ratio: 0.7890625\n",
      "Epoch [881], val_loss: 1040.0798\n",
      "gradient norm: 298.28466796875, minimum ratio: 0.75\n",
      "Epoch [882], val_loss: 1044.1946\n",
      "gradient norm: 299.06930923461914, minimum ratio: 0.765625\n",
      "Epoch [883], val_loss: 1048.3213\n",
      "gradient norm: 299.861270904541, minimum ratio: 0.76953125\n",
      "Epoch [884], val_loss: 1052.4598\n",
      "gradient norm: 300.65062522888184, minimum ratio: 0.76953125\n",
      "Epoch [885], val_loss: 1056.6108\n",
      "gradient norm: 301.44470024108887, minimum ratio: 0.76171875\n",
      "Epoch [886], val_loss: 1060.7738\n",
      "gradient norm: 302.2381057739258, minimum ratio: 0.78125\n",
      "Epoch [887], val_loss: 1064.9490\n",
      "gradient norm: 303.0274085998535, minimum ratio: 0.75390625\n",
      "Epoch [888], val_loss: 1069.1365\n",
      "gradient norm: 303.82767486572266, minimum ratio: 0.77734375\n",
      "Epoch [889], val_loss: 1073.3361\n",
      "gradient norm: 304.62925148010254, minimum ratio: 0.76953125\n",
      "Epoch [890], val_loss: 1077.5481\n",
      "gradient norm: 305.42244148254395, minimum ratio: 0.76953125\n",
      "Epoch [891], val_loss: 1081.7725\n",
      "gradient norm: 306.2210350036621, minimum ratio: 0.765625\n",
      "Epoch [892], val_loss: 1086.0089\n",
      "gradient norm: 307.0158042907715, minimum ratio: 0.765625\n",
      "Epoch [893], val_loss: 1090.2577\n",
      "gradient norm: 307.81947326660156, minimum ratio: 0.7734375\n",
      "Epoch [894], val_loss: 1094.5186\n",
      "gradient norm: 308.625186920166, minimum ratio: 0.7890625\n",
      "Epoch [895], val_loss: 1098.7919\n",
      "gradient norm: 309.4240074157715, minimum ratio: 0.765625\n",
      "Epoch [896], val_loss: 1103.0773\n",
      "gradient norm: 310.2312316894531, minimum ratio: 0.75390625\n",
      "Epoch [897], val_loss: 1107.3750\n",
      "gradient norm: 311.04505348205566, minimum ratio: 0.76953125\n",
      "Epoch [898], val_loss: 1111.6853\n",
      "gradient norm: 311.8556365966797, minimum ratio: 0.75390625\n",
      "Epoch [899], val_loss: 1116.0077\n",
      "gradient norm: 312.6628303527832, minimum ratio: 0.75390625\n",
      "Epoch [900], val_loss: 1120.3425\n",
      "gradient norm: 313.4789009094238, minimum ratio: 0.76953125\n",
      "Epoch [901], val_loss: 1124.6902\n",
      "gradient norm: 314.2930335998535, minimum ratio: 0.76171875\n",
      "Epoch [902], val_loss: 1129.0503\n",
      "gradient norm: 315.11230278015137, minimum ratio: 0.7421875\n",
      "Epoch [903], val_loss: 1133.4230\n",
      "gradient norm: 315.93115043640137, minimum ratio: 0.76171875\n",
      "Epoch [904], val_loss: 1137.8079\n",
      "gradient norm: 316.7519474029541, minimum ratio: 0.765625\n",
      "Epoch [905], val_loss: 1142.2052\n",
      "gradient norm: 317.56689453125, minimum ratio: 0.75390625\n",
      "Epoch [906], val_loss: 1146.6152\n",
      "gradient norm: 318.3798484802246, minimum ratio: 0.76953125\n",
      "Epoch [907], val_loss: 1151.0381\n",
      "gradient norm: 319.19755363464355, minimum ratio: 0.7734375\n",
      "Epoch [908], val_loss: 1155.4738\n",
      "gradient norm: 320.0141353607178, minimum ratio: 0.78125\n",
      "Epoch [909], val_loss: 1159.9215\n",
      "gradient norm: 320.8344497680664, minimum ratio: 0.77734375\n",
      "Epoch [910], val_loss: 1164.3822\n",
      "gradient norm: 321.66643714904785, minimum ratio: 0.78125\n",
      "Epoch [911], val_loss: 1168.8555\n",
      "gradient norm: 322.4945888519287, minimum ratio: 0.7578125\n",
      "Epoch [912], val_loss: 1173.3417\n",
      "gradient norm: 323.3192825317383, minimum ratio: 0.77734375\n",
      "Epoch [913], val_loss: 1177.8406\n",
      "gradient norm: 324.14212226867676, minimum ratio: 0.76953125\n",
      "Epoch [914], val_loss: 1182.3522\n",
      "gradient norm: 324.9738368988037, minimum ratio: 0.75\n",
      "Epoch [915], val_loss: 1186.8766\n",
      "gradient norm: 325.80192375183105, minimum ratio: 0.78515625\n",
      "Epoch [916], val_loss: 1191.4139\n",
      "gradient norm: 326.6398983001709, minimum ratio: 0.79296875\n",
      "Epoch [917], val_loss: 1195.9640\n",
      "gradient norm: 327.476957321167, minimum ratio: 0.78515625\n",
      "Epoch [918], val_loss: 1200.5269\n",
      "gradient norm: 328.3170394897461, minimum ratio: 0.76953125\n",
      "Epoch [919], val_loss: 1205.1027\n",
      "gradient norm: 329.15250968933105, minimum ratio: 0.78515625\n",
      "Epoch [920], val_loss: 1209.6909\n",
      "gradient norm: 329.9876079559326, minimum ratio: 0.77734375\n",
      "Epoch [921], val_loss: 1214.2922\n",
      "gradient norm: 330.82637786865234, minimum ratio: 0.78515625\n",
      "Epoch [922], val_loss: 1218.9062\n",
      "gradient norm: 331.66938972473145, minimum ratio: 0.7734375\n",
      "Epoch [923], val_loss: 1223.5337\n",
      "gradient norm: 332.5113353729248, minimum ratio: 0.765625\n",
      "Epoch [924], val_loss: 1228.1736\n",
      "gradient norm: 333.3537311553955, minimum ratio: 0.7578125\n",
      "Epoch [925], val_loss: 1232.8267\n",
      "gradient norm: 334.19628715515137, minimum ratio: 0.796875\n",
      "Epoch [926], val_loss: 1237.4929\n",
      "gradient norm: 335.05138206481934, minimum ratio: 0.78125\n",
      "Epoch [927], val_loss: 1242.1725\n",
      "gradient norm: 335.8992614746094, minimum ratio: 0.7578125\n",
      "Epoch [928], val_loss: 1246.8652\n",
      "gradient norm: 336.7450428009033, minimum ratio: 0.76953125\n",
      "Epoch [929], val_loss: 1251.5707\n",
      "gradient norm: 337.6015396118164, minimum ratio: 0.7578125\n",
      "Epoch [930], val_loss: 1256.2893\n",
      "gradient norm: 338.4519729614258, minimum ratio: 0.7734375\n",
      "Epoch [931], val_loss: 1261.0210\n",
      "gradient norm: 339.30329513549805, minimum ratio: 0.796875\n",
      "Epoch [932], val_loss: 1265.7657\n",
      "gradient norm: 340.15170097351074, minimum ratio: 0.76171875\n",
      "Epoch [933], val_loss: 1270.5238\n",
      "gradient norm: 341.0126609802246, minimum ratio: 0.7734375\n",
      "Epoch [934], val_loss: 1275.2949\n",
      "gradient norm: 341.869197845459, minimum ratio: 0.76953125\n",
      "Epoch [935], val_loss: 1280.0790\n",
      "gradient norm: 342.73460388183594, minimum ratio: 0.78125\n",
      "Epoch [936], val_loss: 1284.8761\n",
      "gradient norm: 343.60059928894043, minimum ratio: 0.76171875\n",
      "Epoch [937], val_loss: 1289.6870\n",
      "gradient norm: 344.4582405090332, minimum ratio: 0.76953125\n",
      "Epoch [938], val_loss: 1294.5107\n",
      "gradient norm: 345.32094383239746, minimum ratio: 0.765625\n",
      "Epoch [939], val_loss: 1299.3479\n",
      "gradient norm: 346.1811332702637, minimum ratio: 0.76171875\n",
      "Epoch [940], val_loss: 1304.1985\n",
      "gradient norm: 347.052791595459, minimum ratio: 0.78125\n",
      "Epoch [941], val_loss: 1309.0624\n",
      "gradient norm: 347.9234142303467, minimum ratio: 0.78125\n",
      "Epoch [942], val_loss: 1313.9397\n",
      "gradient norm: 348.7901210784912, minimum ratio: 0.74609375\n",
      "Epoch [943], val_loss: 1318.8308\n",
      "gradient norm: 349.6541633605957, minimum ratio: 0.76171875\n",
      "Epoch [944], val_loss: 1323.7351\n",
      "gradient norm: 350.5258541107178, minimum ratio: 0.78515625\n",
      "Epoch [945], val_loss: 1328.6531\n",
      "gradient norm: 351.39527130126953, minimum ratio: 0.75390625\n",
      "Epoch [946], val_loss: 1333.5848\n",
      "gradient norm: 352.26780891418457, minimum ratio: 0.76953125\n",
      "Epoch [947], val_loss: 1338.5302\n",
      "gradient norm: 353.14513778686523, minimum ratio: 0.765625\n",
      "Epoch [948], val_loss: 1343.4890\n",
      "gradient norm: 354.01931381225586, minimum ratio: 0.7734375\n",
      "Epoch [949], val_loss: 1348.4611\n",
      "gradient norm: 354.9003677368164, minimum ratio: 0.7734375\n",
      "Epoch [950], val_loss: 1353.4464\n",
      "gradient norm: 355.7852897644043, minimum ratio: 0.74609375\n",
      "Epoch [951], val_loss: 1358.4454\n",
      "gradient norm: 356.6702880859375, minimum ratio: 0.74609375\n",
      "Epoch [952], val_loss: 1363.4580\n",
      "gradient norm: 357.5410785675049, minimum ratio: 0.7734375\n",
      "Epoch [953], val_loss: 1368.4846\n",
      "gradient norm: 358.42812156677246, minimum ratio: 0.7578125\n",
      "Epoch [954], val_loss: 1373.5248\n",
      "gradient norm: 359.31695556640625, minimum ratio: 0.765625\n",
      "Epoch [955], val_loss: 1378.5790\n",
      "gradient norm: 360.19722747802734, minimum ratio: 0.7578125\n",
      "Epoch [956], val_loss: 1383.6467\n",
      "gradient norm: 361.0892143249512, minimum ratio: 0.75390625\n",
      "Epoch [957], val_loss: 1388.7281\n",
      "gradient norm: 361.9808750152588, minimum ratio: 0.7421875\n",
      "Epoch [958], val_loss: 1393.8229\n",
      "gradient norm: 362.87782859802246, minimum ratio: 0.7578125\n",
      "Epoch [959], val_loss: 1398.9319\n",
      "gradient norm: 363.7626953125, minimum ratio: 0.77734375\n",
      "Epoch [960], val_loss: 1404.0543\n",
      "gradient norm: 364.65527153015137, minimum ratio: 0.76953125\n",
      "Epoch [961], val_loss: 1409.1907\n",
      "gradient norm: 365.54370880126953, minimum ratio: 0.76953125\n",
      "Epoch [962], val_loss: 1414.3406\n",
      "gradient norm: 366.4385395050049, minimum ratio: 0.7578125\n",
      "Epoch [963], val_loss: 1419.5044\n",
      "gradient norm: 367.3381404876709, minimum ratio: 0.7734375\n",
      "Epoch [964], val_loss: 1424.6820\n",
      "gradient norm: 368.24344635009766, minimum ratio: 0.75390625\n",
      "Epoch [965], val_loss: 1429.8734\n",
      "gradient norm: 369.14344024658203, minimum ratio: 0.78125\n",
      "Epoch [966], val_loss: 1435.0787\n",
      "gradient norm: 370.0433006286621, minimum ratio: 0.76171875\n",
      "Epoch [967], val_loss: 1440.2983\n",
      "gradient norm: 370.9511184692383, minimum ratio: 0.734375\n",
      "Epoch [968], val_loss: 1445.5319\n",
      "gradient norm: 371.85551261901855, minimum ratio: 0.76171875\n",
      "Epoch [969], val_loss: 1450.7797\n",
      "gradient norm: 372.7580738067627, minimum ratio: 0.75\n",
      "Epoch [970], val_loss: 1456.0415\n",
      "gradient norm: 373.6651496887207, minimum ratio: 0.76953125\n",
      "Epoch [971], val_loss: 1461.3175\n",
      "gradient norm: 374.5781307220459, minimum ratio: 0.7578125\n",
      "Epoch [972], val_loss: 1466.6069\n",
      "gradient norm: 375.4948196411133, minimum ratio: 0.765625\n",
      "Epoch [973], val_loss: 1471.9106\n",
      "gradient norm: 376.4100818634033, minimum ratio: 0.76171875\n",
      "Epoch [974], val_loss: 1477.2284\n",
      "gradient norm: 377.32162857055664, minimum ratio: 0.77734375\n",
      "Epoch [975], val_loss: 1482.5602\n",
      "gradient norm: 378.2330780029297, minimum ratio: 0.7734375\n",
      "Epoch [976], val_loss: 1487.9060\n",
      "gradient norm: 379.1505184173584, minimum ratio: 0.76171875\n",
      "Epoch [977], val_loss: 1493.2662\n",
      "gradient norm: 380.07421684265137, minimum ratio: 0.76171875\n",
      "Epoch [978], val_loss: 1498.6405\n",
      "gradient norm: 380.9823913574219, minimum ratio: 0.7578125\n",
      "Epoch [979], val_loss: 1504.0289\n",
      "gradient norm: 381.89686012268066, minimum ratio: 0.76953125\n",
      "Epoch [980], val_loss: 1509.4319\n",
      "gradient norm: 382.8082695007324, minimum ratio: 0.7578125\n",
      "Epoch [981], val_loss: 1514.8492\n",
      "gradient norm: 383.7368812561035, minimum ratio: 0.74609375\n",
      "Epoch [982], val_loss: 1520.2809\n",
      "gradient norm: 384.6612739562988, minimum ratio: 0.765625\n",
      "Epoch [983], val_loss: 1525.7268\n",
      "gradient norm: 385.5895881652832, minimum ratio: 0.75\n",
      "Epoch [984], val_loss: 1531.1869\n",
      "gradient norm: 386.5179443359375, minimum ratio: 0.75\n",
      "Epoch [985], val_loss: 1536.6616\n",
      "gradient norm: 387.4423637390137, minimum ratio: 0.76171875\n",
      "Epoch [986], val_loss: 1542.1506\n",
      "gradient norm: 388.36995697021484, minimum ratio: 0.765625\n",
      "Epoch [987], val_loss: 1547.6543\n",
      "gradient norm: 389.3080062866211, minimum ratio: 0.75\n",
      "Epoch [988], val_loss: 1553.1722\n",
      "gradient norm: 390.23938751220703, minimum ratio: 0.7734375\n",
      "Epoch [989], val_loss: 1558.7047\n",
      "gradient norm: 391.16691970825195, minimum ratio: 0.75390625\n",
      "Epoch [990], val_loss: 1564.2515\n",
      "gradient norm: 392.10924530029297, minimum ratio: 0.77734375\n",
      "Epoch [991], val_loss: 1569.8127\n",
      "gradient norm: 393.0533676147461, minimum ratio: 0.76953125\n",
      "Epoch [992], val_loss: 1575.3888\n",
      "gradient norm: 393.99268341064453, minimum ratio: 0.79296875\n",
      "Epoch [993], val_loss: 1580.9790\n",
      "gradient norm: 394.931339263916, minimum ratio: 0.76953125\n",
      "Epoch [994], val_loss: 1586.5841\n",
      "gradient norm: 395.8699264526367, minimum ratio: 0.76171875\n",
      "Epoch [995], val_loss: 1592.2040\n",
      "gradient norm: 396.8123016357422, minimum ratio: 0.7578125\n",
      "Epoch [996], val_loss: 1597.8385\n",
      "gradient norm: 397.7560615539551, minimum ratio: 0.78125\n",
      "Epoch [997], val_loss: 1603.4878\n",
      "gradient norm: 398.6944770812988, minimum ratio: 0.75390625\n",
      "Epoch [998], val_loss: 1609.1516\n",
      "gradient norm: 399.64423751831055, minimum ratio: 0.75390625\n",
      "Epoch [999], val_loss: 1614.8307\n",
      "gradient norm: 400.59272384643555, minimum ratio: 0.7890625\n",
      "Epoch [1000], val_loss: 1620.5238\n",
      "gradient norm: 401.5467987060547, minimum ratio: 0.7421875\n",
      "Epoch [1001], val_loss: 1626.2317\n",
      "gradient norm: 402.49804306030273, minimum ratio: 0.7578125\n",
      "Epoch [1002], val_loss: 1631.9543\n",
      "gradient norm: 403.4425849914551, minimum ratio: 0.78125\n",
      "Epoch [1003], val_loss: 1637.6919\n",
      "gradient norm: 404.40051651000977, minimum ratio: 0.75390625\n",
      "Epoch [1004], val_loss: 1643.4442\n",
      "gradient norm: 405.3604736328125, minimum ratio: 0.796875\n",
      "Epoch [1005], val_loss: 1649.2113\n",
      "gradient norm: 406.3227729797363, minimum ratio: 0.77734375\n",
      "Epoch [1006], val_loss: 1654.9933\n",
      "gradient norm: 407.2791976928711, minimum ratio: 0.75390625\n",
      "Epoch [1007], val_loss: 1660.7896\n",
      "gradient norm: 408.2450981140137, minimum ratio: 0.77734375\n",
      "Epoch [1008], val_loss: 1666.6012\n",
      "gradient norm: 409.2036437988281, minimum ratio: 0.75\n",
      "Epoch [1009], val_loss: 1672.4279\n",
      "gradient norm: 410.1583938598633, minimum ratio: 0.7734375\n",
      "Epoch [1010], val_loss: 1678.2694\n",
      "gradient norm: 411.12743759155273, minimum ratio: 0.75390625\n",
      "Epoch [1011], val_loss: 1684.1261\n",
      "gradient norm: 412.0950012207031, minimum ratio: 0.7578125\n",
      "Epoch [1012], val_loss: 1689.9977\n",
      "gradient norm: 413.0544662475586, minimum ratio: 0.73828125\n",
      "Epoch [1013], val_loss: 1695.8843\n",
      "gradient norm: 414.0262985229492, minimum ratio: 0.7890625\n",
      "Epoch [1014], val_loss: 1701.7858\n",
      "gradient norm: 414.991397857666, minimum ratio: 0.765625\n",
      "Epoch [1015], val_loss: 1707.7028\n",
      "gradient norm: 415.9621124267578, minimum ratio: 0.7578125\n",
      "Epoch [1016], val_loss: 1713.6343\n",
      "gradient norm: 416.93372344970703, minimum ratio: 0.80078125\n",
      "Epoch [1017], val_loss: 1719.5813\n",
      "gradient norm: 417.9042549133301, minimum ratio: 0.796875\n",
      "Epoch [1018], val_loss: 1725.5430\n",
      "gradient norm: 418.8829803466797, minimum ratio: 0.78515625\n",
      "Epoch [1019], val_loss: 1731.5199\n",
      "gradient norm: 419.8699073791504, minimum ratio: 0.76171875\n",
      "Epoch [1020], val_loss: 1737.5118\n",
      "gradient norm: 420.8504333496094, minimum ratio: 0.75390625\n",
      "Epoch [1021], val_loss: 1743.5188\n",
      "gradient norm: 421.8308906555176, minimum ratio: 0.73828125\n",
      "Epoch [1022], val_loss: 1749.5415\n",
      "gradient norm: 422.81960678100586, minimum ratio: 0.78125\n",
      "Epoch [1023], val_loss: 1755.5793\n",
      "gradient norm: 423.7865295410156, minimum ratio: 0.76953125\n",
      "Epoch [1024], val_loss: 1761.6329\n",
      "gradient norm: 424.7706184387207, minimum ratio: 0.76171875\n",
      "Epoch [1025], val_loss: 1767.7014\n",
      "gradient norm: 425.7555923461914, minimum ratio: 0.7578125\n",
      "Epoch [1026], val_loss: 1773.7854\n",
      "gradient norm: 426.74724197387695, minimum ratio: 0.76953125\n",
      "Epoch [1027], val_loss: 1779.8848\n",
      "gradient norm: 427.72591400146484, minimum ratio: 0.7578125\n",
      "Epoch [1028], val_loss: 1785.9996\n",
      "gradient norm: 428.7232437133789, minimum ratio: 0.76171875\n",
      "Epoch [1029], val_loss: 1792.1300\n",
      "gradient norm: 429.7157554626465, minimum ratio: 0.75\n",
      "Epoch [1030], val_loss: 1798.2759\n",
      "gradient norm: 430.7109832763672, minimum ratio: 0.76171875\n",
      "Epoch [1031], val_loss: 1804.4373\n",
      "gradient norm: 431.6970100402832, minimum ratio: 0.74609375\n",
      "Epoch [1032], val_loss: 1810.6138\n",
      "gradient norm: 432.6973876953125, minimum ratio: 0.7578125\n",
      "Epoch [1033], val_loss: 1816.8058\n",
      "gradient norm: 433.69803619384766, minimum ratio: 0.7734375\n",
      "Epoch [1034], val_loss: 1823.0135\n",
      "gradient norm: 434.70227432250977, minimum ratio: 0.7734375\n",
      "Epoch [1035], val_loss: 1829.2365\n",
      "gradient norm: 435.699951171875, minimum ratio: 0.76171875\n",
      "Epoch [1036], val_loss: 1835.4747\n",
      "gradient norm: 436.6924629211426, minimum ratio: 0.78515625\n",
      "Epoch [1037], val_loss: 1841.7285\n",
      "gradient norm: 437.7030601501465, minimum ratio: 0.76171875\n",
      "Epoch [1038], val_loss: 1847.9979\n",
      "gradient norm: 438.70845794677734, minimum ratio: 0.78515625\n",
      "Epoch [1039], val_loss: 1854.2830\n",
      "gradient norm: 439.7149314880371, minimum ratio: 0.7578125\n",
      "Epoch [1040], val_loss: 1860.5840\n",
      "gradient norm: 440.70550537109375, minimum ratio: 0.7734375\n",
      "Epoch [1041], val_loss: 1866.9006\n",
      "gradient norm: 441.7230415344238, minimum ratio: 0.76953125\n",
      "Epoch [1042], val_loss: 1873.2332\n",
      "gradient norm: 442.7246208190918, minimum ratio: 0.78125\n",
      "Epoch [1043], val_loss: 1879.5808\n",
      "gradient norm: 443.7442398071289, minimum ratio: 0.75390625\n",
      "Epoch [1044], val_loss: 1885.9448\n",
      "gradient norm: 444.75403594970703, minimum ratio: 0.75390625\n",
      "Epoch [1045], val_loss: 1892.3246\n",
      "gradient norm: 445.7716407775879, minimum ratio: 0.75390625\n",
      "Epoch [1046], val_loss: 1898.7200\n",
      "gradient norm: 446.78785705566406, minimum ratio: 0.78515625\n",
      "Epoch [1047], val_loss: 1905.1317\n",
      "gradient norm: 447.8080825805664, minimum ratio: 0.76953125\n",
      "Epoch [1048], val_loss: 1911.5592\n",
      "gradient norm: 448.8283386230469, minimum ratio: 0.78125\n",
      "Epoch [1049], val_loss: 1918.0021\n",
      "gradient norm: 449.85530853271484, minimum ratio: 0.7734375\n",
      "Epoch [1050], val_loss: 1924.4606\n",
      "gradient norm: 450.8746643066406, minimum ratio: 0.79296875\n",
      "Epoch [1051], val_loss: 1930.9354\n",
      "gradient norm: 451.90588760375977, minimum ratio: 0.78125\n",
      "Epoch [1052], val_loss: 1937.4258\n",
      "gradient norm: 452.92601013183594, minimum ratio: 0.76953125\n",
      "Epoch [1053], val_loss: 1943.9323\n",
      "gradient norm: 453.93602752685547, minimum ratio: 0.765625\n",
      "Epoch [1054], val_loss: 1950.4550\n",
      "gradient norm: 454.9528465270996, minimum ratio: 0.76953125\n",
      "Epoch [1055], val_loss: 1956.9944\n",
      "gradient norm: 455.97929763793945, minimum ratio: 0.796875\n",
      "Epoch [1056], val_loss: 1963.5496\n",
      "gradient norm: 457.00572967529297, minimum ratio: 0.765625\n",
      "Epoch [1057], val_loss: 1970.1208\n",
      "gradient norm: 458.04588317871094, minimum ratio: 0.76953125\n",
      "Epoch [1058], val_loss: 1976.7084\n",
      "gradient norm: 459.0764274597168, minimum ratio: 0.765625\n",
      "Epoch [1059], val_loss: 1983.3121\n",
      "gradient norm: 460.11907958984375, minimum ratio: 0.76171875\n",
      "Epoch [1060], val_loss: 1989.9315\n",
      "gradient norm: 461.1575126647949, minimum ratio: 0.75390625\n",
      "Epoch [1061], val_loss: 1996.5669\n",
      "gradient norm: 462.2037544250488, minimum ratio: 0.765625\n",
      "Epoch [1062], val_loss: 2003.2184\n",
      "gradient norm: 463.24838638305664, minimum ratio: 0.7890625\n",
      "Epoch [1063], val_loss: 2009.8860\n",
      "gradient norm: 464.29072189331055, minimum ratio: 0.76953125\n",
      "Epoch [1064], val_loss: 2016.5692\n",
      "gradient norm: 465.33810806274414, minimum ratio: 0.76171875\n",
      "Epoch [1065], val_loss: 2023.2692\n",
      "gradient norm: 466.3813819885254, minimum ratio: 0.77734375\n",
      "Epoch [1066], val_loss: 2029.9855\n",
      "gradient norm: 467.4295349121094, minimum ratio: 0.75\n",
      "Epoch [1067], val_loss: 2036.7175\n",
      "gradient norm: 468.4750213623047, minimum ratio: 0.76953125\n",
      "Epoch [1068], val_loss: 2043.4656\n",
      "gradient norm: 469.51874923706055, minimum ratio: 0.76171875\n",
      "Epoch [1069], val_loss: 2050.2307\n",
      "gradient norm: 470.5607490539551, minimum ratio: 0.75390625\n",
      "Epoch [1070], val_loss: 2057.0127\n",
      "gradient norm: 471.6099739074707, minimum ratio: 0.75390625\n",
      "Epoch [1071], val_loss: 2063.8105\n",
      "gradient norm: 472.66071701049805, minimum ratio: 0.75390625\n",
      "Epoch [1072], val_loss: 2070.6255\n",
      "gradient norm: 473.7222900390625, minimum ratio: 0.75390625\n",
      "Epoch [1073], val_loss: 2077.4570\n",
      "gradient norm: 474.7740478515625, minimum ratio: 0.7578125\n",
      "Epoch [1074], val_loss: 2084.3049\n",
      "gradient norm: 475.83417892456055, minimum ratio: 0.75390625\n",
      "Epoch [1075], val_loss: 2091.1697\n",
      "gradient norm: 476.88444900512695, minimum ratio: 0.73828125\n",
      "Epoch [1076], val_loss: 2098.0513\n",
      "gradient norm: 477.95315170288086, minimum ratio: 0.7578125\n",
      "Epoch [1077], val_loss: 2104.9492\n",
      "gradient norm: 479.00393295288086, minimum ratio: 0.7578125\n",
      "Epoch [1078], val_loss: 2111.8630\n",
      "gradient norm: 480.0643005371094, minimum ratio: 0.76171875\n",
      "Epoch [1079], val_loss: 2118.7937\n",
      "gradient norm: 481.1299819946289, minimum ratio: 0.7890625\n",
      "Epoch [1080], val_loss: 2125.7410\n",
      "gradient norm: 482.20126724243164, minimum ratio: 0.73046875\n",
      "Epoch [1081], val_loss: 2132.7048\n",
      "gradient norm: 483.2598304748535, minimum ratio: 0.78125\n",
      "Epoch [1082], val_loss: 2139.6853\n",
      "gradient norm: 484.332763671875, minimum ratio: 0.7890625\n",
      "Epoch [1083], val_loss: 2146.6826\n",
      "gradient norm: 485.40172958374023, minimum ratio: 0.7890625\n",
      "Epoch [1084], val_loss: 2153.6965\n",
      "gradient norm: 486.4745559692383, minimum ratio: 0.78515625\n",
      "Epoch [1085], val_loss: 2160.7268\n",
      "gradient norm: 487.5503807067871, minimum ratio: 0.7734375\n",
      "Epoch [1086], val_loss: 2167.7744\n",
      "gradient norm: 488.6068992614746, minimum ratio: 0.7734375\n",
      "Epoch [1087], val_loss: 2174.8386\n",
      "gradient norm: 489.6868324279785, minimum ratio: 0.7890625\n",
      "Epoch [1088], val_loss: 2181.9194\n",
      "gradient norm: 490.7719383239746, minimum ratio: 0.7734375\n",
      "Epoch [1089], val_loss: 2189.0166\n",
      "gradient norm: 491.85444259643555, minimum ratio: 0.76171875\n",
      "Epoch [1090], val_loss: 2196.1313\n",
      "gradient norm: 492.93367767333984, minimum ratio: 0.76953125\n",
      "Epoch [1091], val_loss: 2203.2625\n",
      "gradient norm: 494.01304626464844, minimum ratio: 0.765625\n",
      "Epoch [1092], val_loss: 2210.4104\n",
      "gradient norm: 495.10538482666016, minimum ratio: 0.79296875\n",
      "Epoch [1093], val_loss: 2217.5754\n",
      "gradient norm: 496.20005798339844, minimum ratio: 0.7734375\n",
      "Epoch [1094], val_loss: 2224.7578\n",
      "gradient norm: 497.28424072265625, minimum ratio: 0.77734375\n",
      "Epoch [1095], val_loss: 2231.9573\n",
      "gradient norm: 498.38122940063477, minimum ratio: 0.76171875\n",
      "Epoch [1096], val_loss: 2239.1733\n",
      "gradient norm: 499.4645538330078, minimum ratio: 0.78515625\n",
      "Epoch [1097], val_loss: 2246.4065\n",
      "gradient norm: 500.5588188171387, minimum ratio: 0.79296875\n",
      "Epoch [1098], val_loss: 2253.6567\n",
      "gradient norm: 501.6427345275879, minimum ratio: 0.796875\n",
      "Epoch [1099], val_loss: 2260.9241\n",
      "gradient norm: 502.72388458251953, minimum ratio: 0.76953125\n",
      "Epoch [1100], val_loss: 2268.2087\n",
      "gradient norm: 503.8194465637207, minimum ratio: 0.765625\n",
      "Epoch [1101], val_loss: 2275.5103\n",
      "gradient norm: 504.9200668334961, minimum ratio: 0.765625\n",
      "Epoch [1102], val_loss: 2282.8291\n",
      "gradient norm: 506.0158882141113, minimum ratio: 0.75390625\n",
      "Epoch [1103], val_loss: 2290.1650\n",
      "gradient norm: 507.11473846435547, minimum ratio: 0.7734375\n",
      "Epoch [1104], val_loss: 2297.5178\n",
      "gradient norm: 508.2116394042969, minimum ratio: 0.7734375\n",
      "Epoch [1105], val_loss: 2304.8882\n",
      "gradient norm: 509.31563568115234, minimum ratio: 0.76953125\n",
      "Epoch [1106], val_loss: 2312.2756\n",
      "gradient norm: 510.428409576416, minimum ratio: 0.75390625\n",
      "Epoch [1107], val_loss: 2319.6802\n",
      "gradient norm: 511.52029037475586, minimum ratio: 0.765625\n",
      "Epoch [1108], val_loss: 2327.1030\n",
      "gradient norm: 512.6203346252441, minimum ratio: 0.7734375\n",
      "Epoch [1109], val_loss: 2334.5425\n",
      "gradient norm: 513.7380447387695, minimum ratio: 0.77734375\n",
      "Epoch [1110], val_loss: 2341.9995\n",
      "gradient norm: 514.8546676635742, minimum ratio: 0.76171875\n",
      "Epoch [1111], val_loss: 2349.4741\n",
      "gradient norm: 515.9685859680176, minimum ratio: 0.75390625\n",
      "Epoch [1112], val_loss: 2356.9656\n",
      "gradient norm: 517.091178894043, minimum ratio: 0.77734375\n",
      "Epoch [1113], val_loss: 2364.4749\n",
      "gradient norm: 518.2118453979492, minimum ratio: 0.76953125\n",
      "Epoch [1114], val_loss: 2372.0002\n",
      "gradient norm: 519.3373718261719, minimum ratio: 0.77734375\n",
      "Epoch [1115], val_loss: 2379.5432\n",
      "gradient norm: 520.4494094848633, minimum ratio: 0.76953125\n",
      "Epoch [1116], val_loss: 2387.1040\n",
      "gradient norm: 521.5763320922852, minimum ratio: 0.7734375\n",
      "Epoch [1117], val_loss: 2394.6826\n",
      "gradient norm: 522.6804084777832, minimum ratio: 0.78515625\n",
      "Epoch [1118], val_loss: 2402.2788\n",
      "gradient norm: 523.8020858764648, minimum ratio: 0.7734375\n",
      "Epoch [1119], val_loss: 2409.8928\n",
      "gradient norm: 524.9167175292969, minimum ratio: 0.7578125\n",
      "Epoch [1120], val_loss: 2417.5247\n",
      "gradient norm: 526.0377998352051, minimum ratio: 0.76953125\n",
      "Epoch [1121], val_loss: 2425.1741\n",
      "gradient norm: 527.1678657531738, minimum ratio: 0.78515625\n",
      "Epoch [1122], val_loss: 2432.8413\n",
      "gradient norm: 528.2963180541992, minimum ratio: 0.77734375\n",
      "Epoch [1123], val_loss: 2440.5264\n",
      "gradient norm: 529.428409576416, minimum ratio: 0.7734375\n",
      "Epoch [1124], val_loss: 2448.2283\n",
      "gradient norm: 530.5693664550781, minimum ratio: 0.74609375\n",
      "Epoch [1125], val_loss: 2455.9482\n",
      "gradient norm: 531.6896476745605, minimum ratio: 0.78125\n",
      "Epoch [1126], val_loss: 2463.6863\n",
      "gradient norm: 532.8329467773438, minimum ratio: 0.75\n",
      "Epoch [1127], val_loss: 2471.4421\n",
      "gradient norm: 533.9624099731445, minimum ratio: 0.796875\n",
      "Epoch [1128], val_loss: 2479.2153\n",
      "gradient norm: 535.0963973999023, minimum ratio: 0.7578125\n",
      "Epoch [1129], val_loss: 2487.0066\n",
      "gradient norm: 536.2329864501953, minimum ratio: 0.78125\n",
      "Epoch [1130], val_loss: 2494.8164\n",
      "gradient norm: 537.365795135498, minimum ratio: 0.76171875\n",
      "Epoch [1131], val_loss: 2502.6438\n",
      "gradient norm: 538.5001907348633, minimum ratio: 0.765625\n",
      "Epoch [1132], val_loss: 2510.4890\n",
      "gradient norm: 539.6539306640625, minimum ratio: 0.78125\n",
      "Epoch [1133], val_loss: 2518.3521\n",
      "gradient norm: 540.794677734375, minimum ratio: 0.75390625\n",
      "Epoch [1134], val_loss: 2526.2332\n",
      "gradient norm: 541.9402122497559, minimum ratio: 0.7421875\n",
      "Epoch [1135], val_loss: 2534.1318\n",
      "gradient norm: 543.0774879455566, minimum ratio: 0.765625\n",
      "Epoch [1136], val_loss: 2542.0488\n",
      "gradient norm: 544.2371520996094, minimum ratio: 0.76953125\n",
      "Epoch [1137], val_loss: 2549.9846\n",
      "gradient norm: 545.3880157470703, minimum ratio: 0.796875\n",
      "Epoch [1138], val_loss: 2557.9377\n",
      "gradient norm: 546.5431251525879, minimum ratio: 0.7734375\n",
      "Epoch [1139], val_loss: 2565.9099\n",
      "gradient norm: 547.6887702941895, minimum ratio: 0.765625\n",
      "Epoch [1140], val_loss: 2573.8999\n",
      "gradient norm: 548.8502006530762, minimum ratio: 0.77734375\n",
      "Epoch [1141], val_loss: 2581.9080\n",
      "gradient norm: 550.0175018310547, minimum ratio: 0.76953125\n",
      "Epoch [1142], val_loss: 2589.9338\n",
      "gradient norm: 551.1720352172852, minimum ratio: 0.77734375\n",
      "Epoch [1143], val_loss: 2597.9780\n",
      "gradient norm: 552.3355751037598, minimum ratio: 0.7578125\n",
      "Epoch [1144], val_loss: 2606.0408\n",
      "gradient norm: 553.5075531005859, minimum ratio: 0.76953125\n",
      "Epoch [1145], val_loss: 2614.1216\n",
      "gradient norm: 554.6810836791992, minimum ratio: 0.765625\n",
      "Epoch [1146], val_loss: 2622.2207\n",
      "gradient norm: 555.849967956543, minimum ratio: 0.77734375\n",
      "Epoch [1147], val_loss: 2630.3374\n",
      "gradient norm: 557.0222625732422, minimum ratio: 0.77734375\n",
      "Epoch [1148], val_loss: 2638.4727\n",
      "gradient norm: 558.1820373535156, minimum ratio: 0.78125\n",
      "Epoch [1149], val_loss: 2646.6257\n",
      "gradient norm: 559.352653503418, minimum ratio: 0.76953125\n",
      "Epoch [1150], val_loss: 2654.7981\n",
      "gradient norm: 560.5065765380859, minimum ratio: 0.78125\n",
      "Epoch [1151], val_loss: 2662.9890\n",
      "gradient norm: 561.6893615722656, minimum ratio: 0.7578125\n",
      "Epoch [1152], val_loss: 2671.1982\n",
      "gradient norm: 562.853931427002, minimum ratio: 0.7578125\n",
      "Epoch [1153], val_loss: 2679.4255\n",
      "gradient norm: 564.024299621582, minimum ratio: 0.7578125\n",
      "Epoch [1154], val_loss: 2687.6716\n",
      "gradient norm: 565.2067222595215, minimum ratio: 0.76171875\n",
      "Epoch [1155], val_loss: 2695.9358\n",
      "gradient norm: 566.395622253418, minimum ratio: 0.7890625\n",
      "Epoch [1156], val_loss: 2704.2178\n",
      "gradient norm: 567.5782356262207, minimum ratio: 0.765625\n",
      "Epoch [1157], val_loss: 2712.5188\n",
      "gradient norm: 568.7574424743652, minimum ratio: 0.75390625\n",
      "Epoch [1158], val_loss: 2720.8384\n",
      "gradient norm: 569.9425659179688, minimum ratio: 0.77734375\n",
      "Epoch [1159], val_loss: 2729.1768\n",
      "gradient norm: 571.1225929260254, minimum ratio: 0.76171875\n",
      "Epoch [1160], val_loss: 2737.5334\n",
      "gradient norm: 572.2971000671387, minimum ratio: 0.75\n",
      "Epoch [1161], val_loss: 2745.9099\n",
      "gradient norm: 573.4793701171875, minimum ratio: 0.76171875\n",
      "Epoch [1162], val_loss: 2754.3047\n",
      "gradient norm: 574.6727523803711, minimum ratio: 0.7578125\n",
      "Epoch [1163], val_loss: 2762.7166\n",
      "gradient norm: 575.8737945556641, minimum ratio: 0.78125\n",
      "Epoch [1164], val_loss: 2771.1475\n",
      "gradient norm: 577.047981262207, minimum ratio: 0.796875\n",
      "Epoch [1165], val_loss: 2779.5974\n",
      "gradient norm: 578.2471466064453, minimum ratio: 0.75\n",
      "Epoch [1166], val_loss: 2788.0667\n",
      "gradient norm: 579.437183380127, minimum ratio: 0.76171875\n",
      "Epoch [1167], val_loss: 2796.5544\n",
      "gradient norm: 580.6281852722168, minimum ratio: 0.75\n",
      "Epoch [1168], val_loss: 2805.0615\n",
      "gradient norm: 581.8358383178711, minimum ratio: 0.76171875\n",
      "Epoch [1169], val_loss: 2813.5874\n",
      "gradient norm: 583.0334167480469, minimum ratio: 0.77734375\n",
      "Epoch [1170], val_loss: 2822.1323\n",
      "gradient norm: 584.2220802307129, minimum ratio: 0.75390625\n",
      "Epoch [1171], val_loss: 2830.6958\n",
      "gradient norm: 585.432991027832, minimum ratio: 0.7578125\n",
      "Epoch [1172], val_loss: 2839.2778\n",
      "gradient norm: 586.6326026916504, minimum ratio: 0.75\n",
      "Epoch [1173], val_loss: 2847.8792\n",
      "gradient norm: 587.8495979309082, minimum ratio: 0.75390625\n",
      "Epoch [1174], val_loss: 2856.4993\n",
      "gradient norm: 589.0595474243164, minimum ratio: 0.76171875\n",
      "Epoch [1175], val_loss: 2865.1387\n",
      "gradient norm: 590.2777786254883, minimum ratio: 0.76953125\n",
      "Epoch [1176], val_loss: 2873.7974\n",
      "gradient norm: 591.4994087219238, minimum ratio: 0.74609375\n",
      "Epoch [1177], val_loss: 2882.4746\n",
      "gradient norm: 592.7153091430664, minimum ratio: 0.7890625\n",
      "Epoch [1178], val_loss: 2891.1711\n",
      "gradient norm: 593.9287719726562, minimum ratio: 0.75390625\n",
      "Epoch [1179], val_loss: 2899.8870\n",
      "gradient norm: 595.1286354064941, minimum ratio: 0.765625\n",
      "Epoch [1180], val_loss: 2908.6226\n",
      "gradient norm: 596.3488960266113, minimum ratio: 0.78125\n",
      "Epoch [1181], val_loss: 2917.3765\n",
      "gradient norm: 597.5700950622559, minimum ratio: 0.76953125\n",
      "Epoch [1182], val_loss: 2926.1499\n",
      "gradient norm: 598.7762603759766, minimum ratio: 0.765625\n",
      "Epoch [1183], val_loss: 2934.9421\n",
      "gradient norm: 599.9978408813477, minimum ratio: 0.7734375\n",
      "Epoch [1184], val_loss: 2943.7537\n",
      "gradient norm: 601.2237014770508, minimum ratio: 0.78125\n",
      "Epoch [1185], val_loss: 2952.5842\n",
      "gradient norm: 602.4519691467285, minimum ratio: 0.78515625\n",
      "Epoch [1186], val_loss: 2961.4343\n",
      "gradient norm: 603.6622314453125, minimum ratio: 0.7734375\n",
      "Epoch [1187], val_loss: 2970.3042\n",
      "gradient norm: 604.8908729553223, minimum ratio: 0.74609375\n",
      "Epoch [1188], val_loss: 2979.1929\n",
      "gradient norm: 606.112361907959, minimum ratio: 0.7734375\n",
      "Epoch [1189], val_loss: 2988.1013\n",
      "gradient norm: 607.3488426208496, minimum ratio: 0.76953125\n",
      "Epoch [1190], val_loss: 2997.0288\n",
      "gradient norm: 608.5854530334473, minimum ratio: 0.765625\n",
      "Epoch [1191], val_loss: 3005.9756\n",
      "gradient norm: 609.8186264038086, minimum ratio: 0.75\n",
      "Epoch [1192], val_loss: 3014.9414\n",
      "gradient norm: 611.045280456543, minimum ratio: 0.78125\n",
      "Epoch [1193], val_loss: 3023.9277\n",
      "gradient norm: 612.2936210632324, minimum ratio: 0.75390625\n",
      "Epoch [1194], val_loss: 3032.9331\n",
      "gradient norm: 613.5399208068848, minimum ratio: 0.77734375\n",
      "Epoch [1195], val_loss: 3041.9575\n",
      "gradient norm: 614.7789344787598, minimum ratio: 0.76953125\n",
      "Epoch [1196], val_loss: 3051.0020\n",
      "gradient norm: 616.028377532959, minimum ratio: 0.7734375\n",
      "Epoch [1197], val_loss: 3060.0654\n",
      "gradient norm: 617.2772636413574, minimum ratio: 0.796875\n",
      "Epoch [1198], val_loss: 3069.1484\n",
      "gradient norm: 618.5333976745605, minimum ratio: 0.76953125\n",
      "Epoch [1199], val_loss: 3078.2505\n",
      "gradient norm: 619.7704963684082, minimum ratio: 0.7578125\n",
      "Epoch [1200], val_loss: 3087.3726\n",
      "gradient norm: 621.0164031982422, minimum ratio: 0.77734375\n",
      "Epoch [1201], val_loss: 3096.5142\n",
      "gradient norm: 622.2773857116699, minimum ratio: 0.75390625\n",
      "Epoch [1202], val_loss: 3105.6758\n",
      "gradient norm: 623.5385246276855, minimum ratio: 0.7578125\n",
      "Epoch [1203], val_loss: 3114.8567\n",
      "gradient norm: 624.7946662902832, minimum ratio: 0.76171875\n",
      "Epoch [1204], val_loss: 3124.0576\n",
      "gradient norm: 626.0395736694336, minimum ratio: 0.75390625\n",
      "Epoch [1205], val_loss: 3133.2776\n",
      "gradient norm: 627.2981224060059, minimum ratio: 0.7734375\n",
      "Epoch [1206], val_loss: 3142.5178\n",
      "gradient norm: 628.5376167297363, minimum ratio: 0.765625\n",
      "Epoch [1207], val_loss: 3151.7776\n",
      "gradient norm: 629.7885475158691, minimum ratio: 0.765625\n",
      "Epoch [1208], val_loss: 3161.0574\n",
      "gradient norm: 631.057674407959, minimum ratio: 0.765625\n",
      "Epoch [1209], val_loss: 3170.3579\n",
      "gradient norm: 632.3202095031738, minimum ratio: 0.75\n",
      "Epoch [1210], val_loss: 3179.6782\n",
      "gradient norm: 633.5956153869629, minimum ratio: 0.77734375\n",
      "Epoch [1211], val_loss: 3189.0176\n",
      "gradient norm: 634.8440399169922, minimum ratio: 0.76953125\n",
      "Epoch [1212], val_loss: 3198.3772\n",
      "gradient norm: 636.0951766967773, minimum ratio: 0.78125\n",
      "Epoch [1213], val_loss: 3207.7566\n",
      "gradient norm: 637.3575897216797, minimum ratio: 0.796875\n",
      "Epoch [1214], val_loss: 3217.1572\n",
      "gradient norm: 638.6355438232422, minimum ratio: 0.76953125\n",
      "Epoch [1215], val_loss: 3226.5774\n",
      "gradient norm: 639.905330657959, minimum ratio: 0.7265625\n",
      "Epoch [1216], val_loss: 3236.0178\n",
      "gradient norm: 641.1793823242188, minimum ratio: 0.75\n",
      "Epoch [1217], val_loss: 3245.4783\n",
      "gradient norm: 642.4527244567871, minimum ratio: 0.765625\n",
      "Epoch [1218], val_loss: 3254.9595\n",
      "gradient norm: 643.7200393676758, minimum ratio: 0.7734375\n",
      "Epoch [1219], val_loss: 3264.4604\n",
      "gradient norm: 645.0098648071289, minimum ratio: 0.765625\n",
      "Epoch [1220], val_loss: 3273.9807\n",
      "gradient norm: 646.2806549072266, minimum ratio: 0.7890625\n",
      "Epoch [1221], val_loss: 3283.5222\n",
      "gradient norm: 647.5479583740234, minimum ratio: 0.75390625\n",
      "Epoch [1222], val_loss: 3293.0835\n",
      "gradient norm: 648.8350067138672, minimum ratio: 0.75\n",
      "Epoch [1223], val_loss: 3302.6653\n",
      "gradient norm: 650.1221656799316, minimum ratio: 0.76953125\n",
      "Epoch [1224], val_loss: 3312.2664\n",
      "gradient norm: 651.4198913574219, minimum ratio: 0.75390625\n",
      "Epoch [1225], val_loss: 3321.8879\n",
      "gradient norm: 652.709041595459, minimum ratio: 0.76953125\n",
      "Epoch [1226], val_loss: 3331.5300\n",
      "gradient norm: 654.0065155029297, minimum ratio: 0.78125\n",
      "Epoch [1227], val_loss: 3341.1921\n",
      "gradient norm: 655.2912902832031, minimum ratio: 0.7578125\n",
      "Epoch [1228], val_loss: 3350.8743\n",
      "gradient norm: 656.5953979492188, minimum ratio: 0.7734375\n",
      "Epoch [1229], val_loss: 3360.5771\n",
      "gradient norm: 657.8919868469238, minimum ratio: 0.765625\n",
      "Epoch [1230], val_loss: 3370.3000\n",
      "gradient norm: 659.1993026733398, minimum ratio: 0.7734375\n",
      "Epoch [1231], val_loss: 3380.0432\n",
      "gradient norm: 660.4916229248047, minimum ratio: 0.765625\n",
      "Epoch [1232], val_loss: 3389.8076\n",
      "gradient norm: 661.8022613525391, minimum ratio: 0.74609375\n",
      "Epoch [1233], val_loss: 3399.5920\n",
      "gradient norm: 663.107852935791, minimum ratio: 0.76171875\n",
      "Epoch [1234], val_loss: 3409.3970\n",
      "gradient norm: 664.4174766540527, minimum ratio: 0.75\n",
      "Epoch [1235], val_loss: 3419.2222\n",
      "gradient norm: 665.7060470581055, minimum ratio: 0.77734375\n",
      "Epoch [1236], val_loss: 3429.0684\n",
      "gradient norm: 666.9855270385742, minimum ratio: 0.76171875\n",
      "Epoch [1237], val_loss: 3438.9353\n",
      "gradient norm: 668.3042793273926, minimum ratio: 0.7734375\n",
      "Epoch [1238], val_loss: 3448.8223\n",
      "gradient norm: 669.6086044311523, minimum ratio: 0.765625\n",
      "Epoch [1239], val_loss: 3458.7300\n",
      "gradient norm: 670.899356842041, minimum ratio: 0.76953125\n",
      "Epoch [1240], val_loss: 3468.6584\n",
      "gradient norm: 672.2130889892578, minimum ratio: 0.7734375\n",
      "Epoch [1241], val_loss: 3478.6074\n",
      "gradient norm: 673.5190696716309, minimum ratio: 0.75\n",
      "Epoch [1242], val_loss: 3488.5767\n",
      "gradient norm: 674.845588684082, minimum ratio: 0.75\n",
      "Epoch [1243], val_loss: 3498.5667\n",
      "gradient norm: 676.1630020141602, minimum ratio: 0.75390625\n",
      "Epoch [1244], val_loss: 3508.5774\n",
      "gradient norm: 677.4615364074707, minimum ratio: 0.7578125\n",
      "Epoch [1245], val_loss: 3518.6091\n",
      "gradient norm: 678.7768707275391, minimum ratio: 0.75\n",
      "Epoch [1246], val_loss: 3528.6621\n",
      "gradient norm: 680.1037979125977, minimum ratio: 0.7734375\n",
      "Epoch [1247], val_loss: 3538.7354\n",
      "gradient norm: 681.4343910217285, minimum ratio: 0.78125\n",
      "Epoch [1248], val_loss: 3548.8289\n",
      "gradient norm: 682.7601203918457, minimum ratio: 0.78125\n",
      "Epoch [1249], val_loss: 3558.9434\n",
      "gradient norm: 684.0810661315918, minimum ratio: 0.7578125\n",
      "Epoch [1250], val_loss: 3569.0789\n",
      "gradient norm: 685.4122161865234, minimum ratio: 0.765625\n",
      "Epoch [1251], val_loss: 3579.2351\n",
      "gradient norm: 686.7417869567871, minimum ratio: 0.75\n",
      "Epoch [1252], val_loss: 3589.4128\n",
      "gradient norm: 688.0739212036133, minimum ratio: 0.75390625\n",
      "Epoch [1253], val_loss: 3599.6113\n",
      "gradient norm: 689.3956909179688, minimum ratio: 0.765625\n",
      "Epoch [1254], val_loss: 3609.8313\n",
      "gradient norm: 690.7293243408203, minimum ratio: 0.76953125\n",
      "Epoch [1255], val_loss: 3620.0718\n",
      "gradient norm: 692.0613594055176, minimum ratio: 0.75390625\n",
      "Epoch [1256], val_loss: 3630.3325\n",
      "gradient norm: 693.4104118347168, minimum ratio: 0.74609375\n",
      "Epoch [1257], val_loss: 3640.6145\n",
      "gradient norm: 694.7524948120117, minimum ratio: 0.765625\n",
      "Epoch [1258], val_loss: 3650.9182\n",
      "gradient norm: 696.0849151611328, minimum ratio: 0.77734375\n",
      "Epoch [1259], val_loss: 3661.2429\n",
      "gradient norm: 697.4257316589355, minimum ratio: 0.765625\n",
      "Epoch [1260], val_loss: 3671.5891\n",
      "gradient norm: 698.7561874389648, minimum ratio: 0.7734375\n",
      "Epoch [1261], val_loss: 3681.9568\n",
      "gradient norm: 700.1116333007812, minimum ratio: 0.76953125\n",
      "Epoch [1262], val_loss: 3692.3459\n",
      "gradient norm: 701.4697036743164, minimum ratio: 0.765625\n",
      "Epoch [1263], val_loss: 3702.7546\n",
      "gradient norm: 702.8231811523438, minimum ratio: 0.77734375\n",
      "Epoch [1264], val_loss: 3713.1858\n",
      "gradient norm: 704.1648406982422, minimum ratio: 0.7734375\n",
      "Epoch [1265], val_loss: 3723.6384\n",
      "gradient norm: 705.5026626586914, minimum ratio: 0.74609375\n",
      "Epoch [1266], val_loss: 3734.1130\n",
      "gradient norm: 706.8569259643555, minimum ratio: 0.7890625\n",
      "Epoch [1267], val_loss: 3744.6074\n",
      "gradient norm: 708.2050285339355, minimum ratio: 0.796875\n",
      "Epoch [1268], val_loss: 3755.1238\n",
      "gradient norm: 709.5712127685547, minimum ratio: 0.765625\n",
      "Epoch [1269], val_loss: 3765.6611\n",
      "gradient norm: 710.9236259460449, minimum ratio: 0.75390625\n",
      "Epoch [1270], val_loss: 3776.2195\n",
      "gradient norm: 712.2801399230957, minimum ratio: 0.765625\n",
      "Epoch [1271], val_loss: 3786.8008\n",
      "gradient norm: 713.6352005004883, minimum ratio: 0.7890625\n",
      "Epoch [1272], val_loss: 3797.4026\n",
      "gradient norm: 715.0057067871094, minimum ratio: 0.7734375\n",
      "Epoch [1273], val_loss: 3808.0271\n",
      "gradient norm: 716.355884552002, minimum ratio: 0.77734375\n",
      "Epoch [1274], val_loss: 3818.6721\n",
      "gradient norm: 717.7087135314941, minimum ratio: 0.7734375\n",
      "Epoch [1275], val_loss: 3829.3379\n",
      "gradient norm: 719.088752746582, minimum ratio: 0.7734375\n",
      "Epoch [1276], val_loss: 3840.0266\n",
      "gradient norm: 720.4677200317383, minimum ratio: 0.78125\n",
      "Epoch [1277], val_loss: 3850.7354\n",
      "gradient norm: 721.8299751281738, minimum ratio: 0.76953125\n",
      "Epoch [1278], val_loss: 3861.4666\n",
      "gradient norm: 723.2010612487793, minimum ratio: 0.76953125\n",
      "Epoch [1279], val_loss: 3872.2195\n",
      "gradient norm: 724.5672378540039, minimum ratio: 0.765625\n",
      "Epoch [1280], val_loss: 3882.9941\n",
      "gradient norm: 725.9413833618164, minimum ratio: 0.7734375\n",
      "Epoch [1281], val_loss: 3893.7908\n",
      "gradient norm: 727.3311424255371, minimum ratio: 0.78125\n",
      "Epoch [1282], val_loss: 3904.6086\n",
      "gradient norm: 728.7225685119629, minimum ratio: 0.76171875\n",
      "Epoch [1283], val_loss: 3915.4482\n",
      "gradient norm: 730.1051406860352, minimum ratio: 0.765625\n",
      "Epoch [1284], val_loss: 3926.3096\n",
      "gradient norm: 731.4736480712891, minimum ratio: 0.7734375\n",
      "Epoch [1285], val_loss: 3937.1934\n",
      "gradient norm: 732.8577079772949, minimum ratio: 0.734375\n",
      "Epoch [1286], val_loss: 3948.0979\n",
      "gradient norm: 734.255744934082, minimum ratio: 0.796875\n",
      "Epoch [1287], val_loss: 3959.0249\n",
      "gradient norm: 735.6554222106934, minimum ratio: 0.75390625\n",
      "Epoch [1288], val_loss: 3969.9729\n",
      "gradient norm: 737.0435791015625, minimum ratio: 0.76171875\n",
      "Epoch [1289], val_loss: 3980.9429\n",
      "gradient norm: 738.4192123413086, minimum ratio: 0.8046875\n",
      "Epoch [1290], val_loss: 3991.9358\n",
      "gradient norm: 739.8072929382324, minimum ratio: 0.76171875\n",
      "Epoch [1291], val_loss: 4002.9500\n",
      "gradient norm: 741.2041320800781, minimum ratio: 0.77734375\n",
      "Epoch [1292], val_loss: 4013.9863\n",
      "gradient norm: 742.6090698242188, minimum ratio: 0.76171875\n",
      "Epoch [1293], val_loss: 4025.0442\n",
      "gradient norm: 744.0047225952148, minimum ratio: 0.78125\n",
      "Epoch [1294], val_loss: 4036.1245\n",
      "gradient norm: 745.3812789916992, minimum ratio: 0.7734375\n",
      "Epoch [1295], val_loss: 4047.2271\n",
      "gradient norm: 746.7841110229492, minimum ratio: 0.7578125\n",
      "Epoch [1296], val_loss: 4058.3516\n",
      "gradient norm: 748.156135559082, minimum ratio: 0.76171875\n",
      "Epoch [1297], val_loss: 4069.4983\n",
      "gradient norm: 749.5699234008789, minimum ratio: 0.7734375\n",
      "Epoch [1298], val_loss: 4080.6675\n",
      "gradient norm: 750.9686889648438, minimum ratio: 0.75390625\n",
      "Epoch [1299], val_loss: 4091.8591\n",
      "gradient norm: 752.359001159668, minimum ratio: 0.76171875\n",
      "Epoch [1300], val_loss: 4103.0728\n",
      "gradient norm: 753.7678604125977, minimum ratio: 0.7734375\n",
      "Epoch [1301], val_loss: 4114.3101\n",
      "gradient norm: 755.1906127929688, minimum ratio: 0.765625\n",
      "Epoch [1302], val_loss: 4125.5679\n",
      "gradient norm: 756.5868530273438, minimum ratio: 0.75\n",
      "Epoch [1303], val_loss: 4136.8481\n",
      "gradient norm: 757.9966888427734, minimum ratio: 0.75390625\n",
      "Epoch [1304], val_loss: 4148.1519\n",
      "gradient norm: 759.4115753173828, minimum ratio: 0.765625\n",
      "Epoch [1305], val_loss: 4159.4775\n",
      "gradient norm: 760.8288955688477, minimum ratio: 0.765625\n",
      "Epoch [1306], val_loss: 4170.8247\n",
      "gradient norm: 762.2599868774414, minimum ratio: 0.77734375\n",
      "Epoch [1307], val_loss: 4182.1943\n",
      "gradient norm: 763.6925811767578, minimum ratio: 0.7734375\n",
      "Epoch [1308], val_loss: 4193.5859\n",
      "gradient norm: 765.1032791137695, minimum ratio: 0.79296875\n",
      "Epoch [1309], val_loss: 4204.9995\n",
      "gradient norm: 766.5217590332031, minimum ratio: 0.76171875\n",
      "Epoch [1310], val_loss: 4216.4365\n",
      "gradient norm: 767.9223403930664, minimum ratio: 0.765625\n",
      "Epoch [1311], val_loss: 4227.8970\n",
      "gradient norm: 769.3477325439453, minimum ratio: 0.76953125\n",
      "Epoch [1312], val_loss: 4239.3794\n",
      "gradient norm: 770.784553527832, minimum ratio: 0.7421875\n",
      "Epoch [1313], val_loss: 4250.8843\n",
      "gradient norm: 772.2270431518555, minimum ratio: 0.7734375\n",
      "Epoch [1314], val_loss: 4262.4106\n",
      "gradient norm: 773.6451568603516, minimum ratio: 0.76953125\n",
      "Epoch [1315], val_loss: 4273.9595\n",
      "gradient norm: 775.0827255249023, minimum ratio: 0.765625\n",
      "Epoch [1316], val_loss: 4285.5322\n",
      "gradient norm: 776.4912414550781, minimum ratio: 0.75\n",
      "Epoch [1317], val_loss: 4297.1274\n",
      "gradient norm: 777.9404678344727, minimum ratio: 0.77734375\n",
      "Epoch [1318], val_loss: 4308.7456\n",
      "gradient norm: 779.3913497924805, minimum ratio: 0.75\n",
      "Epoch [1319], val_loss: 4320.3853\n",
      "gradient norm: 780.8351669311523, minimum ratio: 0.7578125\n",
      "Epoch [1320], val_loss: 4332.0488\n",
      "gradient norm: 782.2784729003906, minimum ratio: 0.77734375\n",
      "Epoch [1321], val_loss: 4343.7344\n",
      "gradient norm: 783.7219772338867, minimum ratio: 0.7890625\n",
      "Epoch [1322], val_loss: 4355.4434\n",
      "gradient norm: 785.1676559448242, minimum ratio: 0.76953125\n",
      "Epoch [1323], val_loss: 4367.1743\n",
      "gradient norm: 786.6267700195312, minimum ratio: 0.76171875\n",
      "Epoch [1324], val_loss: 4378.9277\n",
      "gradient norm: 788.066535949707, minimum ratio: 0.765625\n",
      "Epoch [1325], val_loss: 4390.7061\n",
      "gradient norm: 789.5046844482422, minimum ratio: 0.765625\n",
      "Epoch [1326], val_loss: 4402.5049\n",
      "gradient norm: 790.9562606811523, minimum ratio: 0.7578125\n",
      "Epoch [1327], val_loss: 4414.3281\n",
      "gradient norm: 792.4094467163086, minimum ratio: 0.76953125\n",
      "Epoch [1328], val_loss: 4426.1743\n",
      "gradient norm: 793.8649826049805, minimum ratio: 0.76953125\n",
      "Epoch [1329], val_loss: 4438.0435\n",
      "gradient norm: 795.3061141967773, minimum ratio: 0.765625\n",
      "Epoch [1330], val_loss: 4449.9360\n",
      "gradient norm: 796.7770538330078, minimum ratio: 0.78515625\n",
      "Epoch [1331], val_loss: 4461.8511\n",
      "gradient norm: 798.2316741943359, minimum ratio: 0.77734375\n",
      "Epoch [1332], val_loss: 4473.7886\n",
      "gradient norm: 799.7058944702148, minimum ratio: 0.78125\n",
      "Epoch [1333], val_loss: 4485.7485\n",
      "gradient norm: 801.1692886352539, minimum ratio: 0.76171875\n",
      "Epoch [1334], val_loss: 4497.7324\n",
      "gradient norm: 802.6352005004883, minimum ratio: 0.76171875\n",
      "Epoch [1335], val_loss: 4509.7393\n",
      "gradient norm: 804.0727844238281, minimum ratio: 0.75\n",
      "Epoch [1336], val_loss: 4521.7700\n",
      "gradient norm: 805.5381164550781, minimum ratio: 0.74609375\n",
      "Epoch [1337], val_loss: 4533.8223\n",
      "gradient norm: 806.9935989379883, minimum ratio: 0.7734375\n",
      "Epoch [1338], val_loss: 4545.8975\n",
      "gradient norm: 808.4637908935547, minimum ratio: 0.7578125\n",
      "Epoch [1339], val_loss: 4557.9971\n",
      "gradient norm: 809.9160614013672, minimum ratio: 0.76171875\n",
      "Epoch [1340], val_loss: 4570.1182\n",
      "gradient norm: 811.378547668457, minimum ratio: 0.76953125\n",
      "Epoch [1341], val_loss: 4582.2627\n",
      "gradient norm: 812.8631286621094, minimum ratio: 0.7421875\n",
      "Epoch [1342], val_loss: 4594.4307\n",
      "gradient norm: 814.3385925292969, minimum ratio: 0.73828125\n",
      "Epoch [1343], val_loss: 4606.6235\n",
      "gradient norm: 815.7911682128906, minimum ratio: 0.76171875\n",
      "Epoch [1344], val_loss: 4618.8389\n",
      "gradient norm: 817.27490234375, minimum ratio: 0.7734375\n",
      "Epoch [1345], val_loss: 4631.0776\n",
      "gradient norm: 818.7609176635742, minimum ratio: 0.7578125\n",
      "Epoch [1346], val_loss: 4643.3408\n",
      "gradient norm: 820.2367858886719, minimum ratio: 0.76171875\n",
      "Epoch [1347], val_loss: 4655.6265\n",
      "gradient norm: 821.7355651855469, minimum ratio: 0.77734375\n",
      "Epoch [1348], val_loss: 4667.9351\n",
      "gradient norm: 823.2360153198242, minimum ratio: 0.76171875\n",
      "Epoch [1349], val_loss: 4680.2671\n",
      "gradient norm: 824.7118072509766, minimum ratio: 0.74609375\n",
      "Epoch [1350], val_loss: 4692.6240\n",
      "gradient norm: 826.2157669067383, minimum ratio: 0.76953125\n",
      "Epoch [1351], val_loss: 4705.0029\n",
      "gradient norm: 827.721321105957, minimum ratio: 0.765625\n",
      "Epoch [1352], val_loss: 4717.4053\n",
      "gradient norm: 829.2174224853516, minimum ratio: 0.796875\n",
      "Epoch [1353], val_loss: 4729.8325\n",
      "gradient norm: 830.7264862060547, minimum ratio: 0.765625\n",
      "Epoch [1354], val_loss: 4742.2812\n",
      "gradient norm: 832.2226867675781, minimum ratio: 0.80078125\n",
      "Epoch [1355], val_loss: 4754.7549\n",
      "gradient norm: 833.7238388061523, minimum ratio: 0.7578125\n",
      "Epoch [1356], val_loss: 4767.2515\n",
      "gradient norm: 835.2176895141602, minimum ratio: 0.76171875\n",
      "Epoch [1357], val_loss: 4779.7729\n",
      "gradient norm: 836.7108993530273, minimum ratio: 0.75\n",
      "Epoch [1358], val_loss: 4792.3169\n",
      "gradient norm: 838.198616027832, minimum ratio: 0.765625\n",
      "Epoch [1359], val_loss: 4804.8848\n",
      "gradient norm: 839.6909637451172, minimum ratio: 0.765625\n",
      "Epoch [1360], val_loss: 4817.4771\n",
      "gradient norm: 841.1767959594727, minimum ratio: 0.73828125\n",
      "Epoch [1361], val_loss: 4830.0923\n",
      "gradient norm: 842.6729736328125, minimum ratio: 0.74609375\n",
      "Epoch [1362], val_loss: 4842.7334\n",
      "gradient norm: 844.1970748901367, minimum ratio: 0.7734375\n",
      "Epoch [1363], val_loss: 4855.3965\n",
      "gradient norm: 845.7036743164062, minimum ratio: 0.7734375\n",
      "Epoch [1364], val_loss: 4868.0859\n",
      "gradient norm: 847.191291809082, minimum ratio: 0.7890625\n",
      "Epoch [1365], val_loss: 4880.7969\n",
      "gradient norm: 848.7202072143555, minimum ratio: 0.77734375\n",
      "Epoch [1366], val_loss: 4893.5317\n",
      "gradient norm: 850.2237777709961, minimum ratio: 0.78125\n",
      "Epoch [1367], val_loss: 4906.2910\n",
      "gradient norm: 851.7278213500977, minimum ratio: 0.796875\n",
      "Epoch [1368], val_loss: 4919.0732\n",
      "gradient norm: 853.2615966796875, minimum ratio: 0.7578125\n",
      "Epoch [1369], val_loss: 4931.8804\n",
      "gradient norm: 854.7452850341797, minimum ratio: 0.78515625\n",
      "Epoch [1370], val_loss: 4944.7104\n",
      "gradient norm: 856.2684860229492, minimum ratio: 0.7578125\n",
      "Epoch [1371], val_loss: 4957.5659\n",
      "gradient norm: 857.7974014282227, minimum ratio: 0.76171875\n",
      "Epoch [1372], val_loss: 4970.4448\n",
      "gradient norm: 859.3380279541016, minimum ratio: 0.765625\n",
      "Epoch [1373], val_loss: 4983.3481\n",
      "gradient norm: 860.8663101196289, minimum ratio: 0.75390625\n",
      "Epoch [1374], val_loss: 4996.2759\n",
      "gradient norm: 862.410514831543, minimum ratio: 0.7734375\n",
      "Epoch [1375], val_loss: 5009.2266\n",
      "gradient norm: 863.9430541992188, minimum ratio: 0.75390625\n",
      "Epoch [1376], val_loss: 5022.2031\n",
      "gradient norm: 865.4868774414062, minimum ratio: 0.77734375\n",
      "Epoch [1377], val_loss: 5035.2026\n",
      "gradient norm: 867.0360107421875, minimum ratio: 0.7421875\n",
      "Epoch [1378], val_loss: 5048.2275\n",
      "gradient norm: 868.5519180297852, minimum ratio: 0.74609375\n",
      "Epoch [1379], val_loss: 5061.2759\n",
      "gradient norm: 870.1045227050781, minimum ratio: 0.75\n",
      "Epoch [1380], val_loss: 5074.3481\n",
      "gradient norm: 871.6449279785156, minimum ratio: 0.7421875\n",
      "Epoch [1381], val_loss: 5087.4458\n",
      "gradient norm: 873.2010955810547, minimum ratio: 0.7578125\n",
      "Epoch [1382], val_loss: 5100.5674\n",
      "gradient norm: 874.7588119506836, minimum ratio: 0.7734375\n",
      "Epoch [1383], val_loss: 5113.7139\n",
      "gradient norm: 876.2826690673828, minimum ratio: 0.76953125\n",
      "Epoch [1384], val_loss: 5126.8833\n",
      "gradient norm: 877.8154449462891, minimum ratio: 0.76953125\n",
      "Epoch [1385], val_loss: 5140.0781\n",
      "gradient norm: 879.3781661987305, minimum ratio: 0.74609375\n",
      "Epoch [1386], val_loss: 5153.2964\n",
      "gradient norm: 880.9425048828125, minimum ratio: 0.7421875\n",
      "Epoch [1387], val_loss: 5166.5386\n",
      "gradient norm: 882.4949188232422, minimum ratio: 0.7578125\n",
      "Epoch [1388], val_loss: 5179.8062\n",
      "gradient norm: 884.0363006591797, minimum ratio: 0.7578125\n",
      "Epoch [1389], val_loss: 5193.0972\n",
      "gradient norm: 885.5702362060547, minimum ratio: 0.77734375\n",
      "Epoch [1390], val_loss: 5206.4150\n",
      "gradient norm: 887.1015625, minimum ratio: 0.74609375\n",
      "Epoch [1391], val_loss: 5219.7559\n",
      "gradient norm: 888.6517715454102, minimum ratio: 0.765625\n",
      "Epoch [1392], val_loss: 5233.1216\n",
      "gradient norm: 890.1925277709961, minimum ratio: 0.7734375\n",
      "Epoch [1393], val_loss: 5246.5122\n",
      "gradient norm: 891.7665405273438, minimum ratio: 0.765625\n",
      "Epoch [1394], val_loss: 5259.9277\n",
      "gradient norm: 893.3196640014648, minimum ratio: 0.7734375\n",
      "Epoch [1395], val_loss: 5273.3677\n",
      "gradient norm: 894.8993530273438, minimum ratio: 0.78125\n",
      "Epoch [1396], val_loss: 5286.8320\n",
      "gradient norm: 896.4469299316406, minimum ratio: 0.75390625\n",
      "Epoch [1397], val_loss: 5300.3223\n",
      "gradient norm: 898.0194625854492, minimum ratio: 0.765625\n",
      "Epoch [1398], val_loss: 5313.8364\n",
      "gradient norm: 899.5730819702148, minimum ratio: 0.7734375\n",
      "Epoch [1399], val_loss: 5327.3760\n",
      "gradient norm: 901.1353149414062, minimum ratio: 0.76171875\n",
      "Epoch [1400], val_loss: 5340.9390\n",
      "gradient norm: 902.7120819091797, minimum ratio: 0.76953125\n",
      "Epoch [1401], val_loss: 5354.5288\n",
      "gradient norm: 904.2714538574219, minimum ratio: 0.7578125\n",
      "Epoch [1402], val_loss: 5368.1426\n",
      "gradient norm: 905.8443984985352, minimum ratio: 0.75390625\n",
      "Epoch [1403], val_loss: 5381.7803\n",
      "gradient norm: 907.4226379394531, minimum ratio: 0.75390625\n",
      "Epoch [1404], val_loss: 5395.4448\n",
      "gradient norm: 908.9955978393555, minimum ratio: 0.734375\n",
      "Epoch [1405], val_loss: 5409.1338\n",
      "gradient norm: 910.5857620239258, minimum ratio: 0.76171875\n",
      "Epoch [1406], val_loss: 5422.8472\n",
      "gradient norm: 912.1841583251953, minimum ratio: 0.765625\n",
      "Epoch [1407], val_loss: 5436.5854\n",
      "gradient norm: 913.7582092285156, minimum ratio: 0.7421875\n",
      "Epoch [1408], val_loss: 5450.3501\n",
      "gradient norm: 915.3453674316406, minimum ratio: 0.76171875\n",
      "Epoch [1409], val_loss: 5464.1377\n",
      "gradient norm: 916.9274215698242, minimum ratio: 0.76171875\n",
      "Epoch [1410], val_loss: 5477.9502\n",
      "gradient norm: 918.5177383422852, minimum ratio: 0.74609375\n",
      "Epoch [1411], val_loss: 5491.7896\n",
      "gradient norm: 920.1107025146484, minimum ratio: 0.765625\n",
      "Epoch [1412], val_loss: 5505.6533\n",
      "gradient norm: 921.6714096069336, minimum ratio: 0.7578125\n",
      "Epoch [1413], val_loss: 5519.5439\n",
      "gradient norm: 923.2791137695312, minimum ratio: 0.7421875\n",
      "Epoch [1414], val_loss: 5533.4580\n",
      "gradient norm: 924.8754425048828, minimum ratio: 0.75390625\n",
      "Epoch [1415], val_loss: 5547.3984\n",
      "gradient norm: 926.4892807006836, minimum ratio: 0.7421875\n",
      "Epoch [1416], val_loss: 5561.3628\n",
      "gradient norm: 928.0829315185547, minimum ratio: 0.76171875\n",
      "Epoch [1417], val_loss: 5575.3540\n",
      "gradient norm: 929.695426940918, minimum ratio: 0.76171875\n",
      "Epoch [1418], val_loss: 5589.3701\n",
      "gradient norm: 931.3144836425781, minimum ratio: 0.7578125\n",
      "Epoch [1419], val_loss: 5603.4106\n",
      "gradient norm: 932.9036560058594, minimum ratio: 0.7578125\n",
      "Epoch [1420], val_loss: 5617.4771\n",
      "gradient norm: 934.5259552001953, minimum ratio: 0.7578125\n",
      "Epoch [1421], val_loss: 5631.5674\n",
      "gradient norm: 936.1335830688477, minimum ratio: 0.76171875\n",
      "Epoch [1422], val_loss: 5645.6841\n",
      "gradient norm: 937.7592086791992, minimum ratio: 0.76953125\n",
      "Epoch [1423], val_loss: 5659.8252\n",
      "gradient norm: 939.3504257202148, minimum ratio: 0.76953125\n",
      "Epoch [1424], val_loss: 5673.9912\n",
      "gradient norm: 940.950553894043, minimum ratio: 0.75\n",
      "Epoch [1425], val_loss: 5688.1846\n",
      "gradient norm: 942.5613174438477, minimum ratio: 0.7578125\n",
      "Epoch [1426], val_loss: 5702.4023\n",
      "gradient norm: 944.1822204589844, minimum ratio: 0.765625\n",
      "Epoch [1427], val_loss: 5716.6465\n",
      "gradient norm: 945.8010101318359, minimum ratio: 0.74609375\n",
      "Epoch [1428], val_loss: 5730.9136\n",
      "gradient norm: 947.4057998657227, minimum ratio: 0.76953125\n",
      "Epoch [1429], val_loss: 5745.2085\n",
      "gradient norm: 949.0344619750977, minimum ratio: 0.76171875\n",
      "Epoch [1430], val_loss: 5759.5283\n",
      "gradient norm: 950.6736755371094, minimum ratio: 0.75\n",
      "Epoch [1431], val_loss: 5773.8735\n",
      "gradient norm: 952.2837677001953, minimum ratio: 0.7578125\n",
      "Epoch [1432], val_loss: 5788.2456\n",
      "gradient norm: 953.9265060424805, minimum ratio: 0.76953125\n",
      "Epoch [1433], val_loss: 5802.6421\n",
      "gradient norm: 955.5252075195312, minimum ratio: 0.75390625\n",
      "Epoch [1434], val_loss: 5817.0659\n",
      "gradient norm: 957.1679992675781, minimum ratio: 0.73828125\n",
      "Epoch [1435], val_loss: 5831.5142\n",
      "gradient norm: 958.7633666992188, minimum ratio: 0.734375\n",
      "Epoch [1436], val_loss: 5845.9902\n",
      "gradient norm: 960.3339233398438, minimum ratio: 0.76953125\n",
      "Epoch [1437], val_loss: 5860.4902\n",
      "gradient norm: 961.9573135375977, minimum ratio: 0.76171875\n",
      "Epoch [1438], val_loss: 5875.0176\n",
      "gradient norm: 963.6046981811523, minimum ratio: 0.77734375\n",
      "Epoch [1439], val_loss: 5889.5703\n",
      "gradient norm: 965.2490310668945, minimum ratio: 0.76171875\n",
      "Epoch [1440], val_loss: 5904.1484\n",
      "gradient norm: 966.9052352905273, minimum ratio: 0.76953125\n",
      "Epoch [1441], val_loss: 5918.7520\n",
      "gradient norm: 968.563346862793, minimum ratio: 0.76171875\n",
      "Epoch [1442], val_loss: 5933.3823\n",
      "gradient norm: 970.2230911254883, minimum ratio: 0.76953125\n",
      "Epoch [1443], val_loss: 5948.0396\n",
      "gradient norm: 971.8673248291016, minimum ratio: 0.7734375\n",
      "Epoch [1444], val_loss: 5962.7207\n",
      "gradient norm: 973.4993667602539, minimum ratio: 0.76171875\n",
      "Epoch [1445], val_loss: 5977.4277\n",
      "gradient norm: 975.1520004272461, minimum ratio: 0.76171875\n",
      "Epoch [1446], val_loss: 5992.1626\n",
      "gradient norm: 976.7931594848633, minimum ratio: 0.765625\n",
      "Epoch [1447], val_loss: 6006.9224\n",
      "gradient norm: 978.433219909668, minimum ratio: 0.75\n",
      "Epoch [1448], val_loss: 6021.7095\n",
      "gradient norm: 980.0864944458008, minimum ratio: 0.76953125\n",
      "Epoch [1449], val_loss: 6036.5220\n",
      "gradient norm: 981.7584457397461, minimum ratio: 0.76171875\n",
      "Epoch [1450], val_loss: 6051.3618\n",
      "gradient norm: 983.4158096313477, minimum ratio: 0.7578125\n",
      "Epoch [1451], val_loss: 6066.2261\n",
      "gradient norm: 985.0869216918945, minimum ratio: 0.76171875\n",
      "Epoch [1452], val_loss: 6081.1162\n",
      "gradient norm: 986.7536163330078, minimum ratio: 0.78125\n",
      "Epoch [1453], val_loss: 6096.0347\n",
      "gradient norm: 988.420036315918, minimum ratio: 0.77734375\n",
      "Epoch [1454], val_loss: 6110.9775\n",
      "gradient norm: 990.0808486938477, minimum ratio: 0.75390625\n",
      "Epoch [1455], val_loss: 6125.9482\n",
      "gradient norm: 991.7539749145508, minimum ratio: 0.76171875\n",
      "Epoch [1456], val_loss: 6140.9434\n",
      "gradient norm: 993.4073104858398, minimum ratio: 0.7890625\n",
      "Epoch [1457], val_loss: 6155.9663\n",
      "gradient norm: 995.0600051879883, minimum ratio: 0.73828125\n",
      "Epoch [1458], val_loss: 6171.0146\n",
      "gradient norm: 996.6990051269531, minimum ratio: 0.73046875\n",
      "Epoch [1459], val_loss: 6186.0894\n",
      "gradient norm: 998.3790283203125, minimum ratio: 0.76953125\n",
      "Epoch [1460], val_loss: 6201.1919\n",
      "gradient norm: 1000.046142578125, minimum ratio: 0.73828125\n",
      "Epoch [1461], val_loss: 6216.3198\n",
      "gradient norm: 1001.7387619018555, minimum ratio: 0.7734375\n",
      "Epoch [1462], val_loss: 6231.4756\n",
      "gradient norm: 1003.385368347168, minimum ratio: 0.75390625\n",
      "Epoch [1463], val_loss: 6246.6553\n",
      "gradient norm: 1005.0610427856445, minimum ratio: 0.75390625\n",
      "Epoch [1464], val_loss: 6261.8652\n",
      "gradient norm: 1006.7590179443359, minimum ratio: 0.78515625\n",
      "Epoch [1465], val_loss: 6277.0981\n",
      "gradient norm: 1008.4416046142578, minimum ratio: 0.75\n",
      "Epoch [1466], val_loss: 6292.3584\n",
      "gradient norm: 1010.1170196533203, minimum ratio: 0.75390625\n",
      "Epoch [1467], val_loss: 6307.6460\n",
      "gradient norm: 1011.8127136230469, minimum ratio: 0.76171875\n",
      "Epoch [1468], val_loss: 6322.9600\n",
      "gradient norm: 1013.5175857543945, minimum ratio: 0.7578125\n",
      "Epoch [1469], val_loss: 6338.3013\n",
      "gradient norm: 1015.1996078491211, minimum ratio: 0.73828125\n",
      "Epoch [1470], val_loss: 6353.6680\n",
      "gradient norm: 1016.8639984130859, minimum ratio: 0.7578125\n",
      "Epoch [1471], val_loss: 6369.0635\n",
      "gradient norm: 1018.5416641235352, minimum ratio: 0.77734375\n",
      "Epoch [1472], val_loss: 6384.4854\n",
      "gradient norm: 1020.229736328125, minimum ratio: 0.765625\n",
      "Epoch [1473], val_loss: 6399.9336\n",
      "gradient norm: 1021.9118041992188, minimum ratio: 0.77734375\n",
      "Epoch [1474], val_loss: 6415.4106\n",
      "gradient norm: 1023.5854721069336, minimum ratio: 0.74609375\n",
      "Epoch [1475], val_loss: 6430.9126\n",
      "gradient norm: 1025.3027801513672, minimum ratio: 0.74609375\n",
      "Epoch [1476], val_loss: 6446.4434\n",
      "gradient norm: 1027.0173263549805, minimum ratio: 0.7421875\n",
      "Epoch [1477], val_loss: 6462.0000\n",
      "gradient norm: 1028.7030029296875, minimum ratio: 0.76953125\n",
      "Epoch [1478], val_loss: 6477.5840\n",
      "gradient norm: 1030.4256210327148, minimum ratio: 0.7578125\n",
      "Epoch [1479], val_loss: 6493.1934\n",
      "gradient norm: 1032.141975402832, minimum ratio: 0.78125\n",
      "Epoch [1480], val_loss: 6508.8311\n",
      "gradient norm: 1033.8678741455078, minimum ratio: 0.7578125\n",
      "Epoch [1481], val_loss: 6524.4946\n",
      "gradient norm: 1035.5623397827148, minimum ratio: 0.7421875\n",
      "Epoch [1482], val_loss: 6540.1855\n",
      "gradient norm: 1037.2543182373047, minimum ratio: 0.75390625\n",
      "Epoch [1483], val_loss: 6555.9043\n",
      "gradient norm: 1038.9856338500977, minimum ratio: 0.76171875\n",
      "Epoch [1484], val_loss: 6571.6484\n",
      "gradient norm: 1040.7151184082031, minimum ratio: 0.73828125\n",
      "Epoch [1485], val_loss: 6587.4219\n",
      "gradient norm: 1042.3976745605469, minimum ratio: 0.7890625\n",
      "Epoch [1486], val_loss: 6603.2217\n",
      "gradient norm: 1044.1206436157227, minimum ratio: 0.7734375\n",
      "Epoch [1487], val_loss: 6619.0479\n",
      "gradient norm: 1045.85888671875, minimum ratio: 0.76171875\n",
      "Epoch [1488], val_loss: 6634.9019\n",
      "gradient norm: 1047.5989151000977, minimum ratio: 0.76953125\n",
      "Epoch [1489], val_loss: 6650.7827\n",
      "gradient norm: 1049.272560119629, minimum ratio: 0.74609375\n",
      "Epoch [1490], val_loss: 6666.6919\n",
      "gradient norm: 1051.0105285644531, minimum ratio: 0.765625\n",
      "Epoch [1491], val_loss: 6682.6265\n",
      "gradient norm: 1052.7379989624023, minimum ratio: 0.7578125\n",
      "Epoch [1492], val_loss: 6698.5894\n",
      "gradient norm: 1054.4564361572266, minimum ratio: 0.7578125\n",
      "Epoch [1493], val_loss: 6714.5796\n",
      "gradient norm: 1056.1836318969727, minimum ratio: 0.76171875\n",
      "Epoch [1494], val_loss: 6730.5962\n",
      "gradient norm: 1057.9048690795898, minimum ratio: 0.75390625\n",
      "Epoch [1495], val_loss: 6746.6411\n",
      "gradient norm: 1059.6424560546875, minimum ratio: 0.7578125\n",
      "Epoch [1496], val_loss: 6762.7144\n",
      "gradient norm: 1061.366081237793, minimum ratio: 0.76953125\n",
      "Epoch [1497], val_loss: 6778.8164\n",
      "gradient norm: 1063.0842666625977, minimum ratio: 0.76171875\n",
      "Epoch [1498], val_loss: 6794.9434\n",
      "gradient norm: 1064.841911315918, minimum ratio: 0.76953125\n",
      "Epoch [1499], val_loss: 6811.0996\n",
      "gradient norm: 1066.5830841064453, minimum ratio: 0.76953125\n",
      "Epoch [1500], val_loss: 6827.2822\n",
      "gradient norm: 1068.344108581543, minimum ratio: 0.76953125\n",
      "Epoch [1501], val_loss: 6843.4927\n",
      "gradient norm: 1070.106918334961, minimum ratio: 0.76953125\n",
      "Epoch [1502], val_loss: 6859.7305\n",
      "gradient norm: 1071.8716354370117, minimum ratio: 0.765625\n",
      "Epoch [1503], val_loss: 6875.9956\n",
      "gradient norm: 1073.604263305664, minimum ratio: 0.76171875\n",
      "Epoch [1504], val_loss: 6892.2900\n",
      "gradient norm: 1075.3516082763672, minimum ratio: 0.7734375\n",
      "Epoch [1505], val_loss: 6908.6104\n",
      "gradient norm: 1077.094383239746, minimum ratio: 0.75\n",
      "Epoch [1506], val_loss: 6924.9604\n",
      "gradient norm: 1078.7894287109375, minimum ratio: 0.76953125\n",
      "Epoch [1507], val_loss: 6941.3364\n",
      "gradient norm: 1080.535743713379, minimum ratio: 0.7734375\n",
      "Epoch [1508], val_loss: 6957.7412\n",
      "gradient norm: 1082.2775192260742, minimum ratio: 0.765625\n",
      "Epoch [1509], val_loss: 6974.1743\n",
      "gradient norm: 1084.0543518066406, minimum ratio: 0.76171875\n",
      "Epoch [1510], val_loss: 6990.6323\n",
      "gradient norm: 1085.7549591064453, minimum ratio: 0.76953125\n",
      "Epoch [1511], val_loss: 7007.1206\n",
      "gradient norm: 1087.5351943969727, minimum ratio: 0.76171875\n",
      "Epoch [1512], val_loss: 7023.6357\n",
      "gradient norm: 1089.2959594726562, minimum ratio: 0.75\n",
      "Epoch [1513], val_loss: 7040.1792\n",
      "gradient norm: 1091.0795211791992, minimum ratio: 0.78125\n",
      "Epoch [1514], val_loss: 7056.7485\n",
      "gradient norm: 1092.861213684082, minimum ratio: 0.76171875\n",
      "Epoch [1515], val_loss: 7073.3477\n",
      "gradient norm: 1094.6369094848633, minimum ratio: 0.78125\n",
      "Epoch [1516], val_loss: 7089.9731\n",
      "gradient norm: 1096.3922119140625, minimum ratio: 0.74609375\n",
      "Epoch [1517], val_loss: 7106.6274\n",
      "gradient norm: 1098.1602630615234, minimum ratio: 0.765625\n",
      "Epoch [1518], val_loss: 7123.3110\n",
      "gradient norm: 1099.9431838989258, minimum ratio: 0.76953125\n",
      "Epoch [1519], val_loss: 7140.0210\n",
      "gradient norm: 1101.7062301635742, minimum ratio: 0.765625\n",
      "Epoch [1520], val_loss: 7156.7598\n",
      "gradient norm: 1103.5022277832031, minimum ratio: 0.765625\n",
      "Epoch [1521], val_loss: 7173.5259\n",
      "gradient norm: 1105.2645263671875, minimum ratio: 0.78125\n",
      "Epoch [1522], val_loss: 7190.3208\n",
      "gradient norm: 1107.0360412597656, minimum ratio: 0.7578125\n",
      "Epoch [1523], val_loss: 7207.1436\n",
      "gradient norm: 1108.8371353149414, minimum ratio: 0.76953125\n",
      "Epoch [1524], val_loss: 7223.9932\n",
      "gradient norm: 1110.6206436157227, minimum ratio: 0.765625\n",
      "Epoch [1525], val_loss: 7240.8740\n",
      "gradient norm: 1112.416015625, minimum ratio: 0.7890625\n",
      "Epoch [1526], val_loss: 7257.7817\n",
      "gradient norm: 1114.2038345336914, minimum ratio: 0.77734375\n",
      "Epoch [1527], val_loss: 7274.7173\n",
      "gradient norm: 1115.984733581543, minimum ratio: 0.76171875\n",
      "Epoch [1528], val_loss: 7291.6807\n",
      "gradient norm: 1117.794822692871, minimum ratio: 0.76171875\n",
      "Epoch [1529], val_loss: 7308.6748\n",
      "gradient norm: 1119.5898742675781, minimum ratio: 0.765625\n",
      "Epoch [1530], val_loss: 7325.6948\n",
      "gradient norm: 1121.3774871826172, minimum ratio: 0.7578125\n",
      "Epoch [1531], val_loss: 7342.7441\n",
      "gradient norm: 1123.1586380004883, minimum ratio: 0.765625\n",
      "Epoch [1532], val_loss: 7359.8232\n",
      "gradient norm: 1124.9420852661133, minimum ratio: 0.765625\n",
      "Epoch [1533], val_loss: 7376.9302\n",
      "gradient norm: 1126.7391738891602, minimum ratio: 0.76953125\n",
      "Epoch [1534], val_loss: 7394.0669\n",
      "gradient norm: 1128.5440063476562, minimum ratio: 0.77734375\n",
      "Epoch [1535], val_loss: 7411.2290\n",
      "gradient norm: 1130.3527603149414, minimum ratio: 0.7734375\n",
      "Epoch [1536], val_loss: 7428.4243\n",
      "gradient norm: 1132.1338119506836, minimum ratio: 0.75390625\n",
      "Epoch [1537], val_loss: 7445.6460\n",
      "gradient norm: 1133.9453201293945, minimum ratio: 0.76953125\n",
      "Epoch [1538], val_loss: 7462.8960\n",
      "gradient norm: 1135.7306823730469, minimum ratio: 0.75\n",
      "Epoch [1539], val_loss: 7480.1758\n",
      "gradient norm: 1137.544807434082, minimum ratio: 0.78125\n",
      "Epoch [1540], val_loss: 7497.4814\n",
      "gradient norm: 1139.3703384399414, minimum ratio: 0.765625\n",
      "Epoch [1541], val_loss: 7514.8174\n",
      "gradient norm: 1141.162582397461, minimum ratio: 0.7734375\n",
      "Epoch [1542], val_loss: 7532.1816\n",
      "gradient norm: 1142.9705047607422, minimum ratio: 0.78515625\n",
      "Epoch [1543], val_loss: 7549.5752\n",
      "gradient norm: 1144.807632446289, minimum ratio: 0.76953125\n",
      "Epoch [1544], val_loss: 7566.9976\n",
      "gradient norm: 1146.6130142211914, minimum ratio: 0.78125\n",
      "Epoch [1545], val_loss: 7584.4468\n",
      "gradient norm: 1148.3831939697266, minimum ratio: 0.765625\n",
      "Epoch [1546], val_loss: 7601.9292\n",
      "gradient norm: 1150.183853149414, minimum ratio: 0.765625\n",
      "Epoch [1547], val_loss: 7619.4385\n",
      "gradient norm: 1151.9861221313477, minimum ratio: 0.7578125\n",
      "Epoch [1548], val_loss: 7636.9766\n",
      "gradient norm: 1153.8320083618164, minimum ratio: 0.73828125\n",
      "Epoch [1549], val_loss: 7654.5435\n",
      "gradient norm: 1155.6794357299805, minimum ratio: 0.77734375\n",
      "Epoch [1550], val_loss: 7672.1401\n",
      "gradient norm: 1157.503044128418, minimum ratio: 0.75\n",
      "Epoch [1551], val_loss: 7689.7651\n",
      "gradient norm: 1159.3316192626953, minimum ratio: 0.78125\n",
      "Epoch [1552], val_loss: 7707.4185\n",
      "gradient norm: 1161.1844940185547, minimum ratio: 0.75390625\n",
      "Epoch [1553], val_loss: 7725.1001\n",
      "gradient norm: 1163.0392150878906, minimum ratio: 0.76953125\n",
      "Epoch [1554], val_loss: 7742.8115\n",
      "gradient norm: 1164.8935546875, minimum ratio: 0.75390625\n",
      "Epoch [1555], val_loss: 7760.5518\n",
      "gradient norm: 1166.7310028076172, minimum ratio: 0.75\n",
      "Epoch [1556], val_loss: 7778.3193\n",
      "gradient norm: 1168.590591430664, minimum ratio: 0.76953125\n",
      "Epoch [1557], val_loss: 7796.1177\n",
      "gradient norm: 1170.4125442504883, minimum ratio: 0.78125\n",
      "Epoch [1558], val_loss: 7813.9443\n",
      "gradient norm: 1172.2763214111328, minimum ratio: 0.7421875\n",
      "Epoch [1559], val_loss: 7831.8008\n",
      "gradient norm: 1174.1182403564453, minimum ratio: 0.76953125\n",
      "Epoch [1560], val_loss: 7849.6865\n",
      "gradient norm: 1175.9616775512695, minimum ratio: 0.78515625\n",
      "Epoch [1561], val_loss: 7867.5981\n",
      "gradient norm: 1177.8170928955078, minimum ratio: 0.765625\n",
      "Epoch [1562], val_loss: 7885.5425\n",
      "gradient norm: 1179.6099395751953, minimum ratio: 0.77734375\n",
      "Epoch [1563], val_loss: 7903.5142\n",
      "gradient norm: 1181.4515991210938, minimum ratio: 0.76953125\n",
      "Epoch [1564], val_loss: 7921.5166\n",
      "gradient norm: 1183.3169479370117, minimum ratio: 0.765625\n",
      "Epoch [1565], val_loss: 7939.5469\n",
      "gradient norm: 1185.1626052856445, minimum ratio: 0.765625\n",
      "Epoch [1566], val_loss: 7957.6064\n",
      "gradient norm: 1186.9615783691406, minimum ratio: 0.76171875\n",
      "Epoch [1567], val_loss: 7975.6982\n",
      "gradient norm: 1188.8408966064453, minimum ratio: 0.7578125\n",
      "Epoch [1568], val_loss: 7993.8174\n",
      "gradient norm: 1190.7221603393555, minimum ratio: 0.75\n",
      "Epoch [1569], val_loss: 8011.9683\n",
      "gradient norm: 1192.5534744262695, minimum ratio: 0.76171875\n",
      "Epoch [1570], val_loss: 8030.1460\n",
      "gradient norm: 1194.4163436889648, minimum ratio: 0.7578125\n",
      "Epoch [1571], val_loss: 8048.3564\n",
      "gradient norm: 1196.280776977539, minimum ratio: 0.765625\n",
      "Epoch [1572], val_loss: 8066.5952\n",
      "gradient norm: 1198.1394577026367, minimum ratio: 0.76171875\n",
      "Epoch [1573], val_loss: 8084.8618\n",
      "gradient norm: 1200.0021209716797, minimum ratio: 0.765625\n",
      "Epoch [1574], val_loss: 8103.1602\n",
      "gradient norm: 1201.8674087524414, minimum ratio: 0.76171875\n",
      "Epoch [1575], val_loss: 8121.4873\n",
      "gradient norm: 1203.7614212036133, minimum ratio: 0.765625\n",
      "Epoch [1576], val_loss: 8139.8467\n",
      "gradient norm: 1205.6394958496094, minimum ratio: 0.75390625\n",
      "Epoch [1577], val_loss: 8158.2344\n",
      "gradient norm: 1207.5157852172852, minimum ratio: 0.78125\n",
      "Epoch [1578], val_loss: 8176.6523\n",
      "gradient norm: 1209.4155044555664, minimum ratio: 0.7734375\n",
      "Epoch [1579], val_loss: 8195.0986\n",
      "gradient norm: 1211.2965927124023, minimum ratio: 0.7421875\n",
      "Epoch [1580], val_loss: 8213.5762\n",
      "gradient norm: 1213.1830139160156, minimum ratio: 0.76953125\n",
      "Epoch [1581], val_loss: 8232.0820\n",
      "gradient norm: 1215.088119506836, minimum ratio: 0.78515625\n",
      "Epoch [1582], val_loss: 8250.6172\n",
      "gradient norm: 1216.978775024414, minimum ratio: 0.7578125\n",
      "Epoch [1583], val_loss: 8269.1846\n",
      "gradient norm: 1218.8611373901367, minimum ratio: 0.75\n",
      "Epoch [1584], val_loss: 8287.7803\n",
      "gradient norm: 1220.732322692871, minimum ratio: 0.74609375\n",
      "Epoch [1585], val_loss: 8306.4053\n",
      "gradient norm: 1222.619125366211, minimum ratio: 0.765625\n",
      "Epoch [1586], val_loss: 8325.0596\n",
      "gradient norm: 1224.4816665649414, minimum ratio: 0.76953125\n",
      "Epoch [1587], val_loss: 8343.7451\n",
      "gradient norm: 1226.372413635254, minimum ratio: 0.75390625\n",
      "Epoch [1588], val_loss: 8362.4609\n",
      "gradient norm: 1228.2612915039062, minimum ratio: 0.7578125\n",
      "Epoch [1589], val_loss: 8381.2070\n",
      "gradient norm: 1230.180648803711, minimum ratio: 0.7734375\n",
      "Epoch [1590], val_loss: 8399.9824\n",
      "gradient norm: 1232.060806274414, minimum ratio: 0.76953125\n",
      "Epoch [1591], val_loss: 8418.7881\n",
      "gradient norm: 1233.983642578125, minimum ratio: 0.7578125\n",
      "Epoch [1592], val_loss: 8437.6240\n",
      "gradient norm: 1235.8900527954102, minimum ratio: 0.78125\n",
      "Epoch [1593], val_loss: 8456.4893\n",
      "gradient norm: 1237.7935409545898, minimum ratio: 0.74609375\n",
      "Epoch [1594], val_loss: 8475.3857\n",
      "gradient norm: 1239.7012786865234, minimum ratio: 0.7578125\n",
      "Epoch [1595], val_loss: 8494.3135\n",
      "gradient norm: 1241.5784149169922, minimum ratio: 0.765625\n",
      "Epoch [1596], val_loss: 8513.2705\n",
      "gradient norm: 1243.5011825561523, minimum ratio: 0.7734375\n",
      "Epoch [1597], val_loss: 8532.2588\n",
      "gradient norm: 1245.4005813598633, minimum ratio: 0.73828125\n",
      "Epoch [1598], val_loss: 8551.2754\n",
      "gradient norm: 1247.2968139648438, minimum ratio: 0.7578125\n",
      "Epoch [1599], val_loss: 8570.3262\n",
      "gradient norm: 1249.2119903564453, minimum ratio: 0.75390625\n",
      "Epoch [1600], val_loss: 8589.4033\n",
      "gradient norm: 1251.132942199707, minimum ratio: 0.76171875\n",
      "Epoch [1601], val_loss: 8608.5127\n",
      "gradient norm: 1253.0098876953125, minimum ratio: 0.75390625\n",
      "Epoch [1602], val_loss: 8627.6523\n",
      "gradient norm: 1254.9300231933594, minimum ratio: 0.7734375\n",
      "Epoch [1603], val_loss: 8646.8213\n",
      "gradient norm: 1256.874740600586, minimum ratio: 0.7578125\n",
      "Epoch [1604], val_loss: 8666.0225\n",
      "gradient norm: 1258.7650985717773, minimum ratio: 0.75390625\n",
      "Epoch [1605], val_loss: 8685.2529\n",
      "gradient norm: 1260.7133483886719, minimum ratio: 0.78515625\n",
      "Epoch [1606], val_loss: 8704.5127\n",
      "gradient norm: 1262.6539688110352, minimum ratio: 0.7578125\n",
      "Epoch [1607], val_loss: 8723.8027\n",
      "gradient norm: 1264.567756652832, minimum ratio: 0.76953125\n",
      "Epoch [1608], val_loss: 8743.1221\n",
      "gradient norm: 1266.5124816894531, minimum ratio: 0.79296875\n",
      "Epoch [1609], val_loss: 8762.4746\n",
      "gradient norm: 1268.445167541504, minimum ratio: 0.77734375\n",
      "Epoch [1610], val_loss: 8781.8555\n",
      "gradient norm: 1270.3583984375, minimum ratio: 0.76953125\n",
      "Epoch [1611], val_loss: 8801.2686\n",
      "gradient norm: 1272.2903747558594, minimum ratio: 0.73828125\n",
      "Epoch [1612], val_loss: 8820.7109\n",
      "gradient norm: 1274.232421875, minimum ratio: 0.7734375\n",
      "Epoch [1613], val_loss: 8840.1885\n",
      "gradient norm: 1276.1772003173828, minimum ratio: 0.77734375\n",
      "Epoch [1614], val_loss: 8859.6934\n",
      "gradient norm: 1278.1288375854492, minimum ratio: 0.7734375\n",
      "Epoch [1615], val_loss: 8879.2285\n",
      "gradient norm: 1280.0359344482422, minimum ratio: 0.74609375\n",
      "Epoch [1616], val_loss: 8898.7988\n",
      "gradient norm: 1282.0042343139648, minimum ratio: 0.7578125\n",
      "Epoch [1617], val_loss: 8918.3975\n",
      "gradient norm: 1283.9528579711914, minimum ratio: 0.7578125\n",
      "Epoch [1618], val_loss: 8938.0283\n",
      "gradient norm: 1285.8989944458008, minimum ratio: 0.78125\n",
      "Epoch [1619], val_loss: 8957.6885\n",
      "gradient norm: 1287.823974609375, minimum ratio: 0.77734375\n",
      "Epoch [1620], val_loss: 8977.3809\n",
      "gradient norm: 1289.799446105957, minimum ratio: 0.75\n",
      "Epoch [1621], val_loss: 8997.1016\n",
      "gradient norm: 1291.7648468017578, minimum ratio: 0.765625\n",
      "Epoch [1622], val_loss: 9016.8545\n",
      "gradient norm: 1293.7279357910156, minimum ratio: 0.76953125\n",
      "Epoch [1623], val_loss: 9036.6396\n",
      "gradient norm: 1295.675666809082, minimum ratio: 0.7421875\n",
      "Epoch [1624], val_loss: 9056.4561\n",
      "gradient norm: 1297.5881652832031, minimum ratio: 0.76171875\n",
      "Epoch [1625], val_loss: 9076.3037\n",
      "gradient norm: 1299.5575485229492, minimum ratio: 0.76171875\n",
      "Epoch [1626], val_loss: 9096.1797\n",
      "gradient norm: 1301.5440063476562, minimum ratio: 0.75\n",
      "Epoch [1627], val_loss: 9116.0889\n",
      "gradient norm: 1303.5321044921875, minimum ratio: 0.77734375\n",
      "Epoch [1628], val_loss: 9136.0273\n",
      "gradient norm: 1305.522232055664, minimum ratio: 0.76953125\n",
      "Epoch [1629], val_loss: 9155.9971\n",
      "gradient norm: 1307.487678527832, minimum ratio: 0.76953125\n",
      "Epoch [1630], val_loss: 9175.9980\n",
      "gradient norm: 1309.4820404052734, minimum ratio: 0.75\n",
      "Epoch [1631], val_loss: 9196.0303\n",
      "gradient norm: 1311.4098358154297, minimum ratio: 0.734375\n",
      "Epoch [1632], val_loss: 9216.0938\n",
      "gradient norm: 1313.3873901367188, minimum ratio: 0.75\n",
      "Epoch [1633], val_loss: 9236.1875\n",
      "gradient norm: 1315.356948852539, minimum ratio: 0.7578125\n",
      "Epoch [1634], val_loss: 9256.3154\n",
      "gradient norm: 1317.2859115600586, minimum ratio: 0.75\n",
      "Epoch [1635], val_loss: 9276.4717\n",
      "gradient norm: 1319.2493591308594, minimum ratio: 0.78125\n",
      "Epoch [1636], val_loss: 9296.6621\n",
      "gradient norm: 1321.2539291381836, minimum ratio: 0.75\n",
      "Epoch [1637], val_loss: 9316.8838\n",
      "gradient norm: 1323.1587753295898, minimum ratio: 0.7578125\n",
      "Epoch [1638], val_loss: 9337.1348\n",
      "gradient norm: 1325.1562881469727, minimum ratio: 0.74609375\n",
      "Epoch [1639], val_loss: 9357.4219\n",
      "gradient norm: 1327.1580047607422, minimum ratio: 0.7578125\n",
      "Epoch [1640], val_loss: 9377.7373\n",
      "gradient norm: 1329.145523071289, minimum ratio: 0.76953125\n",
      "Epoch [1641], val_loss: 9398.0859\n",
      "gradient norm: 1331.1593704223633, minimum ratio: 0.75390625\n",
      "Epoch [1642], val_loss: 9418.4648\n",
      "gradient norm: 1333.175148010254, minimum ratio: 0.765625\n",
      "Epoch [1643], val_loss: 9438.8750\n",
      "gradient norm: 1335.1525115966797, minimum ratio: 0.73046875\n",
      "Epoch [1644], val_loss: 9459.3174\n",
      "gradient norm: 1337.1436080932617, minimum ratio: 0.7890625\n",
      "Epoch [1645], val_loss: 9479.7900\n",
      "gradient norm: 1339.138282775879, minimum ratio: 0.75390625\n",
      "Epoch [1646], val_loss: 9500.2979\n",
      "gradient norm: 1341.0930252075195, minimum ratio: 0.75\n",
      "Epoch [1647], val_loss: 9520.8340\n",
      "gradient norm: 1343.1178741455078, minimum ratio: 0.765625\n",
      "Epoch [1648], val_loss: 9541.4053\n",
      "gradient norm: 1345.1416702270508, minimum ratio: 0.74609375\n",
      "Epoch [1649], val_loss: 9562.0049\n",
      "gradient norm: 1347.153190612793, minimum ratio: 0.7578125\n",
      "Epoch [1650], val_loss: 9582.6396\n",
      "gradient norm: 1349.183609008789, minimum ratio: 0.75390625\n",
      "Epoch [1651], val_loss: 9603.3037\n",
      "gradient norm: 1351.1775741577148, minimum ratio: 0.76171875\n",
      "Epoch [1652], val_loss: 9624.0020\n",
      "gradient norm: 1353.1867599487305, minimum ratio: 0.74609375\n",
      "Epoch [1653], val_loss: 9644.7324\n",
      "gradient norm: 1355.2231750488281, minimum ratio: 0.76953125\n",
      "Epoch [1654], val_loss: 9665.4922\n",
      "gradient norm: 1357.2062759399414, minimum ratio: 0.74609375\n",
      "Epoch [1655], val_loss: 9686.2881\n",
      "gradient norm: 1359.2464599609375, minimum ratio: 0.7578125\n",
      "Epoch [1656], val_loss: 9707.1123\n",
      "gradient norm: 1361.2390975952148, minimum ratio: 0.74609375\n",
      "Epoch [1657], val_loss: 9727.9697\n",
      "gradient norm: 1363.2497253417969, minimum ratio: 0.765625\n",
      "Epoch [1658], val_loss: 9748.8555\n",
      "gradient norm: 1365.2858505249023, minimum ratio: 0.74609375\n",
      "Epoch [1659], val_loss: 9769.7754\n",
      "gradient norm: 1367.3126220703125, minimum ratio: 0.7890625\n",
      "Epoch [1660], val_loss: 9790.7275\n",
      "gradient norm: 1369.3318634033203, minimum ratio: 0.77734375\n",
      "Epoch [1661], val_loss: 9811.7129\n",
      "gradient norm: 1371.3459014892578, minimum ratio: 0.7578125\n",
      "Epoch [1662], val_loss: 9832.7305\n",
      "gradient norm: 1373.3780212402344, minimum ratio: 0.765625\n",
      "Epoch [1663], val_loss: 9853.7783\n",
      "gradient norm: 1375.357048034668, minimum ratio: 0.78125\n",
      "Epoch [1664], val_loss: 9874.8604\n",
      "gradient norm: 1377.3946228027344, minimum ratio: 0.765625\n",
      "Epoch [1665], val_loss: 9895.9736\n",
      "gradient norm: 1379.3994445800781, minimum ratio: 0.7578125\n",
      "Epoch [1666], val_loss: 9917.1201\n",
      "gradient norm: 1381.4081802368164, minimum ratio: 0.7578125\n",
      "Epoch [1667], val_loss: 9938.3018\n",
      "gradient norm: 1383.4321823120117, minimum ratio: 0.765625\n",
      "Epoch [1668], val_loss: 9959.5117\n",
      "gradient norm: 1385.487533569336, minimum ratio: 0.765625\n",
      "Epoch [1669], val_loss: 9980.7559\n",
      "gradient norm: 1387.5531616210938, minimum ratio: 0.75390625\n",
      "Epoch [1670], val_loss: 10002.0303\n",
      "gradient norm: 1389.6143493652344, minimum ratio: 0.7421875\n",
      "Epoch [1671], val_loss: 10023.3398\n",
      "gradient norm: 1391.641258239746, minimum ratio: 0.7578125\n",
      "Epoch [1672], val_loss: 10044.6787\n",
      "gradient norm: 1393.712173461914, minimum ratio: 0.75\n",
      "Epoch [1673], val_loss: 10066.0508\n",
      "gradient norm: 1395.7315292358398, minimum ratio: 0.76953125\n",
      "Epoch [1674], val_loss: 10087.4561\n",
      "gradient norm: 1397.8061218261719, minimum ratio: 0.77734375\n",
      "Epoch [1675], val_loss: 10108.8906\n",
      "gradient norm: 1399.8274765014648, minimum ratio: 0.765625\n",
      "Epoch [1676], val_loss: 10130.3613\n",
      "gradient norm: 1401.905746459961, minimum ratio: 0.7890625\n",
      "Epoch [1677], val_loss: 10151.8604\n",
      "gradient norm: 1403.9292373657227, minimum ratio: 0.76953125\n",
      "Epoch [1678], val_loss: 10173.3945\n",
      "gradient norm: 1405.9558715820312, minimum ratio: 0.76953125\n",
      "Epoch [1679], val_loss: 10194.9580\n",
      "gradient norm: 1408.0273666381836, minimum ratio: 0.76953125\n",
      "Epoch [1680], val_loss: 10216.5566\n",
      "gradient norm: 1410.0912322998047, minimum ratio: 0.76171875\n",
      "Epoch [1681], val_loss: 10238.1875\n",
      "gradient norm: 1412.165626525879, minimum ratio: 0.79296875\n",
      "Epoch [1682], val_loss: 10259.8516\n",
      "gradient norm: 1414.254898071289, minimum ratio: 0.7734375\n",
      "Epoch [1683], val_loss: 10281.5508\n",
      "gradient norm: 1416.3070831298828, minimum ratio: 0.7578125\n",
      "Epoch [1684], val_loss: 10303.2812\n",
      "gradient norm: 1418.3793869018555, minimum ratio: 0.765625\n",
      "Epoch [1685], val_loss: 10325.0439\n",
      "gradient norm: 1420.464973449707, minimum ratio: 0.76171875\n",
      "Epoch [1686], val_loss: 10346.8398\n",
      "gradient norm: 1422.4935684204102, minimum ratio: 0.7734375\n",
      "Epoch [1687], val_loss: 10368.6689\n",
      "gradient norm: 1424.5672988891602, minimum ratio: 0.7578125\n",
      "Epoch [1688], val_loss: 10390.5332\n",
      "gradient norm: 1426.6179504394531, minimum ratio: 0.75390625\n",
      "Epoch [1689], val_loss: 10412.4287\n",
      "gradient norm: 1428.7205047607422, minimum ratio: 0.78125\n",
      "Epoch [1690], val_loss: 10434.3584\n",
      "gradient norm: 1430.792869567871, minimum ratio: 0.7421875\n",
      "Epoch [1691], val_loss: 10456.3184\n",
      "gradient norm: 1432.8991012573242, minimum ratio: 0.75390625\n",
      "Epoch [1692], val_loss: 10478.3125\n",
      "gradient norm: 1434.978775024414, minimum ratio: 0.75\n",
      "Epoch [1693], val_loss: 10500.3379\n",
      "gradient norm: 1437.0284805297852, minimum ratio: 0.7578125\n",
      "Epoch [1694], val_loss: 10522.3945\n",
      "gradient norm: 1439.1397171020508, minimum ratio: 0.7890625\n",
      "Epoch [1695], val_loss: 10544.4863\n",
      "gradient norm: 1441.2528533935547, minimum ratio: 0.7734375\n",
      "Epoch [1696], val_loss: 10566.6094\n",
      "gradient norm: 1443.3048782348633, minimum ratio: 0.76953125\n",
      "Epoch [1697], val_loss: 10588.7676\n",
      "gradient norm: 1445.3837890625, minimum ratio: 0.765625\n",
      "Epoch [1698], val_loss: 10610.9570\n",
      "gradient norm: 1447.4700164794922, minimum ratio: 0.7421875\n",
      "Epoch [1699], val_loss: 10633.1807\n",
      "gradient norm: 1449.569923400879, minimum ratio: 0.79296875\n",
      "Epoch [1700], val_loss: 10655.4355\n",
      "gradient norm: 1451.6750946044922, minimum ratio: 0.74609375\n",
      "Epoch [1701], val_loss: 10677.7256\n",
      "gradient norm: 1453.7739944458008, minimum ratio: 0.75\n",
      "Epoch [1702], val_loss: 10700.0508\n",
      "gradient norm: 1455.8778610229492, minimum ratio: 0.7734375\n",
      "Epoch [1703], val_loss: 10722.4053\n",
      "gradient norm: 1457.9739151000977, minimum ratio: 0.77734375\n",
      "Epoch [1704], val_loss: 10744.7959\n",
      "gradient norm: 1460.103874206543, minimum ratio: 0.765625\n",
      "Epoch [1705], val_loss: 10767.2188\n",
      "gradient norm: 1462.1591415405273, minimum ratio: 0.7578125\n",
      "Epoch [1706], val_loss: 10789.6768\n",
      "gradient norm: 1464.2787170410156, minimum ratio: 0.80078125\n",
      "Epoch [1707], val_loss: 10812.1670\n",
      "gradient norm: 1466.3586120605469, minimum ratio: 0.75\n",
      "Epoch [1708], val_loss: 10834.6904\n",
      "gradient norm: 1468.4660034179688, minimum ratio: 0.765625\n",
      "Epoch [1709], val_loss: 10857.2500\n",
      "gradient norm: 1470.5976943969727, minimum ratio: 0.7734375\n",
      "Epoch [1710], val_loss: 10879.8428\n",
      "gradient norm: 1472.7394714355469, minimum ratio: 0.76953125\n",
      "Epoch [1711], val_loss: 10902.4697\n",
      "gradient norm: 1474.8831939697266, minimum ratio: 0.78125\n",
      "Epoch [1712], val_loss: 10925.1279\n",
      "gradient norm: 1476.9876556396484, minimum ratio: 0.7578125\n",
      "Epoch [1713], val_loss: 10947.8232\n",
      "gradient norm: 1479.114112854004, minimum ratio: 0.73828125\n",
      "Epoch [1714], val_loss: 10970.5498\n",
      "gradient norm: 1481.2159957885742, minimum ratio: 0.77734375\n",
      "Epoch [1715], val_loss: 10993.3096\n",
      "gradient norm: 1483.3160781860352, minimum ratio: 0.76953125\n",
      "Epoch [1716], val_loss: 11016.1045\n",
      "gradient norm: 1485.4689483642578, minimum ratio: 0.77734375\n",
      "Epoch [1717], val_loss: 11038.9316\n",
      "gradient norm: 1487.5709533691406, minimum ratio: 0.75390625\n",
      "Epoch [1718], val_loss: 11061.7949\n",
      "gradient norm: 1489.6936645507812, minimum ratio: 0.7578125\n",
      "Epoch [1719], val_loss: 11084.6904\n",
      "gradient norm: 1491.8520889282227, minimum ratio: 0.79296875\n",
      "Epoch [1720], val_loss: 11107.6182\n",
      "gradient norm: 1494.0126113891602, minimum ratio: 0.7421875\n",
      "Epoch [1721], val_loss: 11130.5811\n",
      "gradient norm: 1496.1310348510742, minimum ratio: 0.76171875\n",
      "Epoch [1722], val_loss: 11153.5771\n",
      "gradient norm: 1498.2227172851562, minimum ratio: 0.76953125\n",
      "Epoch [1723], val_loss: 11176.6064\n",
      "gradient norm: 1500.3151168823242, minimum ratio: 0.76171875\n",
      "Epoch [1724], val_loss: 11199.6719\n",
      "gradient norm: 1502.4023056030273, minimum ratio: 0.7578125\n",
      "Epoch [1725], val_loss: 11222.7725\n",
      "gradient norm: 1504.5466613769531, minimum ratio: 0.765625\n",
      "Epoch [1726], val_loss: 11245.9033\n",
      "gradient norm: 1506.718002319336, minimum ratio: 0.7578125\n",
      "Epoch [1727], val_loss: 11269.0723\n",
      "gradient norm: 1508.8913269042969, minimum ratio: 0.7734375\n",
      "Epoch [1728], val_loss: 11292.2705\n",
      "gradient norm: 1511.0548400878906, minimum ratio: 0.7734375\n",
      "Epoch [1729], val_loss: 11315.5059\n",
      "gradient norm: 1513.2071838378906, minimum ratio: 0.78515625\n",
      "Epoch [1730], val_loss: 11338.7754\n",
      "gradient norm: 1515.3519744873047, minimum ratio: 0.76171875\n",
      "Epoch [1731], val_loss: 11362.0781\n",
      "gradient norm: 1517.5025787353516, minimum ratio: 0.74609375\n",
      "Epoch [1732], val_loss: 11385.4131\n",
      "gradient norm: 1519.6852264404297, minimum ratio: 0.76953125\n",
      "Epoch [1733], val_loss: 11408.7842\n",
      "gradient norm: 1521.8358001708984, minimum ratio: 0.7578125\n",
      "Epoch [1734], val_loss: 11432.1895\n",
      "gradient norm: 1523.9646606445312, minimum ratio: 0.7578125\n",
      "Epoch [1735], val_loss: 11455.6279\n",
      "gradient norm: 1526.1087188720703, minimum ratio: 0.7578125\n",
      "Epoch [1736], val_loss: 11479.1006\n",
      "gradient norm: 1528.2134399414062, minimum ratio: 0.75390625\n",
      "Epoch [1737], val_loss: 11502.6104\n",
      "gradient norm: 1530.405517578125, minimum ratio: 0.765625\n",
      "Epoch [1738], val_loss: 11526.1523\n",
      "gradient norm: 1532.5754699707031, minimum ratio: 0.7578125\n",
      "Epoch [1739], val_loss: 11549.7295\n",
      "gradient norm: 1534.7711029052734, minimum ratio: 0.75390625\n",
      "Epoch [1740], val_loss: 11573.3389\n",
      "gradient norm: 1536.9041900634766, minimum ratio: 0.7578125\n",
      "Epoch [1741], val_loss: 11596.9844\n",
      "gradient norm: 1539.1034851074219, minimum ratio: 0.7734375\n",
      "Epoch [1742], val_loss: 11620.6641\n",
      "gradient norm: 1541.2799377441406, minimum ratio: 0.78515625\n",
      "Epoch [1743], val_loss: 11644.3770\n",
      "gradient norm: 1543.4831085205078, minimum ratio: 0.765625\n",
      "Epoch [1744], val_loss: 11668.1270\n",
      "gradient norm: 1545.6819915771484, minimum ratio: 0.77734375\n",
      "Epoch [1745], val_loss: 11691.9072\n",
      "gradient norm: 1547.8888397216797, minimum ratio: 0.7890625\n",
      "Epoch [1746], val_loss: 11715.7246\n",
      "gradient norm: 1550.0613708496094, minimum ratio: 0.7890625\n",
      "Epoch [1747], val_loss: 11739.5732\n",
      "gradient norm: 1552.2442779541016, minimum ratio: 0.765625\n",
      "Epoch [1748], val_loss: 11763.4580\n",
      "gradient norm: 1554.4307861328125, minimum ratio: 0.7734375\n",
      "Epoch [1749], val_loss: 11787.3770\n",
      "gradient norm: 1556.5995635986328, minimum ratio: 0.76953125\n",
      "Epoch [1750], val_loss: 11811.3291\n",
      "gradient norm: 1558.8155364990234, minimum ratio: 0.7421875\n",
      "Epoch [1751], val_loss: 11835.3174\n",
      "gradient norm: 1560.9820251464844, minimum ratio: 0.77734375\n",
      "Epoch [1752], val_loss: 11859.3398\n",
      "gradient norm: 1563.1046905517578, minimum ratio: 0.78515625\n",
      "Epoch [1753], val_loss: 11883.3984\n",
      "gradient norm: 1565.3260803222656, minimum ratio: 0.75\n",
      "Epoch [1754], val_loss: 11907.4883\n",
      "gradient norm: 1567.548110961914, minimum ratio: 0.74609375\n",
      "Epoch [1755], val_loss: 11931.6133\n",
      "gradient norm: 1569.7430114746094, minimum ratio: 0.75390625\n",
      "Epoch [1756], val_loss: 11955.7734\n",
      "gradient norm: 1571.9216918945312, minimum ratio: 0.78515625\n",
      "Epoch [1757], val_loss: 11979.9688\n",
      "gradient norm: 1574.0100708007812, minimum ratio: 0.74609375\n",
      "Epoch [1758], val_loss: 12004.2012\n",
      "gradient norm: 1576.2408905029297, minimum ratio: 0.77734375\n",
      "Epoch [1759], val_loss: 12028.4678\n",
      "gradient norm: 1578.4636535644531, minimum ratio: 0.7734375\n",
      "Epoch [1760], val_loss: 12052.7695\n",
      "gradient norm: 1580.6980743408203, minimum ratio: 0.765625\n",
      "Epoch [1761], val_loss: 12077.1045\n",
      "gradient norm: 1582.9344329833984, minimum ratio: 0.74609375\n",
      "Epoch [1762], val_loss: 12101.4756\n",
      "gradient norm: 1585.1728515625, minimum ratio: 0.7734375\n",
      "Epoch [1763], val_loss: 12125.8818\n",
      "gradient norm: 1587.4013366699219, minimum ratio: 0.77734375\n",
      "Epoch [1764], val_loss: 12150.3213\n",
      "gradient norm: 1589.557144165039, minimum ratio: 0.75390625\n",
      "Epoch [1765], val_loss: 12174.7979\n",
      "gradient norm: 1591.7310180664062, minimum ratio: 0.76171875\n",
      "Epoch [1766], val_loss: 12199.3096\n",
      "gradient norm: 1593.9769287109375, minimum ratio: 0.7734375\n",
      "Epoch [1767], val_loss: 12223.8584\n",
      "gradient norm: 1596.1110229492188, minimum ratio: 0.7734375\n",
      "Epoch [1768], val_loss: 12248.4414\n",
      "gradient norm: 1598.3188171386719, minimum ratio: 0.76953125\n",
      "Epoch [1769], val_loss: 12273.0605\n",
      "gradient norm: 1600.5708770751953, minimum ratio: 0.76953125\n",
      "Epoch [1770], val_loss: 12297.7129\n",
      "gradient norm: 1602.7976989746094, minimum ratio: 0.78125\n",
      "Epoch [1771], val_loss: 12322.4033\n",
      "gradient norm: 1605.0253295898438, minimum ratio: 0.76171875\n",
      "Epoch [1772], val_loss: 12347.1270\n",
      "gradient norm: 1607.2761535644531, minimum ratio: 0.77734375\n",
      "Epoch [1773], val_loss: 12371.8857\n",
      "gradient norm: 1609.5356140136719, minimum ratio: 0.7578125\n",
      "Epoch [1774], val_loss: 12396.6816\n",
      "gradient norm: 1611.7606201171875, minimum ratio: 0.7734375\n",
      "Epoch [1775], val_loss: 12421.5117\n",
      "gradient norm: 1613.9599609375, minimum ratio: 0.76953125\n",
      "Epoch [1776], val_loss: 12446.3799\n",
      "gradient norm: 1616.1749572753906, minimum ratio: 0.734375\n",
      "Epoch [1777], val_loss: 12471.2803\n",
      "gradient norm: 1618.4359588623047, minimum ratio: 0.76171875\n",
      "Epoch [1778], val_loss: 12496.2188\n",
      "gradient norm: 1620.6777648925781, minimum ratio: 0.75390625\n",
      "Epoch [1779], val_loss: 12521.1904\n",
      "gradient norm: 1622.9488067626953, minimum ratio: 0.7421875\n",
      "Epoch [1780], val_loss: 12546.1973\n",
      "gradient norm: 1625.151870727539, minimum ratio: 0.76953125\n",
      "Epoch [1781], val_loss: 12571.2432\n",
      "gradient norm: 1627.4096984863281, minimum ratio: 0.7734375\n",
      "Epoch [1782], val_loss: 12596.3223\n",
      "gradient norm: 1629.6860961914062, minimum ratio: 0.78515625\n",
      "Epoch [1783], val_loss: 12621.4355\n",
      "gradient norm: 1631.88330078125, minimum ratio: 0.75390625\n",
      "Epoch [1784], val_loss: 12646.5850\n",
      "gradient norm: 1634.1634826660156, minimum ratio: 0.75\n",
      "Epoch [1785], val_loss: 12671.7754\n",
      "gradient norm: 1636.4459381103516, minimum ratio: 0.75390625\n",
      "Epoch [1786], val_loss: 12696.9951\n",
      "gradient norm: 1638.7302703857422, minimum ratio: 0.75390625\n",
      "Epoch [1787], val_loss: 12722.2529\n",
      "gradient norm: 1641.0165557861328, minimum ratio: 0.7734375\n",
      "Epoch [1788], val_loss: 12747.5469\n",
      "gradient norm: 1643.2776947021484, minimum ratio: 0.73828125\n",
      "Epoch [1789], val_loss: 12772.8760\n",
      "gradient norm: 1645.567855834961, minimum ratio: 0.76171875\n",
      "Epoch [1790], val_loss: 12798.2412\n",
      "gradient norm: 1647.8143463134766, minimum ratio: 0.7734375\n",
      "Epoch [1791], val_loss: 12823.6416\n",
      "gradient norm: 1650.01611328125, minimum ratio: 0.75390625\n",
      "Epoch [1792], val_loss: 12849.0820\n",
      "gradient norm: 1652.312271118164, minimum ratio: 0.765625\n",
      "Epoch [1793], val_loss: 12874.5537\n",
      "gradient norm: 1654.5672302246094, minimum ratio: 0.78125\n",
      "Epoch [1794], val_loss: 12900.0635\n",
      "gradient norm: 1656.830078125, minimum ratio: 0.765625\n",
      "Epoch [1795], val_loss: 12925.6133\n",
      "gradient norm: 1659.0182647705078, minimum ratio: 0.76171875\n",
      "Epoch [1796], val_loss: 12951.1963\n",
      "gradient norm: 1661.2947082519531, minimum ratio: 0.75390625\n",
      "Epoch [1797], val_loss: 12976.8164\n",
      "gradient norm: 1663.5299530029297, minimum ratio: 0.75\n",
      "Epoch [1798], val_loss: 13002.4717\n",
      "gradient norm: 1665.737060546875, minimum ratio: 0.75390625\n",
      "Epoch [1799], val_loss: 13028.1660\n",
      "gradient norm: 1667.9581298828125, minimum ratio: 0.77734375\n",
      "Epoch [1800], val_loss: 13053.8936\n",
      "gradient norm: 1670.269515991211, minimum ratio: 0.77734375\n",
      "Epoch [1801], val_loss: 13079.6582\n",
      "gradient norm: 1672.5776672363281, minimum ratio: 0.75390625\n",
      "Epoch [1802], val_loss: 13105.4609\n",
      "gradient norm: 1674.8928985595703, minimum ratio: 0.76953125\n",
      "Epoch [1803], val_loss: 13131.2969\n",
      "gradient norm: 1677.1885070800781, minimum ratio: 0.75\n",
      "Epoch [1804], val_loss: 13157.1689\n",
      "gradient norm: 1679.466567993164, minimum ratio: 0.78125\n",
      "Epoch [1805], val_loss: 13183.0781\n",
      "gradient norm: 1681.7873229980469, minimum ratio: 0.77734375\n",
      "Epoch [1806], val_loss: 13209.0244\n",
      "gradient norm: 1684.0623016357422, minimum ratio: 0.76953125\n",
      "Epoch [1807], val_loss: 13235.0049\n",
      "gradient norm: 1686.3866424560547, minimum ratio: 0.74609375\n",
      "Epoch [1808], val_loss: 13261.0195\n",
      "gradient norm: 1688.7076416015625, minimum ratio: 0.76171875\n",
      "Epoch [1809], val_loss: 13287.0781\n",
      "gradient norm: 1690.9779968261719, minimum ratio: 0.75\n",
      "Epoch [1810], val_loss: 13313.1709\n",
      "gradient norm: 1693.2801361083984, minimum ratio: 0.765625\n",
      "Epoch [1811], val_loss: 13339.2998\n",
      "gradient norm: 1695.6128997802734, minimum ratio: 0.75390625\n",
      "Epoch [1812], val_loss: 13365.4658\n",
      "gradient norm: 1697.9477233886719, minimum ratio: 0.76171875\n",
      "Epoch [1813], val_loss: 13391.6699\n",
      "gradient norm: 1700.2281951904297, minimum ratio: 0.73828125\n",
      "Epoch [1814], val_loss: 13417.9082\n",
      "gradient norm: 1702.5247039794922, minimum ratio: 0.77734375\n",
      "Epoch [1815], val_loss: 13444.1816\n",
      "gradient norm: 1704.8370819091797, minimum ratio: 0.76171875\n",
      "Epoch [1816], val_loss: 13470.4922\n",
      "gradient norm: 1707.1486053466797, minimum ratio: 0.7421875\n",
      "Epoch [1817], val_loss: 13496.8389\n",
      "gradient norm: 1709.4807586669922, minimum ratio: 0.7421875\n",
      "Epoch [1818], val_loss: 13523.2236\n",
      "gradient norm: 1711.8271179199219, minimum ratio: 0.7890625\n",
      "Epoch [1819], val_loss: 13549.6445\n",
      "gradient norm: 1714.142074584961, minimum ratio: 0.75\n",
      "Epoch [1820], val_loss: 13576.1025\n",
      "gradient norm: 1716.4922180175781, minimum ratio: 0.75\n",
      "Epoch [1821], val_loss: 13602.5947\n",
      "gradient norm: 1718.7992248535156, minimum ratio: 0.75390625\n",
      "Epoch [1822], val_loss: 13629.1260\n",
      "gradient norm: 1721.1180725097656, minimum ratio: 0.7421875\n",
      "Epoch [1823], val_loss: 13655.6963\n",
      "gradient norm: 1723.4459991455078, minimum ratio: 0.765625\n",
      "Epoch [1824], val_loss: 13682.2998\n",
      "gradient norm: 1725.7674865722656, minimum ratio: 0.765625\n",
      "Epoch [1825], val_loss: 13708.9424\n",
      "gradient norm: 1728.076431274414, minimum ratio: 0.765625\n",
      "Epoch [1826], val_loss: 13735.6211\n",
      "gradient norm: 1730.360580444336, minimum ratio: 0.74609375\n",
      "Epoch [1827], val_loss: 13762.3379\n",
      "gradient norm: 1732.6712341308594, minimum ratio: 0.75390625\n",
      "Epoch [1828], val_loss: 13789.0908\n",
      "gradient norm: 1735.006088256836, minimum ratio: 0.75390625\n",
      "Epoch [1829], val_loss: 13815.8789\n",
      "gradient norm: 1737.373046875, minimum ratio: 0.75\n",
      "Epoch [1830], val_loss: 13842.7061\n",
      "gradient norm: 1739.7015228271484, minimum ratio: 0.765625\n",
      "Epoch [1831], val_loss: 13869.5693\n",
      "gradient norm: 1741.974105834961, minimum ratio: 0.73046875\n",
      "Epoch [1832], val_loss: 13896.4707\n",
      "gradient norm: 1744.2974090576172, minimum ratio: 0.76953125\n",
      "Epoch [1833], val_loss: 13923.4082\n",
      "gradient norm: 1746.6310729980469, minimum ratio: 0.7734375\n",
      "Epoch [1834], val_loss: 13950.3838\n",
      "gradient norm: 1748.9349975585938, minimum ratio: 0.77734375\n",
      "Epoch [1835], val_loss: 13977.3936\n",
      "gradient norm: 1751.3128967285156, minimum ratio: 0.7890625\n",
      "Epoch [1836], val_loss: 14004.4434\n",
      "gradient norm: 1753.6290435791016, minimum ratio: 0.7734375\n",
      "Epoch [1837], val_loss: 14031.5312\n",
      "gradient norm: 1756.0113830566406, minimum ratio: 0.765625\n",
      "Epoch [1838], val_loss: 14058.6533\n",
      "gradient norm: 1758.3542785644531, minimum ratio: 0.75390625\n",
      "Epoch [1839], val_loss: 14085.8154\n",
      "gradient norm: 1760.7399139404297, minimum ratio: 0.7421875\n",
      "Epoch [1840], val_loss: 14113.0098\n",
      "gradient norm: 1763.0924072265625, minimum ratio: 0.78515625\n",
      "Epoch [1841], val_loss: 14140.2451\n",
      "gradient norm: 1765.4464569091797, minimum ratio: 0.7578125\n",
      "Epoch [1842], val_loss: 14167.5166\n",
      "gradient norm: 1767.8353576660156, minimum ratio: 0.76953125\n",
      "Epoch [1843], val_loss: 14194.8281\n",
      "gradient norm: 1770.1578063964844, minimum ratio: 0.7578125\n",
      "Epoch [1844], val_loss: 14222.1768\n",
      "gradient norm: 1772.5535278320312, minimum ratio: 0.7421875\n",
      "Epoch [1845], val_loss: 14249.5635\n",
      "gradient norm: 1774.9261627197266, minimum ratio: 0.78125\n",
      "Epoch [1846], val_loss: 14276.9863\n",
      "gradient norm: 1777.2656555175781, minimum ratio: 0.76171875\n",
      "Epoch [1847], val_loss: 14304.4482\n",
      "gradient norm: 1779.635757446289, minimum ratio: 0.75390625\n",
      "Epoch [1848], val_loss: 14331.9482\n",
      "gradient norm: 1782.0397491455078, minimum ratio: 0.76953125\n",
      "Epoch [1849], val_loss: 14359.4854\n",
      "gradient norm: 1784.4454345703125, minimum ratio: 0.7578125\n",
      "Epoch [1850], val_loss: 14387.0586\n",
      "gradient norm: 1786.8531646728516, minimum ratio: 0.7734375\n",
      "Epoch [1851], val_loss: 14414.6719\n",
      "gradient norm: 1789.2298278808594, minimum ratio: 0.77734375\n",
      "Epoch [1852], val_loss: 14442.3232\n",
      "gradient norm: 1791.5855255126953, minimum ratio: 0.76171875\n",
      "Epoch [1853], val_loss: 14470.0117\n",
      "gradient norm: 1793.970474243164, minimum ratio: 0.75\n",
      "Epoch [1854], val_loss: 14497.7363\n",
      "gradient norm: 1796.382553100586, minimum ratio: 0.73828125\n",
      "Epoch [1855], val_loss: 14525.5000\n",
      "gradient norm: 1798.768295288086, minimum ratio: 0.73046875\n",
      "Epoch [1856], val_loss: 14553.2979\n",
      "gradient norm: 1801.1497039794922, minimum ratio: 0.7578125\n",
      "Epoch [1857], val_loss: 14581.1338\n",
      "gradient norm: 1803.5706329345703, minimum ratio: 0.7421875\n",
      "Epoch [1858], val_loss: 14609.0068\n",
      "gradient norm: 1805.9519500732422, minimum ratio: 0.7421875\n",
      "Epoch [1859], val_loss: 14636.9199\n",
      "gradient norm: 1808.2587585449219, minimum ratio: 0.75\n",
      "Epoch [1860], val_loss: 14664.8701\n",
      "gradient norm: 1810.685302734375, minimum ratio: 0.75390625\n",
      "Epoch [1861], val_loss: 14692.8564\n",
      "gradient norm: 1813.0983428955078, minimum ratio: 0.77734375\n",
      "Epoch [1862], val_loss: 14720.8848\n",
      "gradient norm: 1815.5242156982422, minimum ratio: 0.765625\n",
      "Epoch [1863], val_loss: 14748.9463\n",
      "gradient norm: 1817.8388061523438, minimum ratio: 0.76171875\n",
      "Epoch [1864], val_loss: 14777.0498\n",
      "gradient norm: 1820.1879425048828, minimum ratio: 0.765625\n",
      "Epoch [1865], val_loss: 14805.1934\n",
      "gradient norm: 1822.5045776367188, minimum ratio: 0.7578125\n",
      "Epoch [1866], val_loss: 14833.3730\n",
      "gradient norm: 1824.9427032470703, minimum ratio: 0.74609375\n",
      "Epoch [1867], val_loss: 14861.5947\n",
      "gradient norm: 1827.3831329345703, minimum ratio: 0.7421875\n",
      "Epoch [1868], val_loss: 14889.8496\n",
      "gradient norm: 1829.7818756103516, minimum ratio: 0.76953125\n",
      "Epoch [1869], val_loss: 14918.1445\n",
      "gradient norm: 1832.1712493896484, minimum ratio: 0.76171875\n",
      "Epoch [1870], val_loss: 14946.4785\n",
      "gradient norm: 1834.545425415039, minimum ratio: 0.74609375\n",
      "Epoch [1871], val_loss: 14974.8486\n",
      "gradient norm: 1836.9779052734375, minimum ratio: 0.77734375\n",
      "Epoch [1872], val_loss: 15003.2588\n",
      "gradient norm: 1839.4276123046875, minimum ratio: 0.73046875\n",
      "Epoch [1873], val_loss: 15031.7051\n",
      "gradient norm: 1841.8483123779297, minimum ratio: 0.75390625\n",
      "Epoch [1874], val_loss: 15060.1914\n",
      "gradient norm: 1844.3021545410156, minimum ratio: 0.76171875\n",
      "Epoch [1875], val_loss: 15088.7168\n",
      "gradient norm: 1846.7582702636719, minimum ratio: 0.7734375\n",
      "Epoch [1876], val_loss: 15117.2832\n",
      "gradient norm: 1849.2166290283203, minimum ratio: 0.72265625\n",
      "Epoch [1877], val_loss: 15145.8838\n",
      "gradient norm: 1851.6488189697266, minimum ratio: 0.7734375\n",
      "Epoch [1878], val_loss: 15174.5215\n",
      "gradient norm: 1854.1104888916016, minimum ratio: 0.75390625\n",
      "Epoch [1879], val_loss: 15203.2002\n",
      "gradient norm: 1856.4989318847656, minimum ratio: 0.74609375\n",
      "Epoch [1880], val_loss: 15231.9170\n",
      "gradient norm: 1858.8806762695312, minimum ratio: 0.74609375\n",
      "Epoch [1881], val_loss: 15260.6748\n",
      "gradient norm: 1861.3153686523438, minimum ratio: 0.74609375\n",
      "Epoch [1882], val_loss: 15289.4688\n",
      "gradient norm: 1863.7175598144531, minimum ratio: 0.75\n",
      "Epoch [1883], val_loss: 15318.3037\n",
      "gradient norm: 1866.1891632080078, minimum ratio: 0.75390625\n",
      "Epoch [1884], val_loss: 15347.1729\n",
      "gradient norm: 1868.586685180664, minimum ratio: 0.7421875\n",
      "Epoch [1885], val_loss: 15376.0830\n",
      "gradient norm: 1871.0535125732422, minimum ratio: 0.765625\n",
      "Epoch [1886], val_loss: 15405.0303\n",
      "gradient norm: 1873.50341796875, minimum ratio: 0.75390625\n",
      "Epoch [1887], val_loss: 15434.0166\n",
      "gradient norm: 1875.9827270507812, minimum ratio: 0.75390625\n",
      "Epoch [1888], val_loss: 15463.0430\n",
      "gradient norm: 1878.4366607666016, minimum ratio: 0.76171875\n",
      "Epoch [1889], val_loss: 15492.1055\n",
      "gradient norm: 1880.854019165039, minimum ratio: 0.76171875\n",
      "Epoch [1890], val_loss: 15521.2100\n",
      "gradient norm: 1883.2629089355469, minimum ratio: 0.7734375\n",
      "Epoch [1891], val_loss: 15550.3516\n",
      "gradient norm: 1885.7498321533203, minimum ratio: 0.7578125\n",
      "Epoch [1892], val_loss: 15579.5303\n",
      "gradient norm: 1888.2124938964844, minimum ratio: 0.78515625\n",
      "Epoch [1893], val_loss: 15608.7529\n",
      "gradient norm: 1890.673095703125, minimum ratio: 0.76171875\n",
      "Epoch [1894], val_loss: 15638.0117\n",
      "gradient norm: 1893.1558074951172, minimum ratio: 0.76171875\n",
      "Epoch [1895], val_loss: 15667.3135\n",
      "gradient norm: 1895.594985961914, minimum ratio: 0.75\n",
      "Epoch [1896], val_loss: 15696.6514\n",
      "gradient norm: 1898.0451965332031, minimum ratio: 0.75390625\n",
      "Epoch [1897], val_loss: 15726.0312\n",
      "gradient norm: 1900.5444641113281, minimum ratio: 0.78125\n",
      "Epoch [1898], val_loss: 15755.4521\n",
      "gradient norm: 1902.9539947509766, minimum ratio: 0.7421875\n",
      "Epoch [1899], val_loss: 15784.9062\n",
      "gradient norm: 1905.4123077392578, minimum ratio: 0.796875\n",
      "Epoch [1900], val_loss: 15814.4014\n",
      "gradient norm: 1907.9173889160156, minimum ratio: 0.78515625\n",
      "Epoch [1901], val_loss: 15843.9365\n",
      "gradient norm: 1910.347412109375, minimum ratio: 0.7578125\n",
      "Epoch [1902], val_loss: 15873.5098\n",
      "gradient norm: 1912.8558654785156, minimum ratio: 0.75390625\n",
      "Epoch [1903], val_loss: 15903.1201\n",
      "gradient norm: 1915.3667297363281, minimum ratio: 0.78515625\n",
      "Epoch [1904], val_loss: 15932.7734\n",
      "gradient norm: 1917.8456726074219, minimum ratio: 0.7578125\n",
      "Epoch [1905], val_loss: 15962.4629\n",
      "gradient norm: 1920.3604278564453, minimum ratio: 0.76171875\n",
      "Epoch [1906], val_loss: 15992.1953\n",
      "gradient norm: 1922.8013458251953, minimum ratio: 0.76171875\n",
      "Epoch [1907], val_loss: 16021.9648\n",
      "gradient norm: 1925.3205261230469, minimum ratio: 0.76171875\n",
      "Epoch [1908], val_loss: 16051.7764\n",
      "gradient norm: 1927.7323760986328, minimum ratio: 0.75390625\n",
      "Epoch [1909], val_loss: 16081.6270\n",
      "gradient norm: 1930.1869812011719, minimum ratio: 0.7734375\n",
      "Epoch [1910], val_loss: 16111.5166\n",
      "gradient norm: 1932.6734771728516, minimum ratio: 0.7578125\n",
      "Epoch [1911], val_loss: 16141.4463\n",
      "gradient norm: 1935.2001495361328, minimum ratio: 0.765625\n",
      "Epoch [1912], val_loss: 16171.4121\n",
      "gradient norm: 1937.6891784667969, minimum ratio: 0.75390625\n",
      "Epoch [1913], val_loss: 16201.4229\n",
      "gradient norm: 1940.1449584960938, minimum ratio: 0.73828125\n",
      "Epoch [1914], val_loss: 16231.4736\n",
      "gradient norm: 1942.6442565917969, minimum ratio: 0.76171875\n",
      "Epoch [1915], val_loss: 16261.5586\n",
      "gradient norm: 1945.146469116211, minimum ratio: 0.7578125\n",
      "Epoch [1916], val_loss: 16291.6865\n",
      "gradient norm: 1947.682861328125, minimum ratio: 0.74609375\n",
      "Epoch [1917], val_loss: 16321.8516\n",
      "gradient norm: 1950.1528015136719, minimum ratio: 0.78125\n",
      "Epoch [1918], val_loss: 16352.0586\n",
      "gradient norm: 1952.6500701904297, minimum ratio: 0.765625\n",
      "Epoch [1919], val_loss: 16382.3037\n",
      "gradient norm: 1955.1827850341797, minimum ratio: 0.78125\n",
      "Epoch [1920], val_loss: 16412.5879\n",
      "gradient norm: 1957.6988067626953, minimum ratio: 0.75390625\n",
      "Epoch [1921], val_loss: 16442.9141\n",
      "gradient norm: 1960.1744995117188, minimum ratio: 0.79296875\n",
      "Epoch [1922], val_loss: 16473.2773\n",
      "gradient norm: 1962.7224426269531, minimum ratio: 0.73828125\n",
      "Epoch [1923], val_loss: 16503.6777\n",
      "gradient norm: 1965.2254638671875, minimum ratio: 0.73828125\n",
      "Epoch [1924], val_loss: 16534.1191\n",
      "gradient norm: 1967.7767639160156, minimum ratio: 0.76171875\n",
      "Epoch [1925], val_loss: 16564.6016\n",
      "gradient norm: 1970.3300018310547, minimum ratio: 0.78125\n",
      "Epoch [1926], val_loss: 16595.1211\n",
      "gradient norm: 1972.8853454589844, minimum ratio: 0.77734375\n",
      "Epoch [1927], val_loss: 16625.6816\n",
      "gradient norm: 1975.4278717041016, minimum ratio: 0.7890625\n",
      "Epoch [1928], val_loss: 16656.2793\n",
      "gradient norm: 1977.9354400634766, minimum ratio: 0.7734375\n",
      "Epoch [1929], val_loss: 16686.9219\n",
      "gradient norm: 1980.3285064697266, minimum ratio: 0.73828125\n",
      "Epoch [1930], val_loss: 16717.6035\n",
      "gradient norm: 1982.8917846679688, minimum ratio: 0.7578125\n",
      "Epoch [1931], val_loss: 16748.3242\n",
      "gradient norm: 1985.4136352539062, minimum ratio: 0.75390625\n",
      "Epoch [1932], val_loss: 16779.0879\n",
      "gradient norm: 1987.9811248779297, minimum ratio: 0.765625\n",
      "Epoch [1933], val_loss: 16809.8926\n",
      "gradient norm: 1990.4852142333984, minimum ratio: 0.76171875\n",
      "Epoch [1934], val_loss: 16840.7324\n",
      "gradient norm: 1992.9766998291016, minimum ratio: 0.76953125\n",
      "Epoch [1935], val_loss: 16871.6152\n",
      "gradient norm: 1995.518081665039, minimum ratio: 0.75390625\n",
      "Epoch [1936], val_loss: 16902.5391\n",
      "gradient norm: 1998.093521118164, minimum ratio: 0.78125\n",
      "Epoch [1937], val_loss: 16933.5020\n",
      "gradient norm: 2000.6710205078125, minimum ratio: 0.74609375\n",
      "Epoch [1938], val_loss: 16964.5059\n",
      "gradient norm: 2003.2433471679688, minimum ratio: 0.7578125\n",
      "Epoch [1939], val_loss: 16995.5469\n",
      "gradient norm: 2005.7445220947266, minimum ratio: 0.74609375\n",
      "Epoch [1940], val_loss: 17026.6309\n",
      "gradient norm: 2008.2424926757812, minimum ratio: 0.76953125\n",
      "Epoch [1941], val_loss: 17057.7539\n",
      "gradient norm: 2010.8280487060547, minimum ratio: 0.7578125\n",
      "Epoch [1942], val_loss: 17088.9160\n",
      "gradient norm: 2013.3791046142578, minimum ratio: 0.7265625\n",
      "Epoch [1943], val_loss: 17120.1211\n",
      "gradient norm: 2015.9307861328125, minimum ratio: 0.75\n",
      "Epoch [1944], val_loss: 17151.3672\n",
      "gradient norm: 2018.4727325439453, minimum ratio: 0.74609375\n",
      "Epoch [1945], val_loss: 17182.6504\n",
      "gradient norm: 2021.0289154052734, minimum ratio: 0.76171875\n",
      "Epoch [1946], val_loss: 17213.9746\n",
      "gradient norm: 2023.5847930908203, minimum ratio: 0.76171875\n",
      "Epoch [1947], val_loss: 17245.3418\n",
      "gradient norm: 2026.1281127929688, minimum ratio: 0.7578125\n",
      "Epoch [1948], val_loss: 17276.7441\n",
      "gradient norm: 2028.6582794189453, minimum ratio: 0.74609375\n",
      "Epoch [1949], val_loss: 17308.1914\n",
      "gradient norm: 2031.2285614013672, minimum ratio: 0.75\n",
      "Epoch [1950], val_loss: 17339.6797\n",
      "gradient norm: 2033.7877807617188, minimum ratio: 0.76953125\n",
      "Epoch [1951], val_loss: 17371.2109\n",
      "gradient norm: 2036.3927001953125, minimum ratio: 0.7578125\n",
      "Epoch [1952], val_loss: 17402.7793\n",
      "gradient norm: 2038.9996185302734, minimum ratio: 0.7734375\n",
      "Epoch [1953], val_loss: 17434.3887\n",
      "gradient norm: 2041.5303344726562, minimum ratio: 0.76171875\n",
      "Epoch [1954], val_loss: 17466.0391\n",
      "gradient norm: 2044.0657501220703, minimum ratio: 0.76953125\n",
      "Epoch [1955], val_loss: 17497.7285\n",
      "gradient norm: 2046.608642578125, minimum ratio: 0.8046875\n",
      "Epoch [1956], val_loss: 17529.4570\n",
      "gradient norm: 2049.222686767578, minimum ratio: 0.74609375\n",
      "Epoch [1957], val_loss: 17561.2305\n",
      "gradient norm: 2051.755569458008, minimum ratio: 0.75390625\n",
      "Epoch [1958], val_loss: 17593.0410\n",
      "gradient norm: 2054.3738708496094, minimum ratio: 0.75390625\n",
      "Epoch [1959], val_loss: 17624.8945\n",
      "gradient norm: 2056.994369506836, minimum ratio: 0.74609375\n",
      "Epoch [1960], val_loss: 17656.7891\n",
      "gradient norm: 2059.6168670654297, minimum ratio: 0.765625\n",
      "Epoch [1961], val_loss: 17688.7305\n",
      "gradient norm: 2062.122344970703, minimum ratio: 0.73046875\n",
      "Epoch [1962], val_loss: 17720.7070\n",
      "gradient norm: 2064.7426147460938, minimum ratio: 0.7578125\n",
      "Epoch [1963], val_loss: 17752.7246\n",
      "gradient norm: 2067.3375244140625, minimum ratio: 0.7421875\n",
      "Epoch [1964], val_loss: 17784.7871\n",
      "gradient norm: 2069.9686279296875, minimum ratio: 0.77734375\n",
      "Epoch [1965], val_loss: 17816.8887\n",
      "gradient norm: 2072.5572052001953, minimum ratio: 0.765625\n",
      "Epoch [1966], val_loss: 17849.0371\n",
      "gradient norm: 2075.1924743652344, minimum ratio: 0.77734375\n",
      "Epoch [1967], val_loss: 17881.2188\n",
      "gradient norm: 2077.8294830322266, minimum ratio: 0.7421875\n",
      "Epoch [1968], val_loss: 17913.4395\n",
      "gradient norm: 2080.4684143066406, minimum ratio: 0.765625\n",
      "Epoch [1969], val_loss: 17945.7070\n",
      "gradient norm: 2083.011276245117, minimum ratio: 0.78125\n",
      "Epoch [1970], val_loss: 17978.0156\n",
      "gradient norm: 2085.5747680664062, minimum ratio: 0.7578125\n",
      "Epoch [1971], val_loss: 18010.3613\n",
      "gradient norm: 2088.175308227539, minimum ratio: 0.76171875\n",
      "Epoch [1972], val_loss: 18042.7500\n",
      "gradient norm: 2090.8218688964844, minimum ratio: 0.75\n",
      "Epoch [1973], val_loss: 18075.1777\n",
      "gradient norm: 2093.469970703125, minimum ratio: 0.7734375\n",
      "Epoch [1974], val_loss: 18107.6465\n",
      "gradient norm: 2096.0360412597656, minimum ratio: 0.75390625\n",
      "Epoch [1975], val_loss: 18140.1602\n",
      "gradient norm: 2098.614730834961, minimum ratio: 0.74609375\n",
      "Epoch [1976], val_loss: 18172.7109\n",
      "gradient norm: 2101.2687072753906, minimum ratio: 0.78515625\n",
      "Epoch [1977], val_loss: 18205.3047\n",
      "gradient norm: 2103.890914916992, minimum ratio: 0.75390625\n",
      "Epoch [1978], val_loss: 18237.9414\n",
      "gradient norm: 2106.42626953125, minimum ratio: 0.75390625\n",
      "Epoch [1979], val_loss: 18270.6191\n",
      "gradient norm: 2109.053985595703, minimum ratio: 0.75390625\n",
      "Epoch [1980], val_loss: 18303.3418\n",
      "gradient norm: 2111.7164154052734, minimum ratio: 0.75390625\n",
      "Epoch [1981], val_loss: 18336.0977\n",
      "gradient norm: 2114.3688049316406, minimum ratio: 0.76171875\n",
      "Epoch [1982], val_loss: 18368.9004\n",
      "gradient norm: 2116.978561401367, minimum ratio: 0.734375\n",
      "Epoch [1983], val_loss: 18401.7402\n",
      "gradient norm: 2119.5941009521484, minimum ratio: 0.73828125\n",
      "Epoch [1984], val_loss: 18434.6270\n",
      "gradient norm: 2122.258743286133, minimum ratio: 0.72265625\n",
      "Epoch [1985], val_loss: 18467.5508\n",
      "gradient norm: 2124.8285064697266, minimum ratio: 0.7890625\n",
      "Epoch [1986], val_loss: 18500.5195\n",
      "gradient norm: 2127.5025329589844, minimum ratio: 0.75390625\n",
      "Epoch [1987], val_loss: 18533.5293\n",
      "gradient norm: 2130.1315155029297, minimum ratio: 0.76953125\n",
      "Epoch [1988], val_loss: 18566.5762\n",
      "gradient norm: 2132.809524536133, minimum ratio: 0.765625\n",
      "Epoch [1989], val_loss: 18599.6758\n",
      "gradient norm: 2135.452682495117, minimum ratio: 0.76171875\n",
      "Epoch [1990], val_loss: 18632.8105\n",
      "gradient norm: 2138.096923828125, minimum ratio: 0.76171875\n",
      "Epoch [1991], val_loss: 18665.9883\n",
      "gradient norm: 2140.7410430908203, minimum ratio: 0.75390625\n",
      "Epoch [1992], val_loss: 18699.2090\n",
      "gradient norm: 2143.328323364258, minimum ratio: 0.75390625\n",
      "Epoch [1993], val_loss: 18732.4727\n",
      "gradient norm: 2145.929244995117, minimum ratio: 0.75\n",
      "Epoch [1994], val_loss: 18765.7793\n",
      "gradient norm: 2148.506576538086, minimum ratio: 0.75\n",
      "Epoch [1995], val_loss: 18799.1289\n",
      "gradient norm: 2151.1988677978516, minimum ratio: 0.734375\n",
      "Epoch [1996], val_loss: 18832.5176\n",
      "gradient norm: 2153.8931274414062, minimum ratio: 0.75390625\n",
      "Epoch [1997], val_loss: 18865.9473\n",
      "gradient norm: 2156.548873901367, minimum ratio: 0.75\n",
      "Epoch [1998], val_loss: 18899.4180\n",
      "gradient norm: 2159.2469940185547, minimum ratio: 0.74609375\n",
      "Epoch [1999], val_loss: 18932.9297\n"
     ]
    }
   ],
   "source": [
    "history_1,grad_norm_1,model  = fit(num_epochs, lr, model, data_loader, criterion,optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8ebb7f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'grad_norm': 0.8235280029475689, 'ratio': 0.6875},\n",
       " 1: {'grad_norm': 0.8915635161101818, 'ratio': 0.71875},\n",
       " 2: {'grad_norm': 0.9603039901703596, 'ratio': 0.73046875},\n",
       " 3: {'grad_norm': 1.0295406244695187, 'ratio': 0.6953125},\n",
       " 4: {'grad_norm': 1.0992021076381207, 'ratio': 0.71484375},\n",
       " 5: {'grad_norm': 1.169234350323677, 'ratio': 0.7265625},\n",
       " 6: {'grad_norm': 1.2396116945892572, 'ratio': 0.70703125},\n",
       " 7: {'grad_norm': 1.3102730941027403, 'ratio': 0.7109375},\n",
       " 8: {'grad_norm': 1.3811864703893661, 'ratio': 0.703125},\n",
       " 9: {'grad_norm': 1.4523267354816198, 'ratio': 0.73046875},\n",
       " 10: {'grad_norm': 1.5236770026385784, 'ratio': 0.7265625},\n",
       " 11: {'grad_norm': 1.5952204205095768, 'ratio': 0.72265625},\n",
       " 12: {'grad_norm': 1.666943646967411, 'ratio': 0.71875},\n",
       " 13: {'grad_norm': 1.7388440147042274, 'ratio': 0.72265625},\n",
       " 14: {'grad_norm': 1.8109008111059666, 'ratio': 0.73046875},\n",
       " 15: {'grad_norm': 1.883105468004942, 'ratio': 0.7265625},\n",
       " 16: {'grad_norm': 1.9554706551134586, 'ratio': 0.70703125},\n",
       " 17: {'grad_norm': 2.027964860200882, 'ratio': 0.73046875},\n",
       " 18: {'grad_norm': 2.100593753159046, 'ratio': 0.72265625},\n",
       " 19: {'grad_norm': 2.1733493991196156, 'ratio': 0.7265625},\n",
       " 20: {'grad_norm': 2.246229086071253, 'ratio': 0.7265625},\n",
       " 21: {'grad_norm': 2.3192428275942802, 'ratio': 0.70703125},\n",
       " 22: {'grad_norm': 2.392392113804817, 'ratio': 0.73046875},\n",
       " 23: {'grad_norm': 2.4656244851648808, 'ratio': 0.7265625},\n",
       " 24: {'grad_norm': 2.538997668772936, 'ratio': 0.72265625},\n",
       " 25: {'grad_norm': 2.6124748215079308, 'ratio': 0.74609375},\n",
       " 26: {'grad_norm': 2.6860989294946194, 'ratio': 0.7421875},\n",
       " 27: {'grad_norm': 2.7598377987742424, 'ratio': 0.7265625},\n",
       " 28: {'grad_norm': 2.833683744072914, 'ratio': 0.75},\n",
       " 29: {'grad_norm': 2.9076696820557117, 'ratio': 0.72265625},\n",
       " 30: {'grad_norm': 2.9817573241889477, 'ratio': 0.71484375},\n",
       " 31: {'grad_norm': 3.055971086025238, 'ratio': 0.72265625},\n",
       " 32: {'grad_norm': 3.1302739195525646, 'ratio': 0.73828125},\n",
       " 33: {'grad_norm': 3.204734653234482, 'ratio': 0.7421875},\n",
       " 34: {'grad_norm': 3.2793236300349236, 'ratio': 0.71875},\n",
       " 35: {'grad_norm': 3.3540126383304596, 'ratio': 0.7421875},\n",
       " 36: {'grad_norm': 3.4288570806384087, 'ratio': 0.72265625},\n",
       " 37: {'grad_norm': 3.50380889326334, 'ratio': 0.71484375},\n",
       " 38: {'grad_norm': 3.578916296362877, 'ratio': 0.75390625},\n",
       " 39: {'grad_norm': 3.6541523709893227, 'ratio': 0.70703125},\n",
       " 40: {'grad_norm': 3.729537695646286, 'ratio': 0.7578125},\n",
       " 41: {'grad_norm': 3.8050200790166855, 'ratio': 0.734375},\n",
       " 42: {'grad_norm': 3.8806488066911697, 'ratio': 0.73046875},\n",
       " 43: {'grad_norm': 3.9564497768878937, 'ratio': 0.73828125},\n",
       " 44: {'grad_norm': 4.032356917858124, 'ratio': 0.75390625},\n",
       " 45: {'grad_norm': 4.108404003083706, 'ratio': 0.73046875},\n",
       " 46: {'grad_norm': 4.184586726129055, 'ratio': 0.75390625},\n",
       " 47: {'grad_norm': 4.26091980189085, 'ratio': 0.7421875},\n",
       " 48: {'grad_norm': 4.337426237761974, 'ratio': 0.75},\n",
       " 49: {'grad_norm': 4.414075657725334, 'ratio': 0.73828125},\n",
       " 50: {'grad_norm': 4.490904912352562, 'ratio': 0.734375},\n",
       " 51: {'grad_norm': 4.567852407693863, 'ratio': 0.75390625},\n",
       " 52: {'grad_norm': 4.644963726401329, 'ratio': 0.73828125},\n",
       " 53: {'grad_norm': 4.722197890281677, 'ratio': 0.72265625},\n",
       " 54: {'grad_norm': 4.79959224909544, 'ratio': 0.75},\n",
       " 55: {'grad_norm': 4.877159833908081, 'ratio': 0.71875},\n",
       " 56: {'grad_norm': 4.954899467527866, 'ratio': 0.75390625},\n",
       " 57: {'grad_norm': 5.032802276313305, 'ratio': 0.74609375},\n",
       " 58: {'grad_norm': 5.110862165689468, 'ratio': 0.734375},\n",
       " 59: {'grad_norm': 5.189081892371178, 'ratio': 0.7421875},\n",
       " 60: {'grad_norm': 5.267496608197689, 'ratio': 0.73046875},\n",
       " 61: {'grad_norm': 5.346025072038174, 'ratio': 0.75390625},\n",
       " 62: {'grad_norm': 5.424785807728767, 'ratio': 0.734375},\n",
       " 63: {'grad_norm': 5.503688119351864, 'ratio': 0.7421875},\n",
       " 64: {'grad_norm': 5.582839749753475, 'ratio': 0.7265625},\n",
       " 65: {'grad_norm': 5.662164203822613, 'ratio': 0.73046875},\n",
       " 66: {'grad_norm': 5.741659604012966, 'ratio': 0.765625},\n",
       " 67: {'grad_norm': 5.82134947180748, 'ratio': 0.7421875},\n",
       " 68: {'grad_norm': 5.901173494756222, 'ratio': 0.75390625},\n",
       " 69: {'grad_norm': 5.981208428740501, 'ratio': 0.73828125},\n",
       " 70: {'grad_norm': 6.061456233263016, 'ratio': 0.7578125},\n",
       " 71: {'grad_norm': 6.141924180090427, 'ratio': 0.75},\n",
       " 72: {'grad_norm': 6.222603932023048, 'ratio': 0.72265625},\n",
       " 73: {'grad_norm': 6.303423151373863, 'ratio': 0.75},\n",
       " 74: {'grad_norm': 6.3844787031412125, 'ratio': 0.73828125},\n",
       " 75: {'grad_norm': 6.465806171298027, 'ratio': 0.7421875},\n",
       " 76: {'grad_norm': 6.547274559736252, 'ratio': 0.7578125},\n",
       " 77: {'grad_norm': 6.62901009619236, 'ratio': 0.75390625},\n",
       " 78: {'grad_norm': 6.710871934890747, 'ratio': 0.7265625},\n",
       " 79: {'grad_norm': 6.793015629053116, 'ratio': 0.74609375},\n",
       " 80: {'grad_norm': 6.875336334109306, 'ratio': 0.76953125},\n",
       " 81: {'grad_norm': 6.957880184054375, 'ratio': 0.765625},\n",
       " 82: {'grad_norm': 7.040675163269043, 'ratio': 0.75390625},\n",
       " 83: {'grad_norm': 7.1236827075481415, 'ratio': 0.7421875},\n",
       " 84: {'grad_norm': 7.206952512264252, 'ratio': 0.75},\n",
       " 85: {'grad_norm': 7.290369421243668, 'ratio': 0.71875},\n",
       " 86: {'grad_norm': 7.374066516757011, 'ratio': 0.7578125},\n",
       " 87: {'grad_norm': 7.45803664624691, 'ratio': 0.734375},\n",
       " 88: {'grad_norm': 7.542224034667015, 'ratio': 0.76171875},\n",
       " 89: {'grad_norm': 7.626643612980843, 'ratio': 0.73828125},\n",
       " 90: {'grad_norm': 7.711345478892326, 'ratio': 0.75},\n",
       " 91: {'grad_norm': 7.7962973564863205, 'ratio': 0.734375},\n",
       " 92: {'grad_norm': 7.881477326154709, 'ratio': 0.75},\n",
       " 93: {'grad_norm': 7.966899171471596, 'ratio': 0.73046875},\n",
       " 94: {'grad_norm': 8.052534431219101, 'ratio': 0.74609375},\n",
       " 95: {'grad_norm': 8.138406410813332, 'ratio': 0.734375},\n",
       " 96: {'grad_norm': 8.224633201956749, 'ratio': 0.74609375},\n",
       " 97: {'grad_norm': 8.31118980050087, 'ratio': 0.765625},\n",
       " 98: {'grad_norm': 8.397935181856155, 'ratio': 0.734375},\n",
       " 99: {'grad_norm': 8.484957829117775, 'ratio': 0.73828125},\n",
       " 100: {'grad_norm': 8.572160735726357, 'ratio': 0.75},\n",
       " 101: {'grad_norm': 8.65964725613594, 'ratio': 0.73828125},\n",
       " 102: {'grad_norm': 8.74743638932705, 'ratio': 0.75},\n",
       " 103: {'grad_norm': 8.835606694221497, 'ratio': 0.76171875},\n",
       " 104: {'grad_norm': 8.923951581120491, 'ratio': 0.74609375},\n",
       " 105: {'grad_norm': 9.012556865811348, 'ratio': 0.75},\n",
       " 106: {'grad_norm': 9.101549744606018, 'ratio': 0.75390625},\n",
       " 107: {'grad_norm': 9.190784990787506, 'ratio': 0.75},\n",
       " 108: {'grad_norm': 9.280313432216644, 'ratio': 0.74609375},\n",
       " 109: {'grad_norm': 9.37017223238945, 'ratio': 0.76953125},\n",
       " 110: {'grad_norm': 9.460193052887917, 'ratio': 0.73828125},\n",
       " 111: {'grad_norm': 9.550587370991707, 'ratio': 0.7421875},\n",
       " 112: {'grad_norm': 9.641260907053947, 'ratio': 0.7421875},\n",
       " 113: {'grad_norm': 9.732250526547432, 'ratio': 0.75390625},\n",
       " 114: {'grad_norm': 9.823544561862946, 'ratio': 0.74609375},\n",
       " 115: {'grad_norm': 9.915119871497154, 'ratio': 0.73828125},\n",
       " 116: {'grad_norm': 10.006980136036873, 'ratio': 0.75},\n",
       " 117: {'grad_norm': 10.099140375852585, 'ratio': 0.75390625},\n",
       " 118: {'grad_norm': 10.191658914089203, 'ratio': 0.75390625},\n",
       " 119: {'grad_norm': 10.284352421760559, 'ratio': 0.73828125},\n",
       " 120: {'grad_norm': 10.377447187900543, 'ratio': 0.765625},\n",
       " 121: {'grad_norm': 10.470905154943466, 'ratio': 0.7421875},\n",
       " 122: {'grad_norm': 10.56469240784645, 'ratio': 0.7578125},\n",
       " 123: {'grad_norm': 10.658794492483139, 'ratio': 0.76953125},\n",
       " 124: {'grad_norm': 10.7532759308815, 'ratio': 0.75},\n",
       " 125: {'grad_norm': 10.847934603691101, 'ratio': 0.7578125},\n",
       " 126: {'grad_norm': 10.942928463220596, 'ratio': 0.76953125},\n",
       " 127: {'grad_norm': 11.038236647844315, 'ratio': 0.7578125},\n",
       " 128: {'grad_norm': 11.134035170078278, 'ratio': 0.74609375},\n",
       " 129: {'grad_norm': 11.230251759290695, 'ratio': 0.71875},\n",
       " 130: {'grad_norm': 11.32653722167015, 'ratio': 0.74609375},\n",
       " 131: {'grad_norm': 11.423258125782013, 'ratio': 0.75},\n",
       " 132: {'grad_norm': 11.520293653011322, 'ratio': 0.75390625},\n",
       " 133: {'grad_norm': 11.617722243070602, 'ratio': 0.76171875},\n",
       " 134: {'grad_norm': 11.71555957198143, 'ratio': 0.73828125},\n",
       " 135: {'grad_norm': 11.813597410917282, 'ratio': 0.72265625},\n",
       " 136: {'grad_norm': 11.911925315856934, 'ratio': 0.75},\n",
       " 137: {'grad_norm': 12.010816246271133, 'ratio': 0.73828125},\n",
       " 138: {'grad_norm': 12.110027819871902, 'ratio': 0.76171875},\n",
       " 139: {'grad_norm': 12.209516555070877, 'ratio': 0.75},\n",
       " 140: {'grad_norm': 12.30949580669403, 'ratio': 0.74609375},\n",
       " 141: {'grad_norm': 12.409775644540787, 'ratio': 0.75},\n",
       " 142: {'grad_norm': 12.510366827249527, 'ratio': 0.73828125},\n",
       " 143: {'grad_norm': 12.611181557178497, 'ratio': 0.765625},\n",
       " 144: {'grad_norm': 12.712216943502426, 'ratio': 0.75390625},\n",
       " 145: {'grad_norm': 12.813827455043793, 'ratio': 0.74609375},\n",
       " 146: {'grad_norm': 12.916087299585342, 'ratio': 0.76171875},\n",
       " 147: {'grad_norm': 13.018637746572495, 'ratio': 0.74609375},\n",
       " 148: {'grad_norm': 13.12119272351265, 'ratio': 0.7421875},\n",
       " 149: {'grad_norm': 13.224268287420273, 'ratio': 0.7421875},\n",
       " 150: {'grad_norm': 13.327939927577972, 'ratio': 0.76953125},\n",
       " 151: {'grad_norm': 13.432052582502365, 'ratio': 0.74609375},\n",
       " 152: {'grad_norm': 13.536457061767578, 'ratio': 0.73828125},\n",
       " 153: {'grad_norm': 13.64093890786171, 'ratio': 0.734375},\n",
       " 154: {'grad_norm': 13.745855450630188, 'ratio': 0.7578125},\n",
       " 155: {'grad_norm': 13.851466089487076, 'ratio': 0.7578125},\n",
       " 156: {'grad_norm': 13.957255899906158, 'ratio': 0.7421875},\n",
       " 157: {'grad_norm': 14.063598096370697, 'ratio': 0.75390625},\n",
       " 158: {'grad_norm': 14.170308500528336, 'ratio': 0.7578125},\n",
       " 159: {'grad_norm': 14.277278244495392, 'ratio': 0.74609375},\n",
       " 160: {'grad_norm': 14.384952247142792, 'ratio': 0.78515625},\n",
       " 161: {'grad_norm': 14.492628991603851, 'ratio': 0.7578125},\n",
       " 162: {'grad_norm': 14.600599527359009, 'ratio': 0.74609375},\n",
       " 163: {'grad_norm': 14.709129184484482, 'ratio': 0.75390625},\n",
       " 164: {'grad_norm': 14.818209707736969, 'ratio': 0.73828125},\n",
       " 165: {'grad_norm': 14.927930563688278, 'ratio': 0.75390625},\n",
       " 166: {'grad_norm': 15.03765320777893, 'ratio': 0.734375},\n",
       " 167: {'grad_norm': 15.147936016321182, 'ratio': 0.73828125},\n",
       " 168: {'grad_norm': 15.2585389316082, 'ratio': 0.75390625},\n",
       " 169: {'grad_norm': 15.36992958188057, 'ratio': 0.74609375},\n",
       " 170: {'grad_norm': 15.481323331594467, 'ratio': 0.765625},\n",
       " 171: {'grad_norm': 15.593197464942932, 'ratio': 0.74609375},\n",
       " 172: {'grad_norm': 15.705587655305862, 'ratio': 0.765625},\n",
       " 173: {'grad_norm': 15.818132847547531, 'ratio': 0.7578125},\n",
       " 174: {'grad_norm': 15.93162226676941, 'ratio': 0.75},\n",
       " 175: {'grad_norm': 16.045261293649673, 'ratio': 0.7578125},\n",
       " 176: {'grad_norm': 16.15961766242981, 'ratio': 0.7734375},\n",
       " 177: {'grad_norm': 16.27416580915451, 'ratio': 0.76953125},\n",
       " 178: {'grad_norm': 16.389291405677795, 'ratio': 0.7734375},\n",
       " 179: {'grad_norm': 16.504624485969543, 'ratio': 0.74609375},\n",
       " 180: {'grad_norm': 16.62038028240204, 'ratio': 0.75},\n",
       " 181: {'grad_norm': 16.736924946308136, 'ratio': 0.7578125},\n",
       " 182: {'grad_norm': 16.853754222393036, 'ratio': 0.765625},\n",
       " 183: {'grad_norm': 16.970906257629395, 'ratio': 0.7734375},\n",
       " 184: {'grad_norm': 17.088582396507263, 'ratio': 0.7734375},\n",
       " 185: {'grad_norm': 17.206425666809082, 'ratio': 0.75390625},\n",
       " 186: {'grad_norm': 17.325096666812897, 'ratio': 0.75390625},\n",
       " 187: {'grad_norm': 17.444326043128967, 'ratio': 0.75390625},\n",
       " 188: {'grad_norm': 17.563884019851685, 'ratio': 0.7578125},\n",
       " 189: {'grad_norm': 17.683681547641754, 'ratio': 0.7734375},\n",
       " 190: {'grad_norm': 17.8039191365242, 'ratio': 0.73828125},\n",
       " 191: {'grad_norm': 17.924633860588074, 'ratio': 0.765625},\n",
       " 192: {'grad_norm': 18.04622209072113, 'ratio': 0.76171875},\n",
       " 193: {'grad_norm': 18.168228566646576, 'ratio': 0.765625},\n",
       " 194: {'grad_norm': 18.290478348731995, 'ratio': 0.7578125},\n",
       " 195: {'grad_norm': 18.412890255451202, 'ratio': 0.75390625},\n",
       " 196: {'grad_norm': 18.53627586364746, 'ratio': 0.7265625},\n",
       " 197: {'grad_norm': 18.660108625888824, 'ratio': 0.75390625},\n",
       " 198: {'grad_norm': 18.784533262252808, 'ratio': 0.7265625},\n",
       " 199: {'grad_norm': 18.909347355365753, 'ratio': 0.74609375},\n",
       " 200: {'grad_norm': 19.03425794839859, 'ratio': 0.765625},\n",
       " 201: {'grad_norm': 19.159932672977448, 'ratio': 0.765625},\n",
       " 202: {'grad_norm': 19.286004841327667, 'ratio': 0.75},\n",
       " 203: {'grad_norm': 19.412811994552612, 'ratio': 0.75},\n",
       " 204: {'grad_norm': 19.53963816165924, 'ratio': 0.75},\n",
       " 205: {'grad_norm': 19.667536318302155, 'ratio': 0.75390625},\n",
       " 206: {'grad_norm': 19.795384526252747, 'ratio': 0.765625},\n",
       " 207: {'grad_norm': 19.923970103263855, 'ratio': 0.76953125},\n",
       " 208: {'grad_norm': 20.05348688364029, 'ratio': 0.75390625},\n",
       " 209: {'grad_norm': 20.182883977890015, 'ratio': 0.7578125},\n",
       " 210: {'grad_norm': 20.313156366348267, 'ratio': 0.75390625},\n",
       " 211: {'grad_norm': 20.443967163562775, 'ratio': 0.76953125},\n",
       " 212: {'grad_norm': 20.57516449689865, 'ratio': 0.76171875},\n",
       " 213: {'grad_norm': 20.706835508346558, 'ratio': 0.7734375},\n",
       " 214: {'grad_norm': 20.838925182819366, 'ratio': 0.74609375},\n",
       " 215: {'grad_norm': 20.97147649526596, 'ratio': 0.765625},\n",
       " 216: {'grad_norm': 21.104323029518127, 'ratio': 0.74609375},\n",
       " 217: {'grad_norm': 21.23850190639496, 'ratio': 0.76171875},\n",
       " 218: {'grad_norm': 21.372909784317017, 'ratio': 0.75},\n",
       " 219: {'grad_norm': 21.507905304431915, 'ratio': 0.734375},\n",
       " 220: {'grad_norm': 21.642957866191864, 'ratio': 0.74609375},\n",
       " 221: {'grad_norm': 21.77872908115387, 'ratio': 0.75},\n",
       " 222: {'grad_norm': 21.915141582489014, 'ratio': 0.74609375},\n",
       " 223: {'grad_norm': 22.052405953407288, 'ratio': 0.7421875},\n",
       " 224: {'grad_norm': 22.19037163257599, 'ratio': 0.75},\n",
       " 225: {'grad_norm': 22.327941119670868, 'ratio': 0.76171875},\n",
       " 226: {'grad_norm': 22.466145873069763, 'ratio': 0.75390625},\n",
       " 227: {'grad_norm': 22.604863345623016, 'ratio': 0.74609375},\n",
       " 228: {'grad_norm': 22.7449631690979, 'ratio': 0.7734375},\n",
       " 229: {'grad_norm': 22.88552051782608, 'ratio': 0.75390625},\n",
       " 230: {'grad_norm': 23.02625221014023, 'ratio': 0.765625},\n",
       " 231: {'grad_norm': 23.167146027088165, 'ratio': 0.7734375},\n",
       " 232: {'grad_norm': 23.308553636074066, 'ratio': 0.76171875},\n",
       " 233: {'grad_norm': 23.451205790042877, 'ratio': 0.765625},\n",
       " 234: {'grad_norm': 23.59380578994751, 'ratio': 0.7734375},\n",
       " 235: {'grad_norm': 23.737947285175323, 'ratio': 0.76171875},\n",
       " 236: {'grad_norm': 23.88148272037506, 'ratio': 0.76953125},\n",
       " 237: {'grad_norm': 24.025334239006042, 'ratio': 0.75},\n",
       " 238: {'grad_norm': 24.170790195465088, 'ratio': 0.78125},\n",
       " 239: {'grad_norm': 24.31643670797348, 'ratio': 0.76953125},\n",
       " 240: {'grad_norm': 24.46280688047409, 'ratio': 0.75},\n",
       " 241: {'grad_norm': 24.609951555728912, 'ratio': 0.76171875},\n",
       " 242: {'grad_norm': 24.757156789302826, 'ratio': 0.78125},\n",
       " 243: {'grad_norm': 24.90558785200119, 'ratio': 0.76171875},\n",
       " 244: {'grad_norm': 25.053687632083893, 'ratio': 0.7734375},\n",
       " 245: {'grad_norm': 25.202625393867493, 'ratio': 0.75},\n",
       " 246: {'grad_norm': 25.35180902481079, 'ratio': 0.7890625},\n",
       " 247: {'grad_norm': 25.50300705432892, 'ratio': 0.76171875},\n",
       " 248: {'grad_norm': 25.653994917869568, 'ratio': 0.74609375},\n",
       " 249: {'grad_norm': 25.805219769477844, 'ratio': 0.75},\n",
       " 250: {'grad_norm': 25.956881999969482, 'ratio': 0.76953125},\n",
       " 251: {'grad_norm': 26.109384059906006, 'ratio': 0.75},\n",
       " 252: {'grad_norm': 26.262591004371643, 'ratio': 0.77734375},\n",
       " 253: {'grad_norm': 26.417019724845886, 'ratio': 0.765625},\n",
       " 254: {'grad_norm': 26.571117043495178, 'ratio': 0.7734375},\n",
       " 255: {'grad_norm': 26.72601592540741, 'ratio': 0.76953125},\n",
       " 256: {'grad_norm': 26.88122820854187, 'ratio': 0.7421875},\n",
       " 257: {'grad_norm': 27.037115573883057, 'ratio': 0.75},\n",
       " 258: {'grad_norm': 27.19456446170807, 'ratio': 0.765625},\n",
       " 259: {'grad_norm': 27.351747393608093, 'ratio': 0.76953125},\n",
       " 260: {'grad_norm': 27.509708166122437, 'ratio': 0.765625},\n",
       " 261: {'grad_norm': 27.667835235595703, 'ratio': 0.7578125},\n",
       " 262: {'grad_norm': 27.826779961586, 'ratio': 0.78515625},\n",
       " 263: {'grad_norm': 27.986095666885376, 'ratio': 0.74609375},\n",
       " 264: {'grad_norm': 28.146803498268127, 'ratio': 0.7578125},\n",
       " 265: {'grad_norm': 28.30738341808319, 'ratio': 0.7578125},\n",
       " 266: {'grad_norm': 28.46953284740448, 'ratio': 0.7734375},\n",
       " 267: {'grad_norm': 28.631513595581055, 'ratio': 0.77734375},\n",
       " 268: {'grad_norm': 28.79410672187805, 'ratio': 0.75},\n",
       " 269: {'grad_norm': 28.957695960998535, 'ratio': 0.74609375},\n",
       " 270: {'grad_norm': 29.122185945510864, 'ratio': 0.73828125},\n",
       " 271: {'grad_norm': 29.28665041923523, 'ratio': 0.76171875},\n",
       " 272: {'grad_norm': 29.452451825141907, 'ratio': 0.75390625},\n",
       " 273: {'grad_norm': 29.61804223060608, 'ratio': 0.7578125},\n",
       " 274: {'grad_norm': 29.784618139266968, 'ratio': 0.7734375},\n",
       " 275: {'grad_norm': 29.951084971427917, 'ratio': 0.76953125},\n",
       " 276: {'grad_norm': 30.118988156318665, 'ratio': 0.7578125},\n",
       " 277: {'grad_norm': 30.287484526634216, 'ratio': 0.7734375},\n",
       " 278: {'grad_norm': 30.4575617313385, 'ratio': 0.7578125},\n",
       " 279: {'grad_norm': 30.627432703971863, 'ratio': 0.76953125},\n",
       " 280: {'grad_norm': 30.7976633310318, 'ratio': 0.80078125},\n",
       " 281: {'grad_norm': 30.96788203716278, 'ratio': 0.76171875},\n",
       " 282: {'grad_norm': 31.139859557151794, 'ratio': 0.7734375},\n",
       " 283: {'grad_norm': 31.312097907066345, 'ratio': 0.75390625},\n",
       " 284: {'grad_norm': 31.4859778881073, 'ratio': 0.796875},\n",
       " 285: {'grad_norm': 31.65953528881073, 'ratio': 0.7890625},\n",
       " 286: {'grad_norm': 31.832794547080994, 'ratio': 0.76171875},\n",
       " 287: {'grad_norm': 32.00792992115021, 'ratio': 0.7734375},\n",
       " 288: {'grad_norm': 32.18345582485199, 'ratio': 0.76171875},\n",
       " 289: {'grad_norm': 32.35963523387909, 'ratio': 0.76171875},\n",
       " 290: {'grad_norm': 32.5371755361557, 'ratio': 0.75},\n",
       " 291: {'grad_norm': 32.71473002433777, 'ratio': 0.75},\n",
       " 292: {'grad_norm': 32.89196443557739, 'ratio': 0.75390625},\n",
       " 293: {'grad_norm': 33.07160496711731, 'ratio': 0.765625},\n",
       " 294: {'grad_norm': 33.25058472156525, 'ratio': 0.765625},\n",
       " 295: {'grad_norm': 33.431074142456055, 'ratio': 0.78515625},\n",
       " 296: {'grad_norm': 33.611759662628174, 'ratio': 0.7734375},\n",
       " 297: {'grad_norm': 33.793330669403076, 'ratio': 0.7890625},\n",
       " 298: {'grad_norm': 33.975273966789246, 'ratio': 0.7578125},\n",
       " 299: {'grad_norm': 34.1580536365509, 'ratio': 0.75390625},\n",
       " 300: {'grad_norm': 34.34217381477356, 'ratio': 0.78125},\n",
       " 301: {'grad_norm': 34.526695251464844, 'ratio': 0.765625},\n",
       " 302: {'grad_norm': 34.711416244506836, 'ratio': 0.7734375},\n",
       " 303: {'grad_norm': 34.89703023433685, 'ratio': 0.75390625},\n",
       " 304: {'grad_norm': 35.08287703990936, 'ratio': 0.7421875},\n",
       " 305: {'grad_norm': 35.26929593086243, 'ratio': 0.76171875},\n",
       " 306: {'grad_norm': 35.457539796829224, 'ratio': 0.73828125},\n",
       " 307: {'grad_norm': 35.64583492279053, 'ratio': 0.75390625},\n",
       " 308: {'grad_norm': 35.83448910713196, 'ratio': 0.7734375},\n",
       " 309: {'grad_norm': 36.02384388446808, 'ratio': 0.73828125},\n",
       " 310: {'grad_norm': 36.21467399597168, 'ratio': 0.7421875},\n",
       " 311: {'grad_norm': 36.405001163482666, 'ratio': 0.76171875},\n",
       " 312: {'grad_norm': 36.597424149513245, 'ratio': 0.7578125},\n",
       " 313: {'grad_norm': 36.78995382785797, 'ratio': 0.74609375},\n",
       " 314: {'grad_norm': 36.98275077342987, 'ratio': 0.75390625},\n",
       " 315: {'grad_norm': 37.17578172683716, 'ratio': 0.765625},\n",
       " 316: {'grad_norm': 37.3707093000412, 'ratio': 0.765625},\n",
       " 317: {'grad_norm': 37.56574988365173, 'ratio': 0.7578125},\n",
       " 318: {'grad_norm': 37.761948227882385, 'ratio': 0.7734375},\n",
       " 319: {'grad_norm': 37.958500266075134, 'ratio': 0.75390625},\n",
       " 320: {'grad_norm': 38.15461277961731, 'ratio': 0.7734375},\n",
       " 321: {'grad_norm': 38.35251784324646, 'ratio': 0.76171875},\n",
       " 322: {'grad_norm': 38.55046081542969, 'ratio': 0.7578125},\n",
       " 323: {'grad_norm': 38.75088107585907, 'ratio': 0.765625},\n",
       " 324: {'grad_norm': 38.95126914978027, 'ratio': 0.765625},\n",
       " 325: {'grad_norm': 39.1523163318634, 'ratio': 0.7890625},\n",
       " 326: {'grad_norm': 39.35311758518219, 'ratio': 0.76171875},\n",
       " 327: {'grad_norm': 39.5550080537796, 'ratio': 0.765625},\n",
       " 328: {'grad_norm': 39.75765287876129, 'ratio': 0.76953125},\n",
       " 329: {'grad_norm': 39.96140432357788, 'ratio': 0.7734375},\n",
       " 330: {'grad_norm': 40.164998292922974, 'ratio': 0.765625},\n",
       " 331: {'grad_norm': 40.37116885185242, 'ratio': 0.78515625},\n",
       " 332: {'grad_norm': 40.57639479637146, 'ratio': 0.76171875},\n",
       " 333: {'grad_norm': 40.78210461139679, 'ratio': 0.7734375},\n",
       " 334: {'grad_norm': 40.98922872543335, 'ratio': 0.7421875},\n",
       " 335: {'grad_norm': 41.1967476606369, 'ratio': 0.76953125},\n",
       " 336: {'grad_norm': 41.40601968765259, 'ratio': 0.79296875},\n",
       " 337: {'grad_norm': 41.615853786468506, 'ratio': 0.76953125},\n",
       " 338: {'grad_norm': 41.82501554489136, 'ratio': 0.76953125},\n",
       " 339: {'grad_norm': 42.0361692905426, 'ratio': 0.78125},\n",
       " 340: {'grad_norm': 42.24769401550293, 'ratio': 0.7578125},\n",
       " 341: {'grad_norm': 42.459651470184326, 'ratio': 0.77734375},\n",
       " 342: {'grad_norm': 42.672627449035645, 'ratio': 0.765625},\n",
       " 343: {'grad_norm': 42.88690543174744, 'ratio': 0.76953125},\n",
       " 344: {'grad_norm': 43.101139545440674, 'ratio': 0.76171875},\n",
       " 345: {'grad_norm': 43.31584692001343, 'ratio': 0.765625},\n",
       " 346: {'grad_norm': 43.531800508499146, 'ratio': 0.78125},\n",
       " 347: {'grad_norm': 43.748088359832764, 'ratio': 0.7421875},\n",
       " 348: {'grad_norm': 43.9651415348053, 'ratio': 0.7578125},\n",
       " 349: {'grad_norm': 44.18348789215088, 'ratio': 0.78125},\n",
       " 350: {'grad_norm': 44.40266466140747, 'ratio': 0.765625},\n",
       " 351: {'grad_norm': 44.62206697463989, 'ratio': 0.75390625},\n",
       " 352: {'grad_norm': 44.84123253822327, 'ratio': 0.80859375},\n",
       " 353: {'grad_norm': 45.06248474121094, 'ratio': 0.7734375},\n",
       " 354: {'grad_norm': 45.284648180007935, 'ratio': 0.765625},\n",
       " 355: {'grad_norm': 45.50638818740845, 'ratio': 0.76171875},\n",
       " 356: {'grad_norm': 45.73100233078003, 'ratio': 0.765625},\n",
       " 357: {'grad_norm': 45.954872846603394, 'ratio': 0.74609375},\n",
       " 358: {'grad_norm': 46.17931509017944, 'ratio': 0.75390625},\n",
       " 359: {'grad_norm': 46.40478491783142, 'ratio': 0.77734375},\n",
       " 360: {'grad_norm': 46.63083624839783, 'ratio': 0.74609375},\n",
       " 361: {'grad_norm': 46.85735464096069, 'ratio': 0.76953125},\n",
       " 362: {'grad_norm': 47.08517265319824, 'ratio': 0.7890625},\n",
       " 363: {'grad_norm': 47.315747022628784, 'ratio': 0.77734375},\n",
       " 364: {'grad_norm': 47.544921875, 'ratio': 0.76171875},\n",
       " 365: {'grad_norm': 47.7750985622406, 'ratio': 0.76953125},\n",
       " 366: {'grad_norm': 48.00611090660095, 'ratio': 0.7421875},\n",
       " 367: {'grad_norm': 48.23690342903137, 'ratio': 0.76953125},\n",
       " 368: {'grad_norm': 48.46934962272644, 'ratio': 0.765625},\n",
       " 369: {'grad_norm': 48.702821254730225, 'ratio': 0.75},\n",
       " 370: {'grad_norm': 48.93725299835205, 'ratio': 0.765625},\n",
       " 371: {'grad_norm': 49.17164993286133, 'ratio': 0.7578125},\n",
       " 372: {'grad_norm': 49.40685796737671, 'ratio': 0.77734375},\n",
       " 373: {'grad_norm': 49.642576456069946, 'ratio': 0.73046875},\n",
       " 374: {'grad_norm': 49.87927174568176, 'ratio': 0.75390625},\n",
       " 375: {'grad_norm': 50.117311239242554, 'ratio': 0.73046875},\n",
       " 376: {'grad_norm': 50.35818576812744, 'ratio': 0.73046875},\n",
       " 377: {'grad_norm': 50.59649991989136, 'ratio': 0.76171875},\n",
       " 378: {'grad_norm': 50.83659648895264, 'ratio': 0.7578125},\n",
       " 379: {'grad_norm': 51.077489137649536, 'ratio': 0.7578125},\n",
       " 380: {'grad_norm': 51.31845188140869, 'ratio': 0.7578125},\n",
       " 381: {'grad_norm': 51.56025958061218, 'ratio': 0.76171875},\n",
       " 382: {'grad_norm': 51.804028034210205, 'ratio': 0.75},\n",
       " 383: {'grad_norm': 52.04923129081726, 'ratio': 0.76171875},\n",
       " 384: {'grad_norm': 52.293827056884766, 'ratio': 0.77734375},\n",
       " 385: {'grad_norm': 52.53936219215393, 'ratio': 0.7578125},\n",
       " 386: {'grad_norm': 52.78448987007141, 'ratio': 0.765625},\n",
       " 387: {'grad_norm': 53.032045125961304, 'ratio': 0.7421875},\n",
       " 388: {'grad_norm': 53.280744552612305, 'ratio': 0.765625},\n",
       " 389: {'grad_norm': 53.52993321418762, 'ratio': 0.76171875},\n",
       " 390: {'grad_norm': 53.78042936325073, 'ratio': 0.76953125},\n",
       " 391: {'grad_norm': 54.03025841712952, 'ratio': 0.734375},\n",
       " 392: {'grad_norm': 54.28094720840454, 'ratio': 0.76171875},\n",
       " 393: {'grad_norm': 54.53248167037964, 'ratio': 0.78125},\n",
       " 394: {'grad_norm': 54.78584146499634, 'ratio': 0.74609375},\n",
       " 395: {'grad_norm': 55.04027986526489, 'ratio': 0.75390625},\n",
       " 396: {'grad_norm': 55.294599771499634, 'ratio': 0.7734375},\n",
       " 397: {'grad_norm': 55.550779819488525, 'ratio': 0.78125},\n",
       " 398: {'grad_norm': 55.806273221969604, 'ratio': 0.75},\n",
       " 399: {'grad_norm': 56.061766147613525, 'ratio': 0.76953125},\n",
       " 400: {'grad_norm': 56.32023644447327, 'ratio': 0.7578125},\n",
       " 401: {'grad_norm': 56.57847046852112, 'ratio': 0.765625},\n",
       " 402: {'grad_norm': 56.836925983428955, 'ratio': 0.73828125},\n",
       " 403: {'grad_norm': 57.09808135032654, 'ratio': 0.765625},\n",
       " 404: {'grad_norm': 57.3594012260437, 'ratio': 0.7734375},\n",
       " 405: {'grad_norm': 57.62058424949646, 'ratio': 0.75},\n",
       " 406: {'grad_norm': 57.883185625076294, 'ratio': 0.76171875},\n",
       " 407: {'grad_norm': 58.147400856018066, 'ratio': 0.76171875},\n",
       " 408: {'grad_norm': 58.41097831726074, 'ratio': 0.7734375},\n",
       " 409: {'grad_norm': 58.675878286361694, 'ratio': 0.76171875},\n",
       " 410: {'grad_norm': 58.94222044944763, 'ratio': 0.75390625},\n",
       " 411: {'grad_norm': 59.20985126495361, 'ratio': 0.7734375},\n",
       " 412: {'grad_norm': 59.47554850578308, 'ratio': 0.765625},\n",
       " 413: {'grad_norm': 59.7449209690094, 'ratio': 0.7734375},\n",
       " 414: {'grad_norm': 60.013166666030884, 'ratio': 0.75390625},\n",
       " 415: {'grad_norm': 60.283287048339844, 'ratio': 0.76171875},\n",
       " 416: {'grad_norm': 60.55221366882324, 'ratio': 0.765625},\n",
       " 417: {'grad_norm': 60.82420635223389, 'ratio': 0.75},\n",
       " 418: {'grad_norm': 61.097697496414185, 'ratio': 0.75},\n",
       " 419: {'grad_norm': 61.371363162994385, 'ratio': 0.76953125},\n",
       " 420: {'grad_norm': 61.645644664764404, 'ratio': 0.7421875},\n",
       " 421: {'grad_norm': 61.91996765136719, 'ratio': 0.76953125},\n",
       " 422: {'grad_norm': 62.19692015647888, 'ratio': 0.765625},\n",
       " 423: {'grad_norm': 62.473769426345825, 'ratio': 0.74609375},\n",
       " 424: {'grad_norm': 62.75059986114502, 'ratio': 0.73828125},\n",
       " 425: {'grad_norm': 63.0295844078064, 'ratio': 0.76171875},\n",
       " 426: {'grad_norm': 63.30950951576233, 'ratio': 0.76953125},\n",
       " 427: {'grad_norm': 63.58990144729614, 'ratio': 0.76171875},\n",
       " 428: {'grad_norm': 63.86988687515259, 'ratio': 0.7578125},\n",
       " 429: {'grad_norm': 64.1509850025177, 'ratio': 0.7578125},\n",
       " 430: {'grad_norm': 64.4333107471466, 'ratio': 0.77734375},\n",
       " 431: {'grad_norm': 64.71645212173462, 'ratio': 0.76953125},\n",
       " 432: {'grad_norm': 64.99931836128235, 'ratio': 0.78125},\n",
       " 433: {'grad_norm': 65.28708052635193, 'ratio': 0.7734375},\n",
       " 434: {'grad_norm': 65.57257390022278, 'ratio': 0.76953125},\n",
       " 435: {'grad_norm': 65.85958743095398, 'ratio': 0.765625},\n",
       " 436: {'grad_norm': 66.14703631401062, 'ratio': 0.78515625},\n",
       " 437: {'grad_norm': 66.43601369857788, 'ratio': 0.77734375},\n",
       " 438: {'grad_norm': 66.72554564476013, 'ratio': 0.77734375},\n",
       " 439: {'grad_norm': 67.01695466041565, 'ratio': 0.75390625},\n",
       " 440: {'grad_norm': 67.30804824829102, 'ratio': 0.75390625},\n",
       " 441: {'grad_norm': 67.60087466239929, 'ratio': 0.77734375},\n",
       " 442: {'grad_norm': 67.89265775680542, 'ratio': 0.76171875},\n",
       " 443: {'grad_norm': 68.18528914451599, 'ratio': 0.75},\n",
       " 444: {'grad_norm': 68.47947454452515, 'ratio': 0.79296875},\n",
       " 445: {'grad_norm': 68.7748351097107, 'ratio': 0.78125},\n",
       " 446: {'grad_norm': 69.07330465316772, 'ratio': 0.765625},\n",
       " 447: {'grad_norm': 69.36953115463257, 'ratio': 0.76953125},\n",
       " 448: {'grad_norm': 69.66914892196655, 'ratio': 0.76171875},\n",
       " 449: {'grad_norm': 69.96776247024536, 'ratio': 0.76171875},\n",
       " 450: {'grad_norm': 70.26648330688477, 'ratio': 0.76171875},\n",
       " 451: {'grad_norm': 70.56741905212402, 'ratio': 0.765625},\n",
       " 452: {'grad_norm': 70.87000036239624, 'ratio': 0.7734375},\n",
       " 453: {'grad_norm': 71.17323684692383, 'ratio': 0.7734375},\n",
       " 454: {'grad_norm': 71.47639322280884, 'ratio': 0.76171875},\n",
       " 455: {'grad_norm': 71.78211545944214, 'ratio': 0.765625},\n",
       " 456: {'grad_norm': 72.08731317520142, 'ratio': 0.75},\n",
       " 457: {'grad_norm': 72.39252281188965, 'ratio': 0.76953125},\n",
       " 458: {'grad_norm': 72.69874620437622, 'ratio': 0.77734375},\n",
       " 459: {'grad_norm': 73.00729942321777, 'ratio': 0.74609375},\n",
       " 460: {'grad_norm': 73.3165602684021, 'ratio': 0.75390625},\n",
       " 461: {'grad_norm': 73.62639331817627, 'ratio': 0.78515625},\n",
       " 462: {'grad_norm': 73.9365782737732, 'ratio': 0.75390625},\n",
       " 463: {'grad_norm': 74.25089597702026, 'ratio': 0.765625},\n",
       " 464: {'grad_norm': 74.5626540184021, 'ratio': 0.76171875},\n",
       " 465: {'grad_norm': 74.87406158447266, 'ratio': 0.765625},\n",
       " 466: {'grad_norm': 75.18792247772217, 'ratio': 0.76171875},\n",
       " 467: {'grad_norm': 75.50291681289673, 'ratio': 0.7578125},\n",
       " 468: {'grad_norm': 75.8198561668396, 'ratio': 0.75390625},\n",
       " 469: {'grad_norm': 76.13807535171509, 'ratio': 0.7578125},\n",
       " 470: {'grad_norm': 76.4561710357666, 'ratio': 0.74609375},\n",
       " 471: {'grad_norm': 76.77551746368408, 'ratio': 0.77734375},\n",
       " 472: {'grad_norm': 77.0955138206482, 'ratio': 0.734375},\n",
       " 473: {'grad_norm': 77.41522884368896, 'ratio': 0.76953125},\n",
       " 474: {'grad_norm': 77.73590469360352, 'ratio': 0.75390625},\n",
       " 475: {'grad_norm': 78.05776977539062, 'ratio': 0.75},\n",
       " 476: {'grad_norm': 78.38155364990234, 'ratio': 0.78125},\n",
       " 477: {'grad_norm': 78.70490074157715, 'ratio': 0.7734375},\n",
       " 478: {'grad_norm': 79.03034973144531, 'ratio': 0.765625},\n",
       " 479: {'grad_norm': 79.35782814025879, 'ratio': 0.76953125},\n",
       " 480: {'grad_norm': 79.68462562561035, 'ratio': 0.76953125},\n",
       " 481: {'grad_norm': 80.01269626617432, 'ratio': 0.7578125},\n",
       " 482: {'grad_norm': 80.34200429916382, 'ratio': 0.7578125},\n",
       " 483: {'grad_norm': 80.66897106170654, 'ratio': 0.76171875},\n",
       " 484: {'grad_norm': 81.00171041488647, 'ratio': 0.765625},\n",
       " 485: {'grad_norm': 81.33387994766235, 'ratio': 0.76171875},\n",
       " 486: {'grad_norm': 81.6668610572815, 'ratio': 0.75},\n",
       " 487: {'grad_norm': 82.00052261352539, 'ratio': 0.75390625},\n",
       " 488: {'grad_norm': 82.3354926109314, 'ratio': 0.75390625},\n",
       " 489: {'grad_norm': 82.67068529129028, 'ratio': 0.7578125},\n",
       " 490: {'grad_norm': 83.00695753097534, 'ratio': 0.7578125},\n",
       " 491: {'grad_norm': 83.34126091003418, 'ratio': 0.75390625},\n",
       " 492: {'grad_norm': 83.67936611175537, 'ratio': 0.76953125},\n",
       " 493: {'grad_norm': 84.01873588562012, 'ratio': 0.76171875},\n",
       " 494: {'grad_norm': 84.36116170883179, 'ratio': 0.78125},\n",
       " 495: {'grad_norm': 84.70337104797363, 'ratio': 0.76953125},\n",
       " 496: {'grad_norm': 85.04441165924072, 'ratio': 0.7734375},\n",
       " 497: {'grad_norm': 85.38787841796875, 'ratio': 0.7734375},\n",
       " 498: {'grad_norm': 85.72995281219482, 'ratio': 0.7734375},\n",
       " 499: {'grad_norm': 86.07561540603638, 'ratio': 0.7578125},\n",
       " 500: {'grad_norm': 86.41982746124268, 'ratio': 0.78125},\n",
       " 501: {'grad_norm': 86.76539182662964, 'ratio': 0.76171875},\n",
       " 502: {'grad_norm': 87.11550903320312, 'ratio': 0.76953125},\n",
       " 503: {'grad_norm': 87.46610021591187, 'ratio': 0.7421875},\n",
       " 504: {'grad_norm': 87.81596946716309, 'ratio': 0.765625},\n",
       " 505: {'grad_norm': 88.1643295288086, 'ratio': 0.7734375},\n",
       " 506: {'grad_norm': 88.51622009277344, 'ratio': 0.75390625},\n",
       " 507: {'grad_norm': 88.86709642410278, 'ratio': 0.76171875},\n",
       " 508: {'grad_norm': 89.22138261795044, 'ratio': 0.734375},\n",
       " 509: {'grad_norm': 89.5753846168518, 'ratio': 0.7578125},\n",
       " 510: {'grad_norm': 89.93153285980225, 'ratio': 0.76953125},\n",
       " 511: {'grad_norm': 90.28785610198975, 'ratio': 0.80859375},\n",
       " 512: {'grad_norm': 90.64451932907104, 'ratio': 0.77734375},\n",
       " 513: {'grad_norm': 91.00183391571045, 'ratio': 0.7578125},\n",
       " 514: {'grad_norm': 91.36082315444946, 'ratio': 0.73828125},\n",
       " 515: {'grad_norm': 91.71992826461792, 'ratio': 0.75},\n",
       " 516: {'grad_norm': 92.0806474685669, 'ratio': 0.7578125},\n",
       " 517: {'grad_norm': 92.44416093826294, 'ratio': 0.78125},\n",
       " 518: {'grad_norm': 92.80810737609863, 'ratio': 0.78515625},\n",
       " 519: {'grad_norm': 93.17037725448608, 'ratio': 0.78125},\n",
       " 520: {'grad_norm': 93.53567457199097, 'ratio': 0.7421875},\n",
       " 521: {'grad_norm': 93.9020562171936, 'ratio': 0.7734375},\n",
       " 522: {'grad_norm': 94.26903200149536, 'ratio': 0.7734375},\n",
       " 523: {'grad_norm': 94.63700723648071, 'ratio': 0.76171875},\n",
       " 524: {'grad_norm': 95.0063066482544, 'ratio': 0.7890625},\n",
       " 525: {'grad_norm': 95.37580108642578, 'ratio': 0.7578125},\n",
       " 526: {'grad_norm': 95.74838733673096, 'ratio': 0.78515625},\n",
       " 527: {'grad_norm': 96.12052011489868, 'ratio': 0.73828125},\n",
       " 528: {'grad_norm': 96.49256134033203, 'ratio': 0.76953125},\n",
       " 529: {'grad_norm': 96.86475992202759, 'ratio': 0.765625},\n",
       " 530: {'grad_norm': 97.23907995223999, 'ratio': 0.76171875},\n",
       " 531: {'grad_norm': 97.61397123336792, 'ratio': 0.76953125},\n",
       " 532: {'grad_norm': 97.99065113067627, 'ratio': 0.76953125},\n",
       " 533: {'grad_norm': 98.36965370178223, 'ratio': 0.7734375},\n",
       " 534: {'grad_norm': 98.74747514724731, 'ratio': 0.78125},\n",
       " 535: {'grad_norm': 99.12949275970459, 'ratio': 0.77734375},\n",
       " 536: {'grad_norm': 99.51133680343628, 'ratio': 0.75390625},\n",
       " 537: {'grad_norm': 99.89262199401855, 'ratio': 0.7578125},\n",
       " 538: {'grad_norm': 100.27545738220215, 'ratio': 0.7734375},\n",
       " 539: {'grad_norm': 100.65786981582642, 'ratio': 0.7578125},\n",
       " 540: {'grad_norm': 101.04189682006836, 'ratio': 0.76171875},\n",
       " 541: {'grad_norm': 101.42860555648804, 'ratio': 0.76953125},\n",
       " 542: {'grad_norm': 101.81463575363159, 'ratio': 0.78515625},\n",
       " 543: {'grad_norm': 102.20320320129395, 'ratio': 0.7734375},\n",
       " 544: {'grad_norm': 102.59220266342163, 'ratio': 0.75},\n",
       " 545: {'grad_norm': 102.98156547546387, 'ratio': 0.78515625},\n",
       " 546: {'grad_norm': 103.36953687667847, 'ratio': 0.78125},\n",
       " 547: {'grad_norm': 103.76135396957397, 'ratio': 0.76953125},\n",
       " 548: {'grad_norm': 104.15408277511597, 'ratio': 0.74609375},\n",
       " 549: {'grad_norm': 104.54661703109741, 'ratio': 0.765625},\n",
       " 550: {'grad_norm': 104.94231224060059, 'ratio': 0.7578125},\n",
       " 551: {'grad_norm': 105.33824920654297, 'ratio': 0.76953125},\n",
       " 552: {'grad_norm': 105.73655939102173, 'ratio': 0.73828125},\n",
       " 553: {'grad_norm': 106.13614463806152, 'ratio': 0.78125},\n",
       " 554: {'grad_norm': 106.53550434112549, 'ratio': 0.78125},\n",
       " 555: {'grad_norm': 106.93693256378174, 'ratio': 0.75390625},\n",
       " 556: {'grad_norm': 107.33475685119629, 'ratio': 0.78125},\n",
       " 557: {'grad_norm': 107.73583650588989, 'ratio': 0.79296875},\n",
       " 558: {'grad_norm': 108.14000415802002, 'ratio': 0.7890625},\n",
       " 559: {'grad_norm': 108.5424575805664, 'ratio': 0.77734375},\n",
       " 560: {'grad_norm': 108.94812726974487, 'ratio': 0.78125},\n",
       " 561: {'grad_norm': 109.3547739982605, 'ratio': 0.75390625},\n",
       " 562: {'grad_norm': 109.76104879379272, 'ratio': 0.765625},\n",
       " 563: {'grad_norm': 110.1679310798645, 'ratio': 0.75},\n",
       " 564: {'grad_norm': 110.57646989822388, 'ratio': 0.73828125},\n",
       " 565: {'grad_norm': 110.98698806762695, 'ratio': 0.76171875},\n",
       " 566: {'grad_norm': 111.39694213867188, 'ratio': 0.77734375},\n",
       " 567: {'grad_norm': 111.8107967376709, 'ratio': 0.77734375},\n",
       " 568: {'grad_norm': 112.22482109069824, 'ratio': 0.78125},\n",
       " 569: {'grad_norm': 112.64018487930298, 'ratio': 0.76953125},\n",
       " 570: {'grad_norm': 113.0564169883728, 'ratio': 0.75390625},\n",
       " 571: {'grad_norm': 113.47257232666016, 'ratio': 0.7890625},\n",
       " 572: {'grad_norm': 113.88812494277954, 'ratio': 0.7578125},\n",
       " 573: {'grad_norm': 114.30598211288452, 'ratio': 0.765625},\n",
       " 574: {'grad_norm': 114.72293901443481, 'ratio': 0.73828125},\n",
       " 575: {'grad_norm': 115.14090967178345, 'ratio': 0.76953125},\n",
       " 576: {'grad_norm': 115.56383609771729, 'ratio': 0.76953125},\n",
       " 577: {'grad_norm': 115.98702383041382, 'ratio': 0.76171875},\n",
       " 578: {'grad_norm': 116.41314220428467, 'ratio': 0.77734375},\n",
       " 579: {'grad_norm': 116.83423280715942, 'ratio': 0.78125},\n",
       " 580: {'grad_norm': 117.261155128479, 'ratio': 0.7734375},\n",
       " 581: {'grad_norm': 117.6881217956543, 'ratio': 0.76953125},\n",
       " 582: {'grad_norm': 118.11658573150635, 'ratio': 0.7890625},\n",
       " 583: {'grad_norm': 118.5457935333252, 'ratio': 0.75390625},\n",
       " 584: {'grad_norm': 118.97348690032959, 'ratio': 0.76171875},\n",
       " 585: {'grad_norm': 119.40637588500977, 'ratio': 0.76171875},\n",
       " 586: {'grad_norm': 119.8376350402832, 'ratio': 0.765625},\n",
       " 587: {'grad_norm': 120.27286052703857, 'ratio': 0.765625},\n",
       " 588: {'grad_norm': 120.70567607879639, 'ratio': 0.765625},\n",
       " 589: {'grad_norm': 121.1406192779541, 'ratio': 0.7421875},\n",
       " 590: {'grad_norm': 121.57439041137695, 'ratio': 0.75},\n",
       " 591: {'grad_norm': 122.01083374023438, 'ratio': 0.75},\n",
       " 592: {'grad_norm': 122.44894504547119, 'ratio': 0.75390625},\n",
       " 593: {'grad_norm': 122.88745212554932, 'ratio': 0.74609375},\n",
       " 594: {'grad_norm': 123.32765865325928, 'ratio': 0.7890625},\n",
       " 595: {'grad_norm': 123.7672233581543, 'ratio': 0.76953125},\n",
       " 596: {'grad_norm': 124.21127986907959, 'ratio': 0.77734375},\n",
       " 597: {'grad_norm': 124.65599060058594, 'ratio': 0.7578125},\n",
       " 598: {'grad_norm': 125.10205078125, 'ratio': 0.765625},\n",
       " 599: {'grad_norm': 125.54874801635742, 'ratio': 0.75390625},\n",
       " 600: {'grad_norm': 125.99633502960205, 'ratio': 0.74609375},\n",
       " 601: {'grad_norm': 126.4428596496582, 'ratio': 0.765625},\n",
       " 602: {'grad_norm': 126.8921709060669, 'ratio': 0.765625},\n",
       " 603: {'grad_norm': 127.34437656402588, 'ratio': 0.7578125},\n",
       " 604: {'grad_norm': 127.79144954681396, 'ratio': 0.77734375},\n",
       " 605: {'grad_norm': 128.2470359802246, 'ratio': 0.75},\n",
       " 606: {'grad_norm': 128.69840621948242, 'ratio': 0.74609375},\n",
       " 607: {'grad_norm': 129.14965152740479, 'ratio': 0.74609375},\n",
       " 608: {'grad_norm': 129.6073513031006, 'ratio': 0.7265625},\n",
       " 609: {'grad_norm': 130.0639362335205, 'ratio': 0.765625},\n",
       " 610: {'grad_norm': 130.52017402648926, 'ratio': 0.75390625},\n",
       " 611: {'grad_norm': 130.97829341888428, 'ratio': 0.7578125},\n",
       " 612: {'grad_norm': 131.43901920318604, 'ratio': 0.79296875},\n",
       " 613: {'grad_norm': 131.90092182159424, 'ratio': 0.765625},\n",
       " 614: {'grad_norm': 132.3630132675171, 'ratio': 0.78515625},\n",
       " 615: {'grad_norm': 132.82750988006592, 'ratio': 0.75},\n",
       " 616: {'grad_norm': 133.29193592071533, 'ratio': 0.78125},\n",
       " 617: {'grad_norm': 133.75672054290771, 'ratio': 0.76953125},\n",
       " 618: {'grad_norm': 134.22266101837158, 'ratio': 0.75},\n",
       " 619: {'grad_norm': 134.69027137756348, 'ratio': 0.76953125},\n",
       " 620: {'grad_norm': 135.15773963928223, 'ratio': 0.78125},\n",
       " 621: {'grad_norm': 135.62279891967773, 'ratio': 0.7734375},\n",
       " 622: {'grad_norm': 136.0950164794922, 'ratio': 0.7734375},\n",
       " 623: {'grad_norm': 136.56403255462646, 'ratio': 0.765625},\n",
       " 624: {'grad_norm': 137.03740406036377, 'ratio': 0.77734375},\n",
       " 625: {'grad_norm': 137.51325225830078, 'ratio': 0.7890625},\n",
       " 626: {'grad_norm': 137.98773765563965, 'ratio': 0.7578125},\n",
       " 627: {'grad_norm': 138.4656639099121, 'ratio': 0.765625},\n",
       " 628: {'grad_norm': 138.93897533416748, 'ratio': 0.75390625},\n",
       " 629: {'grad_norm': 139.4182939529419, 'ratio': 0.765625},\n",
       " 630: {'grad_norm': 139.89911937713623, 'ratio': 0.75390625},\n",
       " 631: {'grad_norm': 140.38169193267822, 'ratio': 0.7578125},\n",
       " 632: {'grad_norm': 140.86339473724365, 'ratio': 0.75},\n",
       " 633: {'grad_norm': 141.34836769104004, 'ratio': 0.7734375},\n",
       " 634: {'grad_norm': 141.8299331665039, 'ratio': 0.77734375},\n",
       " 635: {'grad_norm': 142.31490230560303, 'ratio': 0.765625},\n",
       " 636: {'grad_norm': 142.80216121673584, 'ratio': 0.76953125},\n",
       " 637: {'grad_norm': 143.288800239563, 'ratio': 0.7734375},\n",
       " 638: {'grad_norm': 143.77581024169922, 'ratio': 0.7421875},\n",
       " 639: {'grad_norm': 144.26271152496338, 'ratio': 0.75},\n",
       " 640: {'grad_norm': 144.75138759613037, 'ratio': 0.7734375},\n",
       " 641: {'grad_norm': 145.2433271408081, 'ratio': 0.75390625},\n",
       " 642: {'grad_norm': 145.73330402374268, 'ratio': 0.76953125},\n",
       " 643: {'grad_norm': 146.22801876068115, 'ratio': 0.78515625},\n",
       " 644: {'grad_norm': 146.7234926223755, 'ratio': 0.76953125},\n",
       " 645: {'grad_norm': 147.22017002105713, 'ratio': 0.76171875},\n",
       " 646: {'grad_norm': 147.71448612213135, 'ratio': 0.75390625},\n",
       " 647: {'grad_norm': 148.2153377532959, 'ratio': 0.73828125},\n",
       " 648: {'grad_norm': 148.7154188156128, 'ratio': 0.7578125},\n",
       " 649: {'grad_norm': 149.2148952484131, 'ratio': 0.79296875},\n",
       " 650: {'grad_norm': 149.71536922454834, 'ratio': 0.7578125},\n",
       " 651: {'grad_norm': 150.21607875823975, 'ratio': 0.75},\n",
       " 652: {'grad_norm': 150.72389602661133, 'ratio': 0.76953125},\n",
       " 653: {'grad_norm': 151.23032760620117, 'ratio': 0.76953125},\n",
       " 654: {'grad_norm': 151.7341718673706, 'ratio': 0.76953125},\n",
       " 655: {'grad_norm': 152.23940563201904, 'ratio': 0.77734375},\n",
       " 656: {'grad_norm': 152.74777507781982, 'ratio': 0.76171875},\n",
       " 657: {'grad_norm': 153.25599098205566, 'ratio': 0.765625},\n",
       " 658: {'grad_norm': 153.76798057556152, 'ratio': 0.76953125},\n",
       " 659: {'grad_norm': 154.27786922454834, 'ratio': 0.77734375},\n",
       " 660: {'grad_norm': 154.78978729248047, 'ratio': 0.7578125},\n",
       " 661: {'grad_norm': 155.3047742843628, 'ratio': 0.7734375},\n",
       " 662: {'grad_norm': 155.82012367248535, 'ratio': 0.7578125},\n",
       " 663: {'grad_norm': 156.33678531646729, 'ratio': 0.765625},\n",
       " 664: {'grad_norm': 156.8520965576172, 'ratio': 0.7734375},\n",
       " 665: {'grad_norm': 157.37248134613037, 'ratio': 0.75390625},\n",
       " 666: {'grad_norm': 157.8920135498047, 'ratio': 0.76953125},\n",
       " 667: {'grad_norm': 158.4149570465088, 'ratio': 0.7734375},\n",
       " 668: {'grad_norm': 158.93624687194824, 'ratio': 0.76171875},\n",
       " 669: {'grad_norm': 159.45855617523193, 'ratio': 0.78125},\n",
       " 670: {'grad_norm': 159.98294162750244, 'ratio': 0.76171875},\n",
       " 671: {'grad_norm': 160.51024532318115, 'ratio': 0.7578125},\n",
       " 672: {'grad_norm': 161.03672122955322, 'ratio': 0.7734375},\n",
       " 673: {'grad_norm': 161.56389617919922, 'ratio': 0.77734375},\n",
       " 674: {'grad_norm': 162.08970832824707, 'ratio': 0.76953125},\n",
       " 675: {'grad_norm': 162.61627292633057, 'ratio': 0.73828125},\n",
       " 676: {'grad_norm': 163.14367485046387, 'ratio': 0.75390625},\n",
       " 677: {'grad_norm': 163.67613697052002, 'ratio': 0.77734375},\n",
       " 678: {'grad_norm': 164.2066297531128, 'ratio': 0.7734375},\n",
       " 679: {'grad_norm': 164.74270248413086, 'ratio': 0.78125},\n",
       " 680: {'grad_norm': 165.2777271270752, 'ratio': 0.76171875},\n",
       " 681: {'grad_norm': 165.81413459777832, 'ratio': 0.76953125},\n",
       " 682: {'grad_norm': 166.35481071472168, 'ratio': 0.76171875},\n",
       " 683: {'grad_norm': 166.89679718017578, 'ratio': 0.7578125},\n",
       " 684: {'grad_norm': 167.43870639801025, 'ratio': 0.76953125},\n",
       " 685: {'grad_norm': 167.97882461547852, 'ratio': 0.76953125},\n",
       " 686: {'grad_norm': 168.523118019104, 'ratio': 0.73046875},\n",
       " 687: {'grad_norm': 169.06675052642822, 'ratio': 0.7421875},\n",
       " 688: {'grad_norm': 169.61091709136963, 'ratio': 0.765625},\n",
       " 689: {'grad_norm': 170.15497398376465, 'ratio': 0.7578125},\n",
       " 690: {'grad_norm': 170.7005500793457, 'ratio': 0.76953125},\n",
       " 691: {'grad_norm': 171.2514886856079, 'ratio': 0.76171875},\n",
       " 692: {'grad_norm': 171.80509757995605, 'ratio': 0.75390625},\n",
       " 693: {'grad_norm': 172.3588056564331, 'ratio': 0.765625},\n",
       " 694: {'grad_norm': 172.90519332885742, 'ratio': 0.7578125},\n",
       " 695: {'grad_norm': 173.4596643447876, 'ratio': 0.76171875},\n",
       " 696: {'grad_norm': 174.0092544555664, 'ratio': 0.76171875},\n",
       " 697: {'grad_norm': 174.5645170211792, 'ratio': 0.76953125},\n",
       " 698: {'grad_norm': 175.1196060180664, 'ratio': 0.7890625},\n",
       " 699: {'grad_norm': 175.67639827728271, 'ratio': 0.78515625},\n",
       " 700: {'grad_norm': 176.23711681365967, 'ratio': 0.7734375},\n",
       " 701: {'grad_norm': 176.7948179244995, 'ratio': 0.765625},\n",
       " 702: {'grad_norm': 177.36262226104736, 'ratio': 0.74609375},\n",
       " 703: {'grad_norm': 177.92328643798828, 'ratio': 0.76953125},\n",
       " 704: {'grad_norm': 178.4882640838623, 'ratio': 0.7578125},\n",
       " 705: {'grad_norm': 179.05261135101318, 'ratio': 0.75390625},\n",
       " 706: {'grad_norm': 179.61554718017578, 'ratio': 0.78125},\n",
       " 707: {'grad_norm': 180.18046855926514, 'ratio': 0.7890625},\n",
       " 708: {'grad_norm': 180.74910354614258, 'ratio': 0.77734375},\n",
       " 709: {'grad_norm': 181.31770038604736, 'ratio': 0.75390625},\n",
       " 710: {'grad_norm': 181.8878583908081, 'ratio': 0.75},\n",
       " 711: {'grad_norm': 182.45764255523682, 'ratio': 0.7421875},\n",
       " 712: {'grad_norm': 183.03173351287842, 'ratio': 0.796875},\n",
       " 713: {'grad_norm': 183.60821342468262, 'ratio': 0.765625},\n",
       " 714: {'grad_norm': 184.18094730377197, 'ratio': 0.80078125},\n",
       " 715: {'grad_norm': 184.75659084320068, 'ratio': 0.77734375},\n",
       " 716: {'grad_norm': 185.33466720581055, 'ratio': 0.7734375},\n",
       " 717: {'grad_norm': 185.91426467895508, 'ratio': 0.7578125},\n",
       " 718: {'grad_norm': 186.49246788024902, 'ratio': 0.78515625},\n",
       " 719: {'grad_norm': 187.07364082336426, 'ratio': 0.78125},\n",
       " 720: {'grad_norm': 187.65952110290527, 'ratio': 0.78125},\n",
       " 721: {'grad_norm': 188.24258518218994, 'ratio': 0.765625},\n",
       " 722: {'grad_norm': 188.8258762359619, 'ratio': 0.74609375},\n",
       " 723: {'grad_norm': 189.41672801971436, 'ratio': 0.75390625},\n",
       " 724: {'grad_norm': 190.0027256011963, 'ratio': 0.76953125},\n",
       " 725: {'grad_norm': 190.59314823150635, 'ratio': 0.765625},\n",
       " 726: {'grad_norm': 191.1825647354126, 'ratio': 0.796875},\n",
       " 727: {'grad_norm': 191.76771926879883, 'ratio': 0.75390625},\n",
       " 728: {'grad_norm': 192.35692596435547, 'ratio': 0.78515625},\n",
       " 729: {'grad_norm': 192.94898223876953, 'ratio': 0.76171875},\n",
       " 730: {'grad_norm': 193.54571437835693, 'ratio': 0.7734375},\n",
       " 731: {'grad_norm': 194.14403820037842, 'ratio': 0.78515625},\n",
       " 732: {'grad_norm': 194.7391119003296, 'ratio': 0.7734375},\n",
       " 733: {'grad_norm': 195.33568954467773, 'ratio': 0.79296875},\n",
       " 734: {'grad_norm': 195.9338665008545, 'ratio': 0.76171875},\n",
       " 735: {'grad_norm': 196.53758430480957, 'ratio': 0.75},\n",
       " 736: {'grad_norm': 197.13841915130615, 'ratio': 0.79296875},\n",
       " 737: {'grad_norm': 197.74069213867188, 'ratio': 0.76171875},\n",
       " 738: {'grad_norm': 198.34460163116455, 'ratio': 0.7578125},\n",
       " 739: {'grad_norm': 198.94950103759766, 'ratio': 0.75},\n",
       " 740: {'grad_norm': 199.55541229248047, 'ratio': 0.77734375},\n",
       " 741: {'grad_norm': 200.16390895843506, 'ratio': 0.7734375},\n",
       " 742: {'grad_norm': 200.7744436264038, 'ratio': 0.77734375},\n",
       " 743: {'grad_norm': 201.38260746002197, 'ratio': 0.78125},\n",
       " 744: {'grad_norm': 201.99292087554932, 'ratio': 0.73046875},\n",
       " 745: {'grad_norm': 202.60340881347656, 'ratio': 0.75},\n",
       " 746: {'grad_norm': 203.21248531341553, 'ratio': 0.7734375},\n",
       " 747: {'grad_norm': 203.82677936553955, 'ratio': 0.765625},\n",
       " 748: {'grad_norm': 204.441876411438, 'ratio': 0.78125},\n",
       " 749: {'grad_norm': 205.05875778198242, 'ratio': 0.7578125},\n",
       " 750: {'grad_norm': 205.68031311035156, 'ratio': 0.76953125},\n",
       " 751: {'grad_norm': 206.30031967163086, 'ratio': 0.76953125},\n",
       " 752: {'grad_norm': 206.92185878753662, 'ratio': 0.76953125},\n",
       " 753: {'grad_norm': 207.54601287841797, 'ratio': 0.75390625},\n",
       " 754: {'grad_norm': 208.16719818115234, 'ratio': 0.78125},\n",
       " 755: {'grad_norm': 208.79170417785645, 'ratio': 0.7578125},\n",
       " 756: {'grad_norm': 209.42301750183105, 'ratio': 0.7734375},\n",
       " 757: {'grad_norm': 210.05326652526855, 'ratio': 0.765625},\n",
       " 758: {'grad_norm': 210.67583465576172, 'ratio': 0.77734375},\n",
       " 759: {'grad_norm': 211.30084991455078, 'ratio': 0.77734375},\n",
       " 760: {'grad_norm': 211.93047332763672, 'ratio': 0.76953125},\n",
       " 761: {'grad_norm': 212.5605297088623, 'ratio': 0.765625},\n",
       " 762: {'grad_norm': 213.1936206817627, 'ratio': 0.7734375},\n",
       " 763: {'grad_norm': 213.83019256591797, 'ratio': 0.76171875},\n",
       " 764: {'grad_norm': 214.46384811401367, 'ratio': 0.76953125},\n",
       " 765: {'grad_norm': 215.10063362121582, 'ratio': 0.7734375},\n",
       " 766: {'grad_norm': 215.7372341156006, 'ratio': 0.76171875},\n",
       " 767: {'grad_norm': 216.38260078430176, 'ratio': 0.765625},\n",
       " 768: {'grad_norm': 217.02366065979004, 'ratio': 0.74609375},\n",
       " 769: {'grad_norm': 217.66728591918945, 'ratio': 0.7734375},\n",
       " 770: {'grad_norm': 218.30828094482422, 'ratio': 0.765625},\n",
       " 771: {'grad_norm': 218.95043182373047, 'ratio': 0.74609375},\n",
       " 772: {'grad_norm': 219.58733558654785, 'ratio': 0.76171875},\n",
       " 773: {'grad_norm': 220.23371696472168, 'ratio': 0.7890625},\n",
       " 774: {'grad_norm': 220.88415908813477, 'ratio': 0.7421875},\n",
       " 775: {'grad_norm': 221.53673362731934, 'ratio': 0.75},\n",
       " 776: {'grad_norm': 222.19131660461426, 'ratio': 0.7734375},\n",
       " 777: {'grad_norm': 222.8430061340332, 'ratio': 0.75},\n",
       " 778: {'grad_norm': 223.49984550476074, 'ratio': 0.76953125},\n",
       " 779: {'grad_norm': 224.14782524108887, 'ratio': 0.75},\n",
       " 780: {'grad_norm': 224.80060958862305, 'ratio': 0.7734375},\n",
       " 781: {'grad_norm': 225.45823669433594, 'ratio': 0.7734375},\n",
       " 782: {'grad_norm': 226.11627578735352, 'ratio': 0.76171875},\n",
       " 783: {'grad_norm': 226.77593612670898, 'ratio': 0.765625},\n",
       " 784: {'grad_norm': 227.4276180267334, 'ratio': 0.76953125},\n",
       " 785: {'grad_norm': 228.08185386657715, 'ratio': 0.75390625},\n",
       " 786: {'grad_norm': 228.74496841430664, 'ratio': 0.77734375},\n",
       " 787: {'grad_norm': 229.41098403930664, 'ratio': 0.765625},\n",
       " 788: {'grad_norm': 230.07827377319336, 'ratio': 0.77734375},\n",
       " 789: {'grad_norm': 230.7496337890625, 'ratio': 0.78125},\n",
       " 790: {'grad_norm': 231.42040061950684, 'ratio': 0.7578125},\n",
       " 791: {'grad_norm': 232.09036445617676, 'ratio': 0.75390625},\n",
       " 792: {'grad_norm': 232.7564239501953, 'ratio': 0.765625},\n",
       " 793: {'grad_norm': 233.42710494995117, 'ratio': 0.80078125},\n",
       " 794: {'grad_norm': 234.1016616821289, 'ratio': 0.765625},\n",
       " 795: {'grad_norm': 234.77619171142578, 'ratio': 0.77734375},\n",
       " 796: {'grad_norm': 235.4499797821045, 'ratio': 0.7578125},\n",
       " 797: {'grad_norm': 236.12847709655762, 'ratio': 0.75390625},\n",
       " 798: {'grad_norm': 236.80262565612793, 'ratio': 0.76171875},\n",
       " 799: {'grad_norm': 237.47842979431152, 'ratio': 0.79296875},\n",
       " 800: {'grad_norm': 238.16037559509277, 'ratio': 0.75390625},\n",
       " 801: {'grad_norm': 238.84755516052246, 'ratio': 0.7734375},\n",
       " 802: {'grad_norm': 239.53068733215332, 'ratio': 0.77734375},\n",
       " 803: {'grad_norm': 240.2106647491455, 'ratio': 0.73828125},\n",
       " 804: {'grad_norm': 240.89117813110352, 'ratio': 0.77734375},\n",
       " 805: {'grad_norm': 241.57861709594727, 'ratio': 0.76171875},\n",
       " 806: {'grad_norm': 242.26810264587402, 'ratio': 0.7734375},\n",
       " 807: {'grad_norm': 242.9604721069336, 'ratio': 0.7734375},\n",
       " 808: {'grad_norm': 243.6511993408203, 'ratio': 0.765625},\n",
       " 809: {'grad_norm': 244.34062957763672, 'ratio': 0.76953125},\n",
       " 810: {'grad_norm': 245.0332374572754, 'ratio': 0.7734375},\n",
       " 811: {'grad_norm': 245.72973442077637, 'ratio': 0.78125},\n",
       " 812: {'grad_norm': 246.41829872131348, 'ratio': 0.7578125},\n",
       " 813: {'grad_norm': 247.1113796234131, 'ratio': 0.76953125},\n",
       " 814: {'grad_norm': 247.8145923614502, 'ratio': 0.76171875},\n",
       " 815: {'grad_norm': 248.51133918762207, 'ratio': 0.7734375},\n",
       " 816: {'grad_norm': 249.21366119384766, 'ratio': 0.76953125},\n",
       " 817: {'grad_norm': 249.9153823852539, 'ratio': 0.7734375},\n",
       " 818: {'grad_norm': 250.61899948120117, 'ratio': 0.7578125},\n",
       " 819: {'grad_norm': 251.32446479797363, 'ratio': 0.7578125},\n",
       " 820: {'grad_norm': 252.03057289123535, 'ratio': 0.76953125},\n",
       " 821: {'grad_norm': 252.74083709716797, 'ratio': 0.78515625},\n",
       " 822: {'grad_norm': 253.44930839538574, 'ratio': 0.76953125},\n",
       " 823: {'grad_norm': 254.15867233276367, 'ratio': 0.765625},\n",
       " 824: {'grad_norm': 254.87202835083008, 'ratio': 0.7890625},\n",
       " 825: {'grad_norm': 255.57101249694824, 'ratio': 0.77734375},\n",
       " 826: {'grad_norm': 256.28700828552246, 'ratio': 0.765625},\n",
       " 827: {'grad_norm': 256.9941215515137, 'ratio': 0.78515625},\n",
       " 828: {'grad_norm': 257.70678520202637, 'ratio': 0.77734375},\n",
       " 829: {'grad_norm': 258.42501068115234, 'ratio': 0.79296875},\n",
       " 830: {'grad_norm': 259.1454391479492, 'ratio': 0.7734375},\n",
       " 831: {'grad_norm': 259.866024017334, 'ratio': 0.7578125},\n",
       " 832: {'grad_norm': 260.5857620239258, 'ratio': 0.7734375},\n",
       " 833: {'grad_norm': 261.3104438781738, 'ratio': 0.74609375},\n",
       " 834: {'grad_norm': 262.0383548736572, 'ratio': 0.7578125},\n",
       " 835: {'grad_norm': 262.7640132904053, 'ratio': 0.765625},\n",
       " 836: {'grad_norm': 263.4966812133789, 'ratio': 0.76171875},\n",
       " 837: {'grad_norm': 264.22546195983887, 'ratio': 0.77734375},\n",
       " 838: {'grad_norm': 264.9539051055908, 'ratio': 0.78125},\n",
       " 839: {'grad_norm': 265.67918586730957, 'ratio': 0.78125},\n",
       " 840: {'grad_norm': 266.4120407104492, 'ratio': 0.78515625},\n",
       " 841: {'grad_norm': 267.14108657836914, 'ratio': 0.77734375},\n",
       " 842: {'grad_norm': 267.8772716522217, 'ratio': 0.765625},\n",
       " 843: {'grad_norm': 268.60631370544434, 'ratio': 0.77734375},\n",
       " 844: {'grad_norm': 269.34329414367676, 'ratio': 0.765625},\n",
       " 845: {'grad_norm': 270.07349014282227, 'ratio': 0.75390625},\n",
       " 846: {'grad_norm': 270.8093204498291, 'ratio': 0.75390625},\n",
       " 847: {'grad_norm': 271.5506248474121, 'ratio': 0.76953125},\n",
       " 848: {'grad_norm': 272.29491424560547, 'ratio': 0.76953125},\n",
       " 849: {'grad_norm': 273.042516708374, 'ratio': 0.76953125},\n",
       " 850: {'grad_norm': 273.788667678833, 'ratio': 0.75},\n",
       " 851: {'grad_norm': 274.53499031066895, 'ratio': 0.78125},\n",
       " 852: {'grad_norm': 275.2786388397217, 'ratio': 0.78515625},\n",
       " 853: {'grad_norm': 276.0232582092285, 'ratio': 0.76953125},\n",
       " 854: {'grad_norm': 276.7726078033447, 'ratio': 0.76953125},\n",
       " 855: {'grad_norm': 277.5205497741699, 'ratio': 0.80078125},\n",
       " 856: {'grad_norm': 278.2741527557373, 'ratio': 0.76171875},\n",
       " 857: {'grad_norm': 279.0283546447754, 'ratio': 0.765625},\n",
       " 858: {'grad_norm': 279.7809638977051, 'ratio': 0.76171875},\n",
       " 859: {'grad_norm': 280.5346050262451, 'ratio': 0.76171875},\n",
       " 860: {'grad_norm': 281.2949161529541, 'ratio': 0.80078125},\n",
       " 861: {'grad_norm': 282.059289932251, 'ratio': 0.77734375},\n",
       " 862: {'grad_norm': 282.8191337585449, 'ratio': 0.765625},\n",
       " 863: {'grad_norm': 283.5842514038086, 'ratio': 0.78125},\n",
       " 864: {'grad_norm': 284.3459949493408, 'ratio': 0.75390625},\n",
       " 865: {'grad_norm': 285.1140480041504, 'ratio': 0.7890625},\n",
       " 866: {'grad_norm': 285.86586570739746, 'ratio': 0.765625},\n",
       " 867: {'grad_norm': 286.6331539154053, 'ratio': 0.78125},\n",
       " 868: {'grad_norm': 287.3983211517334, 'ratio': 0.74609375},\n",
       " 869: {'grad_norm': 288.16779708862305, 'ratio': 0.7734375},\n",
       " 870: {'grad_norm': 288.9383792877197, 'ratio': 0.77734375},\n",
       " 871: {'grad_norm': 289.71239280700684, 'ratio': 0.76953125},\n",
       " 872: {'grad_norm': 290.4824848175049, 'ratio': 0.76953125},\n",
       " 873: {'grad_norm': 291.2596378326416, 'ratio': 0.76171875},\n",
       " 874: {'grad_norm': 292.0377426147461, 'ratio': 0.76171875},\n",
       " 875: {'grad_norm': 292.810941696167, 'ratio': 0.75},\n",
       " 876: {'grad_norm': 293.5892581939697, 'ratio': 0.78125},\n",
       " 877: {'grad_norm': 294.37340545654297, 'ratio': 0.77734375},\n",
       " 878: {'grad_norm': 295.1553497314453, 'ratio': 0.77734375},\n",
       " 879: {'grad_norm': 295.9322681427002, 'ratio': 0.76953125},\n",
       " 880: {'grad_norm': 296.71178245544434, 'ratio': 0.78125},\n",
       " 881: {'grad_norm': 297.4963798522949, 'ratio': 0.7890625},\n",
       " 882: {'grad_norm': 298.28466796875, 'ratio': 0.75},\n",
       " 883: {'grad_norm': 299.06930923461914, 'ratio': 0.765625},\n",
       " 884: {'grad_norm': 299.861270904541, 'ratio': 0.76953125},\n",
       " 885: {'grad_norm': 300.65062522888184, 'ratio': 0.76953125},\n",
       " 886: {'grad_norm': 301.44470024108887, 'ratio': 0.76171875},\n",
       " 887: {'grad_norm': 302.2381057739258, 'ratio': 0.78125},\n",
       " 888: {'grad_norm': 303.0274085998535, 'ratio': 0.75390625},\n",
       " 889: {'grad_norm': 303.82767486572266, 'ratio': 0.77734375},\n",
       " 890: {'grad_norm': 304.62925148010254, 'ratio': 0.76953125},\n",
       " 891: {'grad_norm': 305.42244148254395, 'ratio': 0.76953125},\n",
       " 892: {'grad_norm': 306.2210350036621, 'ratio': 0.765625},\n",
       " 893: {'grad_norm': 307.0158042907715, 'ratio': 0.765625},\n",
       " 894: {'grad_norm': 307.81947326660156, 'ratio': 0.7734375},\n",
       " 895: {'grad_norm': 308.625186920166, 'ratio': 0.7890625},\n",
       " 896: {'grad_norm': 309.4240074157715, 'ratio': 0.765625},\n",
       " 897: {'grad_norm': 310.2312316894531, 'ratio': 0.75390625},\n",
       " 898: {'grad_norm': 311.04505348205566, 'ratio': 0.76953125},\n",
       " 899: {'grad_norm': 311.8556365966797, 'ratio': 0.75390625},\n",
       " 900: {'grad_norm': 312.6628303527832, 'ratio': 0.75390625},\n",
       " 901: {'grad_norm': 313.4789009094238, 'ratio': 0.76953125},\n",
       " 902: {'grad_norm': 314.2930335998535, 'ratio': 0.76171875},\n",
       " 903: {'grad_norm': 315.11230278015137, 'ratio': 0.7421875},\n",
       " 904: {'grad_norm': 315.93115043640137, 'ratio': 0.76171875},\n",
       " 905: {'grad_norm': 316.7519474029541, 'ratio': 0.765625},\n",
       " 906: {'grad_norm': 317.56689453125, 'ratio': 0.75390625},\n",
       " 907: {'grad_norm': 318.3798484802246, 'ratio': 0.76953125},\n",
       " 908: {'grad_norm': 319.19755363464355, 'ratio': 0.7734375},\n",
       " 909: {'grad_norm': 320.0141353607178, 'ratio': 0.78125},\n",
       " 910: {'grad_norm': 320.8344497680664, 'ratio': 0.77734375},\n",
       " 911: {'grad_norm': 321.66643714904785, 'ratio': 0.78125},\n",
       " 912: {'grad_norm': 322.4945888519287, 'ratio': 0.7578125},\n",
       " 913: {'grad_norm': 323.3192825317383, 'ratio': 0.77734375},\n",
       " 914: {'grad_norm': 324.14212226867676, 'ratio': 0.76953125},\n",
       " 915: {'grad_norm': 324.9738368988037, 'ratio': 0.75},\n",
       " 916: {'grad_norm': 325.80192375183105, 'ratio': 0.78515625},\n",
       " 917: {'grad_norm': 326.6398983001709, 'ratio': 0.79296875},\n",
       " 918: {'grad_norm': 327.476957321167, 'ratio': 0.78515625},\n",
       " 919: {'grad_norm': 328.3170394897461, 'ratio': 0.76953125},\n",
       " 920: {'grad_norm': 329.15250968933105, 'ratio': 0.78515625},\n",
       " 921: {'grad_norm': 329.9876079559326, 'ratio': 0.77734375},\n",
       " 922: {'grad_norm': 330.82637786865234, 'ratio': 0.78515625},\n",
       " 923: {'grad_norm': 331.66938972473145, 'ratio': 0.7734375},\n",
       " 924: {'grad_norm': 332.5113353729248, 'ratio': 0.765625},\n",
       " 925: {'grad_norm': 333.3537311553955, 'ratio': 0.7578125},\n",
       " 926: {'grad_norm': 334.19628715515137, 'ratio': 0.796875},\n",
       " 927: {'grad_norm': 335.05138206481934, 'ratio': 0.78125},\n",
       " 928: {'grad_norm': 335.8992614746094, 'ratio': 0.7578125},\n",
       " 929: {'grad_norm': 336.7450428009033, 'ratio': 0.76953125},\n",
       " 930: {'grad_norm': 337.6015396118164, 'ratio': 0.7578125},\n",
       " 931: {'grad_norm': 338.4519729614258, 'ratio': 0.7734375},\n",
       " 932: {'grad_norm': 339.30329513549805, 'ratio': 0.796875},\n",
       " 933: {'grad_norm': 340.15170097351074, 'ratio': 0.76171875},\n",
       " 934: {'grad_norm': 341.0126609802246, 'ratio': 0.7734375},\n",
       " 935: {'grad_norm': 341.869197845459, 'ratio': 0.76953125},\n",
       " 936: {'grad_norm': 342.73460388183594, 'ratio': 0.78125},\n",
       " 937: {'grad_norm': 343.60059928894043, 'ratio': 0.76171875},\n",
       " 938: {'grad_norm': 344.4582405090332, 'ratio': 0.76953125},\n",
       " 939: {'grad_norm': 345.32094383239746, 'ratio': 0.765625},\n",
       " 940: {'grad_norm': 346.1811332702637, 'ratio': 0.76171875},\n",
       " 941: {'grad_norm': 347.052791595459, 'ratio': 0.78125},\n",
       " 942: {'grad_norm': 347.9234142303467, 'ratio': 0.78125},\n",
       " 943: {'grad_norm': 348.7901210784912, 'ratio': 0.74609375},\n",
       " 944: {'grad_norm': 349.6541633605957, 'ratio': 0.76171875},\n",
       " 945: {'grad_norm': 350.5258541107178, 'ratio': 0.78515625},\n",
       " 946: {'grad_norm': 351.39527130126953, 'ratio': 0.75390625},\n",
       " 947: {'grad_norm': 352.26780891418457, 'ratio': 0.76953125},\n",
       " 948: {'grad_norm': 353.14513778686523, 'ratio': 0.765625},\n",
       " 949: {'grad_norm': 354.01931381225586, 'ratio': 0.7734375},\n",
       " 950: {'grad_norm': 354.9003677368164, 'ratio': 0.7734375},\n",
       " 951: {'grad_norm': 355.7852897644043, 'ratio': 0.74609375},\n",
       " 952: {'grad_norm': 356.6702880859375, 'ratio': 0.74609375},\n",
       " 953: {'grad_norm': 357.5410785675049, 'ratio': 0.7734375},\n",
       " 954: {'grad_norm': 358.42812156677246, 'ratio': 0.7578125},\n",
       " 955: {'grad_norm': 359.31695556640625, 'ratio': 0.765625},\n",
       " 956: {'grad_norm': 360.19722747802734, 'ratio': 0.7578125},\n",
       " 957: {'grad_norm': 361.0892143249512, 'ratio': 0.75390625},\n",
       " 958: {'grad_norm': 361.9808750152588, 'ratio': 0.7421875},\n",
       " 959: {'grad_norm': 362.87782859802246, 'ratio': 0.7578125},\n",
       " 960: {'grad_norm': 363.7626953125, 'ratio': 0.77734375},\n",
       " 961: {'grad_norm': 364.65527153015137, 'ratio': 0.76953125},\n",
       " 962: {'grad_norm': 365.54370880126953, 'ratio': 0.76953125},\n",
       " 963: {'grad_norm': 366.4385395050049, 'ratio': 0.7578125},\n",
       " 964: {'grad_norm': 367.3381404876709, 'ratio': 0.7734375},\n",
       " 965: {'grad_norm': 368.24344635009766, 'ratio': 0.75390625},\n",
       " 966: {'grad_norm': 369.14344024658203, 'ratio': 0.78125},\n",
       " 967: {'grad_norm': 370.0433006286621, 'ratio': 0.76171875},\n",
       " 968: {'grad_norm': 370.9511184692383, 'ratio': 0.734375},\n",
       " 969: {'grad_norm': 371.85551261901855, 'ratio': 0.76171875},\n",
       " 970: {'grad_norm': 372.7580738067627, 'ratio': 0.75},\n",
       " 971: {'grad_norm': 373.6651496887207, 'ratio': 0.76953125},\n",
       " 972: {'grad_norm': 374.5781307220459, 'ratio': 0.7578125},\n",
       " 973: {'grad_norm': 375.4948196411133, 'ratio': 0.765625},\n",
       " 974: {'grad_norm': 376.4100818634033, 'ratio': 0.76171875},\n",
       " 975: {'grad_norm': 377.32162857055664, 'ratio': 0.77734375},\n",
       " 976: {'grad_norm': 378.2330780029297, 'ratio': 0.7734375},\n",
       " 977: {'grad_norm': 379.1505184173584, 'ratio': 0.76171875},\n",
       " 978: {'grad_norm': 380.07421684265137, 'ratio': 0.76171875},\n",
       " 979: {'grad_norm': 380.9823913574219, 'ratio': 0.7578125},\n",
       " 980: {'grad_norm': 381.89686012268066, 'ratio': 0.76953125},\n",
       " 981: {'grad_norm': 382.8082695007324, 'ratio': 0.7578125},\n",
       " 982: {'grad_norm': 383.7368812561035, 'ratio': 0.74609375},\n",
       " 983: {'grad_norm': 384.6612739562988, 'ratio': 0.765625},\n",
       " 984: {'grad_norm': 385.5895881652832, 'ratio': 0.75},\n",
       " 985: {'grad_norm': 386.5179443359375, 'ratio': 0.75},\n",
       " 986: {'grad_norm': 387.4423637390137, 'ratio': 0.76171875},\n",
       " 987: {'grad_norm': 388.36995697021484, 'ratio': 0.765625},\n",
       " 988: {'grad_norm': 389.3080062866211, 'ratio': 0.75},\n",
       " 989: {'grad_norm': 390.23938751220703, 'ratio': 0.7734375},\n",
       " 990: {'grad_norm': 391.16691970825195, 'ratio': 0.75390625},\n",
       " 991: {'grad_norm': 392.10924530029297, 'ratio': 0.77734375},\n",
       " 992: {'grad_norm': 393.0533676147461, 'ratio': 0.76953125},\n",
       " 993: {'grad_norm': 393.99268341064453, 'ratio': 0.79296875},\n",
       " 994: {'grad_norm': 394.931339263916, 'ratio': 0.76953125},\n",
       " 995: {'grad_norm': 395.8699264526367, 'ratio': 0.76171875},\n",
       " 996: {'grad_norm': 396.8123016357422, 'ratio': 0.7578125},\n",
       " 997: {'grad_norm': 397.7560615539551, 'ratio': 0.78125},\n",
       " 998: {'grad_norm': 398.6944770812988, 'ratio': 0.75390625},\n",
       " 999: {'grad_norm': 399.64423751831055, 'ratio': 0.75390625},\n",
       " ...}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_norm_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d246dfe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses_1 = [r['val_loss'] for r in history_1]\n",
    "len(val_losses_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eee7e6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minimal_ratio_eps = [i['ratio'] for i in grad_norm_1.values() ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "606b3bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9,12))\n",
    "plt.scatter(minimal_ratio_eps,val_losses_1,color = 'Blue')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Minimal Ratio')\n",
    "plt.title('Minimal Ratio To The Loss');\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
